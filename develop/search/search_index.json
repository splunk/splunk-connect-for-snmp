{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#splunk-connect-for-snmp","title":"Splunk Connect for SNMP","text":"<p>Splunk welcomes your experimentation and feedback. Let your account team know that you are testing Splunk Connect for SNMP.</p> <p>Splunk Connect for SNMP is an edge-deployed, containerized, and highly available solution for collecting SNMP data for Splunk Enterprise, Splunk Enterprise Cloud, and Splunk Infrastructure Monitoring.</p> <p>SC4SNMP provides context-full information. It not only forwards SNMP data to Splunk, but also integrates the data into  meaningful objects. For example, you do not need to write queries in order to gather information about interfaces of the device, because SC4SNMP does that automatically:</p> <p></p> <p>This makes it easy to visualize the data in Splunk Analytics:</p> <p></p> <p>Here is a short presentation of how to browse SNMP data in Splunk:</p> <p></p> <p>SC4SNMP can also easily monitor trap events sent by different SNMP devices. Trap events are JSON formatted, and are stored under the <code>netops</code> index.</p> <p></p>"},{"location":"dashboard/","title":"Monitoring dashboard","text":""},{"location":"dashboard/#dashboard","title":"Dashboard","text":"<p>The dashboard is a monitoring tool to ensure that SC4SNMP is working correctly. It is a set of charts that  show the status of SC4SNMP tasks.</p>"},{"location":"dashboard/#presetting","title":"Presetting","text":"<p>Info</p> <p>Dashboard is compatible starting from version 1.11.0</p> <ol> <li>Create metrics index in Splunk.</li> <li>Enable metrics logging for your runtime:<ul> <li>For Kubernetes install Splunk OpenTelemetry Collector for K8S</li> <li>For Docker Compose use Splunk logging driver for docker</li> </ul> </li> </ol>"},{"location":"dashboard/#install-dashboard","title":"Install dashboard","text":"<ol> <li>In Splunk platform open Search -&gt; Dashboards.</li> <li>Click on Create New Dashboard and make an empty dashboard. Be sure to choose Classic Dashboards.</li> <li>In the Edit Dashboard view, go to Source and replace the initial xml with the contents of dashboard.xml.     The file can be found on release page in     attachments under your SC4SNMP version. </li> <li>Save your changes. The dashboard is ready to use.</li> </ol>"},{"location":"dashboard/#metrics-explanation","title":"Metrics explanation","text":""},{"location":"dashboard/#polling-dashboards","title":"Polling dashboards","text":"<p>To check that polling on your device is working correctly, look at SNMP schedule of polling tasks dashboard. With this chart you can understand when SC4SNMP scheduled polling for your device last time. The process works if  it runs regularly.</p> <p>After double-checking that SC4SNMP scheduled polling tasks for your SNMP device we need to be sure that polling is working. For that look at another dashboard SNMP polling status and if everything is working you will see only succeeded status of polling. If something is going wrong you will see also another status (like on screenshot), then use troubleshooting docs  for that.</p> <p></p>"},{"location":"dashboard/#walk-dashboards","title":"Walk dashboards","text":"<p>To check that walk on your device is working correctly first of all check SNMP schedule of walk tasks dashboard. Using this chart you can understand when SC4SNMP scheduled walk for your SNMP device last time. The process works if it runs regularly.</p> <p>After double-checking that SC4SNMP scheduled walk tasks for your SNMP device we need to be sure walk is running. For that look at another dashboard SNMP walk status and if everything is working you will see only succeeded status of walk. If something is going wrong you will see another status (like on screenshot), then use troubleshooting docs  for that.</p> <p></p>"},{"location":"dashboard/#trap-dashboards","title":"Trap dashboards","text":"<p>First of all check SNMP traps authorisation dashboard, if you see only succeeded status it means that authorisation  is configured correctly, otherwise please use troubleshooting docs for that.</p> <p>After checking that we do not have any authorisation traps issues we can check that trap tasks are working correctly.  For that we need to go SNMP trap status dashboard, if we have only succeeded status it means that everything is working,  otherwise we will see information with another status.</p> <p></p>"},{"location":"dashboard/#other-dashboards","title":"Other dashboards","text":"<p>We also have tasks that will be a callback for walk and poll. For example send will publish result in Splunk.  We need to be sure that after successful walk and poll those callbacks have completed. Please check that we have only  successful status for those tasks.</p> <p></p>"},{"location":"ha/","title":"High Availability","text":""},{"location":"ha/#high-availability","title":"High Availability","text":"<p>The SNMP protocol uses UDP as the transport protocol. Network reliability is a constraint. Consider network architecture when designing for high availability:</p> <ul> <li>When using a single node collector, ensure automatic recovery from virtual infrastructure, such as VMware or Openstack.</li> <li>When using a multi-node cluster, ensure nodes are not located in a way where the majority of nodes can be lost.  For example, consider row, rack, network, power and storage.</li> <li>When determining the placement of clusters, the closest location by the number of network hops should be used.</li> <li>For \u201cdata center\u201d applications, collection should be local to the data center.</li> <li>Consider using IP Anycast.</li> </ul>"},{"location":"improved-polling/","title":"Improved polling performance","text":""},{"location":"improved-polling/#improved-polling-performance","title":"Improved polling performance","text":"<p>SC4SNMP now offers beta support for improved polling performance.</p> <p>While this is in beta, we encourage users to explore it. Although we have conducted extensive testing, occasional issues may arise. Your feedback during this phase is crucial in refining and optimizing and can be shared using issues. To get started, the zip file with helm chart must be downloaded. It can be found on feat/improve-polling-time branch.</p> <p>On the left-hand side click <code>create-charts-zip</code>: </p> <p>At the bottom of the page in the <code>Artifacts</code> section there will be  <code>charts</code> package. Download it and unzip it in your environment.</p> <p></p> <p>In <code>values.yaml</code> set the following image settings:</p> <pre><code>image:\n  repository: ghcr.io/splunk/splunk-connect-for-snmp/improved-polling-time\n  tag: \"latest\"\n</code></pre> <p>Change the directory to <code>charts/splunk-connect-for-snmp</code> and run <code>microk8s helm3 dep update</code>. You can exit <code>charts/splunk-connect-for-snmp</code> directory. While running <code>microk8s helm3 install</code> or <code>microk8s helm3 upgrade</code> commands, path to the helm chart must be modified.  In the sc4snmp installation documentation, the following commands are presented: </p><pre><code>microk8s helm3 install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <p>In order to use this beta release, <code>splunk-connect-for-snmp/splunk-connect-for-snmp</code> needs to be changed to the path of the <code>charts/splunk-connect-for-snmp</code> directory. </p> <p>To learn how the new improved polling works, refer to the documentation Poller Configuration - Define maxRepetitions  for instructions.</p> <p>Your involvement in testing new polling support is pivotal, and we look forward to hearing about your experiences.</p>"},{"location":"mib-request/","title":"Request MIB","text":""},{"location":"mib-request/#mib-submission-process","title":"MIB submission process","text":"<p>To achieve human-readable OIDs, the corresponding MIB files are necessary. They are stored in the MIB server, which is one of the components of SC4SNMP.</p> <p>See the following link for a list of currently available MIBs: https://pysnmp.github.io/mibs/index.csv</p> <p>An alternative way to check if the MIB you are interested in is being served is to check the following link: <code>https://pysnmp.github.io/mibs/asn1/@mib@</code> where <code>@mib@</code> is the name of MIB, for example, <code>IF-MIB</code>. If the file  is downloading, that means the MIB file exists in the MIB server.</p>"},{"location":"mib-request/#submit-new-mib-file","title":"Submit new MIB file","text":"<p>In case you want to add a new MIB file to the MIB server, see the following steps:</p> <ol> <li> <p>Create a fork of the https://github.com/pysnmp/mibs repository.</p> </li> <li> <p>Put one or more MIB files under <code>src/vendor/@vendor_name@</code> where <code>@vendor_name@</code> is the name of the MIB file\u2019s vendor.  If there is currently no directory of vendors that you need, create it yourself.</p> </li> <li> <p>Create a pull request to a <code>main</code> branch.</p> </li> <li> <p>Name the pull request the following way: <code>feat: add @vendor_name@ MIB files</code>.</p> </li> </ol> <p>An alternative way of adding MIBs to the MIB server is to create an issue in the https://github.com/pysnmp/mibs repository, attaching the files and information about  the vendor.</p>"},{"location":"mib-request/#update-your-instance-of-sc4snmp-with-the-newest-mib-server","title":"Update your instance of SC4SNMP with the newest MIB server","text":"<p>Usually SC4SNMP is released with the newest version of MIB server every time the new MIB files are added. But, if you want to use the newest MIB server right after it is released, you can do it manually using the <code>values. yaml</code> file:</p> <ol> <li> <p>Append <code>mibserver</code> configuration to the values.yaml, with the <code>mibserver.image.tag</code> of a value of the newest <code>mibserver</code>, for example: </p><pre><code>mibserver:\n  image:\n    tag: \"1.15.13\"\n</code></pre> Check all the MIB server releases in https://github.com/pysnmp/mibs/releases.  </li> <li> <p>Run <code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace</code>.</p> </li> <li> <p>Restart the following worker-trap and worker-poller deployments:</p> </li> </ol> <pre><code>microk8s kubectl rollout restart deployment snmp-splunk-connect-for-snmp-worker-trap -n sc4snmp\nmicrok8s kubectl rollout restart deployment snmp-splunk-connect-for-snmp-worker-poller -n sc4snmp\n</code></pre>"},{"location":"mib-request/#beta-use-mib-server-with-local-mibs","title":"Beta: use MIB server with local MIBs","text":"<p>From the <code>1.15.0</code> version of the MIB server, there is a way to use local MIB files. This may be useful when your MIB  files are proprietary, or you use SC4SNMP offline. This way, you can update necessary MIBs by yourself, without having to go through the MIB request procedure.</p> <p>In order to add your MIB files to the MIB server in standalone SC4SNMP installation:</p> <ol> <li>Create or choose a directory on the machine where SC4SNMP is installed, for example, <code>/home/user/local_mibs</code>.</li> <li>Create vendor directories inside. For example, if you have MIB files from <code>VENDOR1</code> and <code>VENDOR2</code>, create <code>/home/user/local_mibs/VENDOR1</code> and <code>/home/user/local_mibs/VENDOR2</code> and put files inside accordingly. Putting wrong  vendor names will not make compilation fail, this is more for the logging purposes. Segregating your files will make  troubleshooting easier.</li> <li>MIB files should be named the same as the contained MIB module. The MIB module name is specified at the beginning of the MIB file before <code>::= BEGIN</code> keyword.</li> <li>Add the following to the <code>values.yaml</code>:</li> </ol> <pre><code>mibserver:\n  localMibs:\n    pathToMibs: \"/home/user/local_mibs\"\n</code></pre> <p>To verify that the process of compilation was completed successfully, check the mibserver logs using the following command:</p> <pre><code>microk8s kubectl logs -f deployments/snmp-mibserver -n sc4snmp\n</code></pre> <p>This creates a Kubernetes pvc with MIB files inside and maps it to the MIB server pod. Also, you can change the storageClass and size of persistence according to the <code>mibserver</code> schema, see https://github.com/pysnmp/mibs/blob/main/charts/mibserver/values.yaml. The default persistence size is 1 Gibibyte, so consider reducing or expanding it to the amount you actually need. Whenever you add new MIB files, rollout restart MIB server pods to compile them again, using the following command:</p> <pre><code>microk8s kubectl rollout restart deployment snmp-mibserver -n sc4snmp\n</code></pre> <p>For a multi-node Kubernetes installation, create pvc beforehand, copy files onto it, and add it to the MIB server using <code>persistence.existingClaim</code>. If you go with the <code>localMibs.pathToMibs</code> solution for a multi-node installation (with <code>nodeSelector</code> set up to schedule MIB server pods on the same node where the MIB files are), when the Node with the mapped hostPath fails, you will have to access the MIB files on another node.</p>"},{"location":"releases/","title":"Releases","text":""},{"location":"releases/#base-information","title":"Base Information","text":""},{"location":"releases/#known-issues","title":"Known Issues","text":"<p>The list of open known issues is available under Known issue link.</p>"},{"location":"releases/#open-issues-to-the-product","title":"Open issues to the product","text":"<p>To open an issue for Splunk Connect for SNMP, go to the github SC4SNMP project and open an issue.   </p>"},{"location":"releases/#releases","title":"Releases","text":"<p>To check Splunk Connect for SNMP releases, see: SC4SNMP Releases</p>"},{"location":"security/","title":"Security","text":""},{"location":"security/#security-considerations","title":"Security Considerations","text":"<p>The SC4SNMP solution implements SNMP in a compatible mode for current and legacy network device gear. SNMP is a protocol widely considered to be risky and requires threat mitigation at the network level.</p> <ul> <li>Do not expose SNMP endpoints to untrusted connections such as the internet or general LAN network of a typical enterprise.</li> <li>Do not allow SNMPv1 or SNMPv2 connections to cross a network zone where a man in the middle interception is possible.</li> <li>Many SNMPv3 devices rely on insecure cryptography including DES, MD5, and SHA. Do not assume that SNMPv3 devices and connections are secure by default.</li> <li>When possible use SNMPv3 with the most secure mutually supported protocol options. </li> <li>The default IP of each node should be considered a management interface and should be protected from network access by an untrusted device by a hardware or software firewall. When possible the IP allocated for SNMP communication should not be shared by the management interface.</li> </ul>"},{"location":"small-environment/","title":"Lightweight installation","text":""},{"location":"small-environment/#lightweight-sc4snmp-installation","title":"Lightweight SC4SNMP installation","text":"<p>SC4SNMP can be successfully installed in small environments with 2 CPUs and 4 GB of memory. However, Splunk OpenTelemetry Collector for Kubernetes cannot be installed in a small environment along with SC4SNMP. Additionally, the <code>resources</code> limits must be set for Kubernetes pods or Docker containers. See the example of <code>values.yaml</code> with the appropriate resources here.</p> <p>For the rest of installation process you can follow the instructions from Getting started section with the deployment of your choice.</p> <p>Keep in mind that a lightweight instance of SC4SNMP will not be able to poll from many devices and may experience delays  if there is frequent polling.</p>"},{"location":"architecture/design/","title":"High-level design","text":""},{"location":"architecture/design/#architecture","title":"Architecture","text":"<p>SC4SNMP is deployed using a Kubernetes distribution, typically MicroK8s, that is designed to be a low-touch experience for integration with sensitive edge network devices. It will typically be deployed in the same network management zone as the monitored devices and separated from Splunk by an existing firewall.</p> <p></p>"},{"location":"architecture/design/#high-level-design","title":"High-level Design","text":"<p>SC4SNMP has two main purposes. The first one is used to collect SNMP data from network  devices according to planned schedules and the second one is responsible for listening to SNMP traps.</p> <p></p> <p>Diagram above present high level architecture of Splunk Connector for SNMP, it contains following components:</p> <ul> <li>UI - user interface for configuring the SC4SNMP profiles, groups, and inventory. It is applying changes to    SC4SNMP by creating the inventory job.</li> <li>Poller - responsible for getting selected data from SNMP agents in set periods of time. Celery is used for    planning the schedules and executing the incoming tasks, signaled from Redis as message broker.</li> <li>Trap - responsible for listening and receiving trap notifications from SNMP agents. The listener is always    waiting for the messages coming on the specified port and passing them to the trap worker for further    processing.</li> <li>MIB Server - responsible for serving MIBs to SNMP Workers and translating oids to varbinds.</li> <li>MongoDB - used for storing configuration and state of the SC4SNMP.</li> <li>Inventory - job used for updating the information about SC4SNMP configuration. It is run after every update to    the <code>values.yaml</code> file if polling is enabled.</li> <li>Sender - responsible for sending data received from poller or trap workers to the Splunk HEC or OTel (SignalFx).</li> </ul>"},{"location":"architecture/planning/","title":"Infrastructure Planning","text":""},{"location":"architecture/planning/#planning","title":"Planning","text":"<p>Splunk Connect for SNMP (SC4SNMP) is a solution that allows the customer to get data from network devices and appliances when a more feature-complete solution, such as the Splunk Universal Forwarder, is not available.</p>"},{"location":"architecture/planning/#requirements","title":"Requirements","text":"<ul> <li>A supported deployment of MicroK8s or Docker Compose</li> <li>16 Core/32 threads x64 architecture server or virtual machine (single instance)     12 GB ram</li> <li>HA Requires 3 or more instances (odd numbers) 8 core/16 thread 16 GB     ram</li> <li>50 GB root mount</li> <li>HTTP access (non-proxy) allowed for the HTTP(s) connection from     SC4SNMP to the Splunk destination.</li> <li>Splunk Enterprise/Cloud 8.x or newer and/or Splunk Infrastructure Monitoring     (SignalFx) </li> </ul>"},{"location":"architecture/planning/#planning-infrastructure","title":"Planning Infrastructure","text":"<p>When planning infrastructure for Splunk Connect for SNMP, (SC4SNMP), remember the following limitations: </p> <p>A single installation of Splunk Connect for SNMP (SC4SNMP) on a machine with 16 Core/32 threads x64 and 64 GB RAM will be able to handle up to 1500 SNMP TRAPs per second.</p> <p>A single installation of Splunk Connect for SNMP (SC4SNMP) on a machine with 16 Core/32 threads x64 and 64 GB RAM is able  to handle up to 2750 SNMP varbinds per second. As for events per second that are visible in Splunk, a single SC4SNMP event  can contain more than one varbind inside, which is an automatic grouping feature. For example, the network interface would  be grouped into one event, with varbinds grouped together to describe the same thing. That is why, depending on the configuration, the number of events per second may vary.</p>"},{"location":"dockercompose/1-install-docker/","title":"Install Docker","text":""},{"location":"dockercompose/1-install-docker/#install-docker","title":"Install Docker","text":"<p>To install <code>Docker</code> in your environment follow steps from the <code>Install using the apt repository</code> section from the Docker documentation. Install the  latest version.</p>"},{"location":"dockercompose/10-enable-ipv6/","title":"Enable IPv6","text":""},{"location":"dockercompose/10-enable-ipv6/#enabling-ipv6-for-sc4snmp","title":"Enabling IPv6 for SC4SNMP","text":"<p>Default installation of SC4SNMP does not support polling or receiving trap notifications from IPv6 addresses.  To enable IPv6, follow instruction below.</p>"},{"location":"dockercompose/10-enable-ipv6/#docker","title":"Docker","text":"<p>Older versions of Docker do not support IPv6 or have know issues with IPv6 configuration.  To avoid any problem with configuring the network, it is recommended to use the latest version of Docker. </p> <p>To enable IPv6 for SC4SNMP, set <code>IPv6_ENABLED</code> variable to <code>true</code> in <code>.env</code> file. The default subnet used for SC4SNMP network in docker is <code>fd02::/64</code>, this configuration can be changed in <code>.env</code> file under <code>Network configuration</code> section. In case of configuring more than one IPv4 and IPv6 subnet in IPAM, <code>networks</code> section of <code>docker-compose.yaml</code> should be edited.</p> <p>Default trap port for notifications for IPv6 is <code>2163</code>. You can change it to any other port if needed with <code>IPv6_TRAPS_PORT</code> parameter in <code>.env</code> file. The IPv6 port and IPv4 port cannot be the same.</p> <p>For more information about IPv6 networking in docker, you can check the official Docker documentation.</p>"},{"location":"dockercompose/2-download-package/","title":"Download package","text":""},{"location":"dockercompose/2-download-package/#download-package-with-docker-compose-files","title":"Download package with docker compose files","text":""},{"location":"dockercompose/2-download-package/#downloading-a-package","title":"Downloading a package","text":"<p>Package with docker compose configuration files (<code>docker_compose.zip</code>) can be downloaded from the Github release.</p>"},{"location":"dockercompose/2-download-package/#configuration","title":"Configuration","text":"<p>To configure the deployment, follow the instructions in Inventory configuration,  Scheduler configuration, Traps configuration, .env file configuration, SNMPv3 secrets.</p>"},{"location":"dockercompose/2-download-package/#deploying-the-app","title":"Deploying the app","text":"<p>After configuration, application can be deployed by running the following command inside the <code>docker_compose</code> directory:</p> <pre><code>sudo docker compose up -d\n</code></pre> <p>Info</p> <p>The installation process changed from version 1.12.1. For lower version refer to the corresponding  documentation. </p> <p>The same command can be run to apply any updated configuration changes.</p>"},{"location":"dockercompose/2-download-package/#uninstall-the-app","title":"Uninstall the app","text":"<p>To uninstall the app, run the following command inside the <code>docker_compose</code> directory:</p> <pre><code>sudo docker compose  down\n</code></pre>"},{"location":"dockercompose/3-inventory-configuration/","title":"Inventory configuration","text":""},{"location":"dockercompose/3-inventory-configuration/#inventory-configuration","title":"Inventory configuration","text":"<p>Inventory configuration is stored in the <code>inventory.csv</code> file. Structure of this file is the same as the one of the  <code>poller.inventory</code> section in <code>values.yaml</code> file. Documentation of this section can be found in configure inventory.</p>"},{"location":"dockercompose/3-inventory-configuration/#example-of-the-configuration","title":"Example of the configuration","text":"<pre><code>address,port,version,community,secret,securityEngine,walk_interval,profiles,smart_profiles,delete\n0.0.0.0,161,2c,public,,,1800,small_walk;test_profile,t,\nmy_group,161,3,,my_secret,,1800,single_metric,t,\n</code></pre>"},{"location":"dockercompose/4-scheduler-configuration/","title":"Scheduler configuration","text":""},{"location":"dockercompose/4-scheduler-configuration/#scheduler-configuration","title":"Scheduler configuration","text":"<p>Scheduler configuration is stored in the <code>scheduler-config.yaml</code> file. This file has the following sections:</p> <pre><code>communities:\n  2c:\n    public:\n      communityIndex:\n      contextEngineId:\n      contextName:\n      tag:\n      securityName:\ncustomTranslations:\nprofiles:\ngroups:\n</code></pre> <ul> <li><code>communities</code>: communities used for version <code>1</code> and <code>2c</code> of the <code>snmp</code>. The default one is <code>public</code>.</li> <li><code>customTranslations</code>: configuration of the custom translations. Configuration of this section looks the same as in the <code>values.yaml</code> in <code>scheduler.customTranslations</code> section, which can be checked in the documentation of custom translations.</li> <li><code>profiles</code>: configuration of the profiles. Configuration of this section looks the same as in the <code>values.yaml</code> in <code>scheduler.profiles</code> section, which can be checked in the documentation of profiles configuration.</li> <li><code>groups</code>: configuration of the groups. Configuration of this section looks the same as in the <code>values.yaml</code> in <code>scheduler.groups</code> section, which can be checked in the documentation of groups configuration.</li> </ul>"},{"location":"dockercompose/4-scheduler-configuration/#example-of-the-configuration","title":"Example of the configuration","text":"<pre><code>communities:\n  2c:\n    public:\n      communityIndex:\n      contextEngineId:\n      contextName:\n      tag:\n      securityName:\ncustomTranslations:\n  IF-MIB:\n    ifInDiscards: myCustomName1\n    ifOutErrors: myCustomName2\n  SNMPv2-MIB:\n    sysDescr: myCustomName3\nprofiles:\n  small_walk:\n    condition:\n      type: \"walk\"\n    varBinds:\n      - [ 'IP-MIB' ]\n      - [ 'IF-MIB' ]\n      - [ 'TCP-MIB' ]\n      - [ 'UDP-MIB' ]\n  multiple_conditions:\n    frequency: 10\n    conditions:\n      - field: IF-MIB.ifIndex\n        operation: \"gt\"\n        value: 1\n      - field: IF-MIB.ifDescr\n        operation: \"in\"\n        value:\n          - \"eth0\"\n          - \"test value\"\n    varBinds:\n      - [ 'IF-MIB', 'ifOutDiscards' ]\ngroups:\n  group1:\n    - address: 18.116.10.255\n      port: 1163\n</code></pre>"},{"location":"dockercompose/5-traps-configuration/","title":"Traps configuration","text":"<p>Scheduler configuration is stored in the <code>traps-config.yaml</code> file. This file has the following sections:</p> <pre><code>communities:\n  2c:\n    public:\n      communityIndex:\n      contextEngineId:\n      contextName:\n      tag:\n      securityName:\nusernameSecrets: []\n</code></pre> <ul> <li><code>communities</code>: communities used for version <code>1</code> and <code>2c</code> of the snmp. The default one is <code>public</code>.</li> <li><code>usernameSecrets</code>: names of the secrets configured in docker used for <code>snmp v3</code> traps .</li> </ul>"},{"location":"dockercompose/5-traps-configuration/#example-of-the-configuration","title":"Example of the configuration","text":"<pre><code>communities:\n  2c:\n    public:\n      communityIndex:\n      contextEngineId:\n      contextName:\n      tag:\n      securityName:\nusernameSecrets: \n  - my_secret\n</code></pre>"},{"location":"dockercompose/6-env-file-configuration/","title":".env file configuration","text":""},{"location":"dockercompose/6-env-file-configuration/#env-file-configuration","title":".env file configuration","text":"<p>Inside the directory with the docker compose files, there is a <code>.env</code>. Variables in it can be divided into few sections.</p>"},{"location":"dockercompose/6-env-file-configuration/#deployment","title":"Deployment","text":"Variable Description <code>SC4SNMP_IMAGE</code> The registry and name of the SC4SNMP image to pull <code>SC4SNMP_TAG</code> SC4SNMP image tag to pull <code>SCHEDULER_CONFIG_FILE_ABSOLUTE_PATH</code> Absolute path to scheduler-config.yaml file <code>TRAPS_CONFIG_FILE_ABSOLUTE_PATH</code> Absolute path to traps-config.yaml file <code>INVENTORY_FILE_ABSOLUTE_PATH</code> Absolute path to inventory.csv file <code>COREFILE_ABS_PATH</code> Absolute path to Corefile used by coreDNS. Default Corefile can be found inside the <code>docker_compose</code> <code>SC4SNMP_VERSION</code> Version of SC4SNMP"},{"location":"dockercompose/6-env-file-configuration/#network-configuration","title":"Network configuration","text":"Variable Description <code>COREDNS_ADDRESS</code> IP address of the coredns inside docker network. Should not be changed <code>COREDNS_ADDRESS_IPv6</code> IPv6 address of the coredns inside docker network. Should not be changed <code>IPv6_ENABLED</code> Enable receiving traps and polling from IPv6 devices <code>IPAM_SUBNET</code> Subnet in CIDR format that represents a network segment <code>IPAM_GATEWAY</code> IPv4 gateway for the master subnet <code>IPAM_SUBNET_IPv6</code> Subnet in CIDR format that represents a network segment for IPv6 <code>IPAM_GATEWAY_IPv6</code> IPv6 gateway for the master subnet <p>Info</p> <p>In case of configuring more than one IPv4 and IPv6 subnet in IPAM, docker compose file should be edited.</p>"},{"location":"dockercompose/6-env-file-configuration/#images-of-dependencies","title":"Images of dependencies","text":"Variable Description <code>COREDNS_IMAGE</code> Registry and name of Coredns image <code>COREDNS_TAG</code> Coredns image tag to pull <code>MIBSERVER_IMAGE</code> Registry and name of Mibserver image <code>MIBSERVER_TAG</code> Mibserver image tag to pull <code>REDIS_IMAGE</code> Registry and name of Redis image <code>REDIS_TAG</code> Redis image tag to pull <code>MONGO_IMAGE</code> Registry and name of MongoDB image <code>MONGO_TAG</code> MongoDB image tag to pull"},{"location":"dockercompose/6-env-file-configuration/#splunk-instance","title":"Splunk instance","text":"Variable Description <code>SPLUNK_HEC_HOST</code> IP address or a domain name of a Splunk instance to send data to <code>SPLUNK_HEC_PROTOCOL</code> The protocol of the HEC endpoint: <code>https</code> or <code>http</code> <code>SPLUNK_HEC_PORT</code> The port of the HEC endpoint <code>SPLUNK_HEC_TOKEN</code> Splunk HTTP Event Collector token <code>SPLUNK_HEC_INSECURESSL</code> Whether to skip checking the certificate of the HEC endpoint when sending data over HTTPS <code>SPLUNK_SOURCETYPE_TRAPS</code> Splunk sourcetype for trap events <code>SPLUNK_SOURCETYPE_POLLING_EVENTS</code> Splunk sourcetype for non-metric polling events <code>SPLUNK_SOURCETYPE_POLLING_METRICS</code> Splunk sourcetype for metric polling events <code>SPLUNK_HEC_INDEX_EVENTS</code> Name of the Splunk event index <code>SPLUNK_HEC_INDEX_METRICS</code> Name of the Splunk metrics index <code>SPLUNK_HEC_PATH</code> Path for the HEC endpoint <code>SPLUNK_AGGREGATE_TRAPS_EVENTS</code> When set to true makes traps events collected as one event inside splunk <code>IGNORE_EMPTY_VARBINDS</code> Details can be found in empty snmp response message issue <code>SPLUNK_LOG_INDEX</code> Event index in Splunk where logs from docker containers would be sent"},{"location":"dockercompose/6-env-file-configuration/#workers","title":"Workers","text":""},{"location":"dockercompose/6-env-file-configuration/#general","title":"General","text":"Variable Description <code>WALK_RETRY_MAX_INTERVAL</code> Maximum time interval between walk attempts <code>WALK_MAX_RETRIES</code> Maximum number of walk retries <code>METRICS_INDEXING_ENABLED</code> Details can be found in append oid index part to the metrics <code>POLL_BASE_PROFILES</code> Enable polling base profiles (with IF-MIB and SNMPv2-MIB) <code>IGNORE_NOT_INCREASING_OIDS</code> Ignoring <code>occurred: OID not increasing</code> issues for hosts specified in the array, ex: IGNORE_NOT_INCREASING_OIDS=127.0.0.1:164,127.0.0.6 <code>WORKER_LOG_LEVEL</code> Logging level of the workers, possible options: DEBUG, INFO, WARNING, ERROR, CRITICAL, or FATAL <code>UDP_CONNECTION_TIMEOUT</code> Timeout in seconds for SNMP operations <code>MAX_OID_TO_PROCESS</code> Sometimes SNMP Agent cannot accept more than X OIDs per once, so if the error \u201cTooBig\u201d is visible in logs, decrease the number of MAX_OID_TO_PROCESS"},{"location":"dockercompose/6-env-file-configuration/#worker-poller","title":"Worker Poller","text":"Variable Description <code>WORKER_POLLER_CONCURRENCY</code> Minimum number of threads in the poller container <code>PREFETCH_POLLER_COUNT</code> How many tasks are consumed from the queue at once in the poller container <code>WORKER_POLLER_REPLICAS</code> Number of docker replicas of worker poller container <code>WORKER_POLLER_CPU_LIMIT</code> Limit of cpu that worker poller container can use <code>WORKER_POLLER_MEMORY_LIMIT</code> Limit of memory that worker poller container can use <code>WORKER_POLLER_CPU_RESERVATIONS</code> Dedicated cpu resources for worker poller container <code>WORKER_POLLER_MEMORY_RESERVATIONS</code> Dedicated memory resources for worker poller container"},{"location":"dockercompose/6-env-file-configuration/#worker-sender","title":"Worker Sender","text":"Variable Description <code>WORKER_SENDER_CONCURRENCY</code> Minimum number of threads in the sender container <code>PREFETCH_SENDER_COUNT</code> How many tasks are consumed from the queue at once in the sender container <code>WORKER_SENDER_REPLICAS</code> Number of docker replicas of worker sender container <code>WORKER_SENDER_CPU_LIMIT</code> Limit of cpu that worker sender container can use <code>WORKER_SENDER_MEMORY_LIMIT</code> Limit of memory that worker sender container can use <code>WORKER_SENDER_CPU_RESERVATIONS</code> Dedicated cpu resources for worker sender container <code>WORKER_SENDER_MEMORY_RESERVATIONS</code> Dedicated memory resources for worker sender container"},{"location":"dockercompose/6-env-file-configuration/#worker-trap","title":"Worker Trap","text":"Variable Description <code>WORKER_TRAP_CONCURRENCY</code> Minimum number of threads in the trap container <code>PREFETCH_TRAP_COUNT</code> How many tasks are consumed from the queue at once in the trap container <code>RESOLVE_TRAP_ADDRESS</code> Use reverse dns lookup for trap IP address and send the hostname to Splunk <code>MAX_DNS_CACHE_SIZE_TRAPS</code> If RESOLVE_TRAP_ADDRESS is set to true, this is the maximum number of records in cache <code>TTL_DNS_CACHE_TRAPS</code> If RESOLVE_TRAP_ADDRESS is set to true, this is the time to live of the cached record in seconds <code>WORKER_TRAP_REPLICAS</code> Number of docker replicas of worker trap container <code>WORKER_TRAP_CPU_LIMIT</code> Limit of cpu that worker trap container can use <code>WORKER_TRAP_MEMORY_LIMIT</code> Limit of memory that worker trap container can use <code>WORKER_TRAP_CPU_RESERVATIONS</code> Dedicated cpu resources for worker trap container <code>WORKER_TRAP_MEMORY_RESERVATIONS</code> Dedicated memory resources for worker trap container"},{"location":"dockercompose/6-env-file-configuration/#inventory","title":"Inventory","text":"Variable Description <code>INVENTORY_LOG_LEVEL</code> Logging level of the inventory, possible options: DEBUG, INFO, WARNING, ERROR, CRITICAL, or FATAL <code>CHAIN_OF_TASKS_EXPIRY_TIME</code> Tasks expirations time in seconds"},{"location":"dockercompose/6-env-file-configuration/#traps","title":"Traps","text":"Variable Description <code>SNMP_V3_SECURITY_ENGINE_ID</code> SNMPv3 TRAPs require the configuration SNMP Engine ID of the TRAP sending application for the USM users table of the TRAP receiving application for each USM user, for example: SNMP_V3_SECURITY_ENGINE_ID=80003a8c04,aab123456 <code>TRAPS_PORT</code> External port exposed for traps server <code>IPv6_TRAPS_PORT</code> External port exposed for traps server for IPv6"},{"location":"dockercompose/6-env-file-configuration/#scheduler","title":"Scheduler","text":"Variable Description <code>SCHEDULER_LOG_LEVEL</code> Logging level of the scheduler, possible options: DEBUG, INFO, WARNING, ERROR, CRITICAL, or FATAL"},{"location":"dockercompose/7-snmpv3-secrets/","title":"SNMPv3 secrets configuration","text":""},{"location":"dockercompose/7-snmpv3-secrets/#snmpv3-secrets","title":"SNMPv3 secrets","text":"<p>Creating a secret requires updating configuration of several docker compose files. To simplify this process, inside the  <code>docker_compose</code> package there is a <code>manage_secrets.py</code> file which will automatically manage secrets.</p>"},{"location":"dockercompose/7-snmpv3-secrets/#prerequisites","title":"Prerequisites","text":"<p>Running script requires installation of <code>ruamel.yaml</code> package for python. It can be done with command: </p><pre><code>pip3 install ruamel.yaml\n</code></pre>"},{"location":"dockercompose/7-snmpv3-secrets/#creating-a-new-secret","title":"Creating a new secret","text":"<p>To create a new secret, <code>manage_secrets.py</code> must be run with the following flags:</p> Flag Description <code>--secret_name</code> New secret name <code>--path_to_compose</code> Absolute path to directory with docker compose files <code>--worker_poller</code> [OPTIONAL] Add new secrets to worker poller. Default value is set to \u2018true\u2019. <code>--traps</code> [OPTIONAL] Add new secrets to traps server. Default value is set to \u2018true\u2019. <code>--userName</code> SNMPv3 userName <code>--privProtocol</code> SNMPv3 privProtocol <code>--privKey</code> SNMPv3 privKey <code>--authProtocol</code> SNMPv3 authProtocol <code>--authKey</code> SNMPv3 authKey <code>--contextEngineId</code> [OPTIONAL] SNMPv3 engine id <p>This script, apart from updating configuration files, creates environmental variables with values of the secret at the  end of the <code>.env</code> file in the <code>docker_compose</code> directory. To apply those secrets run the  <code>sudo docker compose up -d</code> command inside the <code>docker_compose</code> directory. After execution of the command, plain text secrets  from the <code>.env</code> file can be deleted. </p> <p>NOTE: In case of any changes in <code>.env</code>, the secrets must be recreated by deleting any  previously existing secrets and creating them once again. Changes in <code>.env</code> include creating new secrets.</p>"},{"location":"dockercompose/7-snmpv3-secrets/#example-of-creating-a-secret","title":"Example of creating a secret","text":"<pre><code>python3 &lt;path_to_manage_secrets.py&gt; --path_to_compose &lt;path_to_compose&gt; \\\n--secret_name my_secret \\\n--userName r-wuser \\\n--privProtocol AES \\\n--privKey admin1234 \\\n--authProtocol SHA \\\n--authKey admin1234 \\\n--contextEngineId 090807060504037\n</code></pre> <p>Inside <code>docker_compose</code> directory run:</p> <pre><code>sudo docker compose up -d\n</code></pre> <p>Now, the following lines from the <code>.env</code> can be deleted:</p> <pre><code>my_secret_userName=r-wuser\nmy_secret_privProtocol=AES\nmy_secret_privKey=admin1234\nmy_secret_authProtocol=SHA\nmy_secret_authKey=admin1234\nmy_secret_contextEngineId=090807060504037\n</code></pre>"},{"location":"dockercompose/7-snmpv3-secrets/#deleting-a-secret","title":"Deleting a secret","text":"<p>To delete a secret, <code>manage_secrets.py</code> must be run with the following flags:</p> Flag Description <code>--secret_name</code> Secret name <code>--path_to_compose</code> Absolute path to directory with docker compose files <code>--delete</code> Set this flag to true to delete the secret <p>This will delete the secret with a given name from all docker compose files. If this secret hasn\u2019t been deleted from <code>.env</code>  file, it will be removed from there. </p>"},{"location":"dockercompose/7-snmpv3-secrets/#example-of-deleting-a-secret","title":"Example of deleting a secret","text":"<pre><code>python3 &lt;path_to_manage_secrets.py&gt; --path_to_compose &lt;path_to_compose&gt; \\\n--secret_name my_secret \\\n--delete true \n</code></pre>"},{"location":"dockercompose/8-offline-installation/","title":"Offline installation","text":""},{"location":"dockercompose/8-offline-installation/#offline-installation","title":"Offline installation","text":"<p>In order to install SC4SNMP using docker compose in the offline environment, several docker images must be imported.  These images can be found in <code>.env</code> file:</p> <ul> <li><code>SC4SNMP_IMAGE</code> and <code>SC4SNMP_TAG</code> in <code>Deployment configuration</code> section</li> <li><code>COREDNS_IMAGE</code> and <code>COREDNS_TAG</code> in <code>Dependencies images</code> section</li> <li><code>MIBSERVER_IMAGE</code> and <code>MIBSERVER_TAG</code> in <code>Dependencies images</code> section</li> <li><code>REDIS_IMAGE</code> and <code>REDIS_TAG</code> in <code>Dependencies images</code> section</li> <li><code>MONGO_IMAGE</code> and <code>MONGO_TAG</code> in <code>Dependencies images</code> section</li> </ul> <p>Following images must be downloaded in the online environment, saved to <code>.tar</code> archive and moved to the offline environment.</p>"},{"location":"dockercompose/8-offline-installation/#steps-required-to-install-necessary-images","title":"Steps required to install necessary images","text":"<p>Suppose that <code>.env</code> contains the following images:</p> <pre><code>SC4SNMP_IMAGE=ghcr.io/splunk/splunk-connect-for-snmp/container\nSC4SNMP_TAG=latest\n\nCOREDNS_IMAGE=coredns/coredns\nCOREDNS_TAG=1.11.1\n\nMIBSERVER_IMAGE=ghcr.io/pysnmp/mibs/container\nMIBSERVER_TAG=latest\n\nREDIS_IMAGE=docker.io/bitnami/redis\nREDIS_TAG=7.2.1-debian-11-r0\n\nMONGO_IMAGE=docker.io/bitnami/mongodb\nMONGO_TAG=6.0.9-debian-11-r5\n</code></pre> <p>They must be downloaded in the online environment by following commands:</p> <pre><code>docker pull ghcr.io/splunk/splunk-connect-for-snmp/container:latest\ndocker pull coredns/coredns:1.11.1\ndocker pull ghcr.io/pysnmp/mibs/container:latest\ndocker pull docker.io/bitnami/redis:7.2.1-debian-11-r0\ndocker pull docker.io/bitnami/mongodb:6.0.9-debian-11-r5\n</code></pre> <p>Next step is to save them to <code>sc4snmp_offline_images.tar</code> archive: </p><pre><code>docker save ghcr.io/splunk/splunk-connect-for-snmp/container:latest \\\ncoredns/coredns:1.11.1 \\\nghcr.io/pysnmp/mibs/container:latest \\\ndocker.io/bitnami/redis:7.2.1-debian-11-r0 \\\ndocker.io/bitnami/mongodb:6.0.9-debian-11-r5 &gt; sc4snmp_offline_images.tar\n</code></pre> <p>After moving <code>sc4snmp_offline_images.tar</code> archive to the offline environment, images can be loaded to docker: </p><pre><code>docker load --input sc4snmp_offline_images.tar\n</code></pre>"},{"location":"dockercompose/9-splunk-logging/","title":"Sending logs to Splunk","text":""},{"location":"dockercompose/9-splunk-logging/#logging","title":"Logging","text":"<p>The default configuration of docker compose is not sending the logs to Splunk. Container logs can be accessed with command: </p><pre><code>docker logs &lt;container_name/id&gt;\n</code></pre> <p>Creating logs requires updating configuration of several docker compose files. To simplify this process, inside the  <code>docker_compose</code> package there is a <code>manage_logs.py</code> file which will automatically manage logs.</p>"},{"location":"dockercompose/9-splunk-logging/#prerequisites","title":"Prerequisites","text":"<p>Running script requires installation of <code>ruamel.yaml</code> package for python. It can be done with command: </p><pre><code>pip3 install ruamel.yaml\n</code></pre> <p>The following parameters have to be configured in <code>.env</code> file: <code>SPLUNK_HEC_TOKEN</code>, <code>SPLUNK_HEC_PROTOCOL</code>, <code>SPLUNK_HEC_HOST</code>, <code>SPLUNK_HEC_PORT</code>, <code>SPLUNK_LOG_INDEX</code>, <code>SPLUNK_HEC_INSECURESSL</code>.</p> <p>More about <code>.env</code> configuration can be found in .env file configuration.</p>"},{"location":"dockercompose/9-splunk-logging/#enabling-logging","title":"Enabling logging","text":"<p>To enable a logging <code>manage_logs.py</code> must be run with the following flags:</p> Flag Description <code>-e</code>, <code>--enable_logs</code> Flag enabling the logs <code>-p</code>, <code>--path_to_compose</code> Absolute path to directory with docker compose files <p>Example of enabling logs: </p><pre><code>python3 manage_logs.py --path_to_compose /home/ubuntu/docker_compose --enable_logs\n</code></pre> <p>The script will add required configuration for logging under services in docker compose files. To apply the changes run the:  </p><pre><code>sudo docker compose up -d\n</code></pre> command inside the <code>docker_compose</code> directory."},{"location":"dockercompose/9-splunk-logging/#disabling-the-logs","title":"Disabling the logs","text":"<p>To disable logs <code>manage_logs.py</code> must be run with the following flags:</p> Flag Description <code>-d</code>, <code>--disable_logs</code> Flag disabling the logs <code>-p</code>, <code>--path_to_compose</code> Absolute path to directory with docker compose files <p>Running the disable command will replace the <code>logging.driver</code> section with default docker driver <code>json-file</code>. </p> <p>Example of disabling logs: </p><pre><code>python3 manage_logs.py --path_to_compose /home/ubuntu/docker_compose --disable_logs\n</code></pre> <p>To apply the changes run the: </p><pre><code>sudo docker compose up -d\n</code></pre> command inside the <code>docker_compose</code> directory. <p>After that the logs can be reached with <code>docker logs</code> command.</p>"},{"location":"microk8s/enable-ipv6/","title":"Enable IPv6","text":""},{"location":"microk8s/enable-ipv6/#enabling-ipv6-for-sc4snmp","title":"Enabling IPv6 for SC4SNMP","text":"<p>Default installation of SC4SNMP does not support polling or receiving trap notifications from IPv6 addresses. To enable IPv6, follow instruction below.</p>"},{"location":"microk8s/enable-ipv6/#microk8s","title":"Microk8s","text":"<p>To configure dual-stack network on microk8s follow instructions at Microk8s page. After completing the steps, you can follow the instruction at Microk8s installation on Ubuntu  to install microk8s.</p>"},{"location":"microk8s/enable-ipv6/#calico","title":"Calico","text":"<p>The default CNI used for microk8s is Calico. For pods to be able to reach internet over IPv6, you need to enable  the <code>natOutgoing</code> parameter in ipv6 ip pool configuration from calico. To set it create the yaml file with the following content: </p><pre><code># calico-ippool.yaml\n---\napiVersion: crd.projectcalico.org/v1\nkind: IPPool\nmetadata:\n  name: default-ipv6-ippool\nspec:\n  natOutgoing: true\n</code></pre> You can check with command <code>microk8s kubectl get ippools -n kube-system</code> the default name of the ip pool for IPv6.  If it differs from <code>default-ipv6-ippool</code> you need to change the name in the yaml file. Then apply the configuration with the following command: <pre><code>microk8s kubectl apply -f calico-ippool.yaml\n</code></pre> <p>After those changes you can restart the microk8s for the changes to be applied with the following commands: </p><pre><code>microk8s stop\nmicrok8s start\n</code></pre>"},{"location":"microk8s/enable-ipv6/#metallb","title":"Metallb","text":"<p>As of version <code>1.30</code> of microk8s, Metallb add-on does not support passing the IPv6 addresses in enable command. To  add the IPv6 addresses to your Metallb configuration, you can prepare the yaml file with configuration like below: </p><pre><code># addresspool.yaml\n---\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: default-addresspool\n  namespace: metallb-system\nspec: \n  addresses:\n  - 1.1.1.1/32\n  - 2001:0db8:ac10:fe01:0000:0000:0000:0001/128\n</code></pre> You can check with command <code>microk8s kubectl get ipaddresspool -n metallb-system</code> the default name of the ip address pool created in metallb. If it differs from <code>default-addresspool</code> you need to change the name in the yaml file. You can add the single ip or subnets for both IPv4 and IPv6 under <code>spec.addresses</code> section. After preparing the yaml file, apply the configuration with the following command: <pre><code>microk8s kubectl apply -f addresspool.yaml\n</code></pre>"},{"location":"microk8s/enable-ipv6/#sc4snmp","title":"SC4SNMP","text":"<p>To configure traps to receive notification from IPv4 and IPv6 addresses, you need to add the following configuration to the <code>values.yaml</code> file: </p><pre><code>traps:\n  ipFamilyPolicy: RequireDualStack\n  ipFamilies: [\"IPv4\", \"IPv6\"]\n</code></pre> Default trap port for notifications for IPv6 is <code>2163</code>. You can change it to any other port if needed with <code>traps.service.ipv6Port</code> parameter. The IPv6 port and IPv4 port cannot be the same."},{"location":"microk8s/sc4snmp-installation/","title":"Install SC4SNMP","text":""},{"location":"microk8s/sc4snmp-installation/#sc4snmp-helm-installation","title":"SC4SNMP Helm installation","text":"<p>The basic installation and configuration process discussed in this section is typical  for single node non-HA deployments. It does not have resource requests and limits. See the mongo, redis, scheduler, worker, and traps configuration sections for guidance on production configuration.</p>"},{"location":"microk8s/sc4snmp-installation/#installing-splunk-connect-for-snmp-on-linux-redhat","title":"Installing Splunk Connect for SNMP on Linux RedHat","text":"<p>Installation of RedHat may be blocking ports required by microk8s. Installing microk8s on RedHat  requires checking to see if the firewall is not blocking any of the required microk8s ports. </p>"},{"location":"microk8s/sc4snmp-installation/#installation-process","title":"Installation process","text":""},{"location":"microk8s/sc4snmp-installation/#offline-installation","title":"Offline installation","text":"<p>For offline installation instructions see this page.</p>"},{"location":"microk8s/sc4snmp-installation/#online-installation","title":"Online installation","text":""},{"location":"microk8s/sc4snmp-installation/#add-sc4snmp-repository","title":"Add SC4SNMP repository","text":"<pre><code>microk8s helm3 repo add splunk-connect-for-snmp https://splunk.github.io/splunk-connect-for-snmp\nmicrok8s helm3 repo update\n</code></pre> Now the package should be visible in <code>helm3</code> search command result: <pre><code>microk8s helm3 search repo snmp\n</code></pre> Example output: <pre><code>NAME                                               CHART VERSION  APP VERSION    DESCRIPTION                           \nsplunk-connect-for-snmp/splunk-connect-for-snmp        1.0.0        1.0.0       A Helm chart for SNMP Connect for SNMP\n</code></pre>"},{"location":"microk8s/sc4snmp-installation/#download-and-modify-valuesyaml","title":"Download and modify values.yaml","text":"<p>The installation of SC4SNMP requires the creation of a <code>values.yaml</code> file, which serves as the configuration file. To configure this file, see the following steps: </p> <ol> <li>Review the basic configuration template.</li> <li>Review the examples to determine which areas require configuration.</li> <li>For more advanced configuration options, refer to the complete default values.yaml or download it directly from Helm using the command <code>microk8s helm3 show values splunk-connect-for-snmp/splunk-connect-for-snmp</code> </li> <li>In order to learn more about each of the configuration parts, check configuration section.</li> </ol> <p>It is recommended to start by completing the base template and gradually add additional configurations as needed.</p> <p>The <code>values.yaml</code> file is validated using <code>JSON schema</code> built into <code>helm chart</code> and inside the code. To ensure that your <code>values.yaml</code> follows formatting standards, you can use <code>yamllint</code>. In order to download <code>yamllint</code> refer to the installation instructions.  Then create <code>custom-config.yamllint</code> file and add the following configuration: </p><pre><code>extends: default\n\nrules:\n  line-length:\n    max: 80\n    level: warning\n</code></pre> Configuration above can be found in the <code>examples</code> directory in SC4SNMP GitHub repository. Next run <code>yamllint -c &lt;path to custom-config.yamllint&gt; &lt;path to values.yaml&gt;</code> command. Warnings can be ignored."},{"location":"microk8s/sc4snmp-installation/#install-sc4snmp","title":"Install SC4SNMP","text":"<p>After the <code>values.yaml</code> creation, you can proceed with the SC4SNMP installation:</p> <pre><code>microk8s helm3 install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <p>From now on, when editing SC4SNMP configuration, the configuration change must be inserted in the corresponding section of <code>values.yaml</code>. For more details see configuration section.</p> <p>Use the following command to propagate configuration changes: </p><pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre>"},{"location":"microk8s/sc4snmp-installation/#verification-of-the-deployment","title":"Verification of the deployment","text":"<p>In a few minutes, all pods should be up and running. It can be verified with:</p> <pre><code>microk8s kubectl get pods -n sc4snmp\n</code></pre> <p>Example output:</p> <pre><code>NAME                                                      READY   STATUS             RESTARTS      AGE\nsnmp-splunk-connect-for-snmp-scheduler-7ddbc8d75-bljsj        1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-worker-poller-57cd8f4665-9z9vx   1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-worker-sender-5c44cbb9c5-ppmb5   1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-worker-trap-549766d4-28qzh       1/1     Running   0          133m\nsnmp-mibserver-7f879c5b7c-hz9tz                               1/1     Running   0          133m\nsnmp-mongodb-869cc8586f-vvr9f                                 2/2     Running   0          133m\nsnmp-redis-master-0                                           1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-trap-78759bfc8b-79m6d            1/1     Running   0          99m\nsnmp-splunk-connect-for-snmp-inventory-mjccw                  0/1     Completed 0          6s\n</code></pre> <p>The output might vary depending on the configuration. In the above example, both polling and traps are configured,  and the data is being sent to Splunk.</p> <p>If you have <code>traps</code> configured, you should see <code>EXTERNAL-IP</code> in the <code>snmp-splunk-connect-for-snmp-trap</code> service. Check it using the following command:</p> <pre><code>microk8s kubectl get svc -n sc4snmp \n</code></pre> <p>Here is an example of the correct setup:</p> <pre><code>NAME                                TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE\nsnmp-redis-headless                 ClusterIP      None             &lt;none&gt;        6379/TCP        33h\nsnmp-mongodb                        ClusterIP      10.152.183.147   &lt;none&gt;        27017/TCP       33h\nsnmp-mibserver                      ClusterIP      10.152.183.253   &lt;none&gt;        80/TCP          33h\nsnmp-redis-master                   ClusterIP      10.152.183.135   &lt;none&gt;        6379/TCP        33h\nsnmp-mongodb-metrics                ClusterIP      10.152.183.217   &lt;none&gt;        9216/TCP        33h\nsnmp-splunk-connect-for-snmp-trap   LoadBalancer   10.152.183.33    10.202.9.21   162:30161/UDP   33h\n</code></pre> <p>If you see <code>&lt;pending&gt;</code> communicate instead of the IP address, that means you either provided the wrong IP address in <code>traps.loadBalancerIP</code> or there is something wrong with the <code>metallb</code> microk8s addon.</p> <p>In the following example, the default indexes are used, the metric data goes to <code>netmetrics</code>, and the events goes to <code>netops</code>.</p>"},{"location":"microk8s/sc4snmp-installation/#test-snmp-traps","title":"Test SNMP Traps","text":"<ol> <li>Simulate the event. On a Linux system, you can download <code>snmpd</code> package for its purpose and run:</li> </ol> <pre><code>apt update\napt-get install snmpd\nsnmptrap -v2c -c public EXTERNAL-IP 123 1.3.6.1.2.1.1.4 1.3.6.1.2.1.1.4 s test\n</code></pre> <p>Remember to replace the <code>EXTERNAL-IP</code> with the IP address of the <code>snmp-splunk-connect-for-snmp-trap</code> service from the previous list.</p> <ol> <li>After using the following command in the Splunk search box, you should see one event per trap command, with the host value of the test machine <code>EXTERNAL-IP</code> IP address: </li> </ol> <pre><code>index=\"netops\" sourcetype=\"sc4snmp:traps\"\n</code></pre>"},{"location":"microk8s/sc4snmp-installation/#test-snmp-poller","title":"Test SNMP Poller","text":"<ol> <li>To test SNMP poller, you can either use the device you already have, or configure snmpd on your Linux system.  Snmpd needs to be configured to listen on the external IP. To enable snmpd to listen to external IP, go to the <code>/etc/snmp/snmpd.conf</code> configuration file, and replace the IP address <code>10.0.101.22</code> with the server IP address where snmpd is configured: <code>agentaddress  10.0.101.22,127.0.0.1,[::1]</code>. Restart snmpd through the following execute command:</li> </ol> <pre><code>service snmpd stop\nservice snmpd start\n</code></pre> <ol> <li> <p>Configure SC4SNMP Poller to test and add the IP address which you want to poll. Add the configuration entry into the <code>values.yaml</code> file by  replacing the IP address <code>10.0.101.22</code> with the server IP address where the snmpd was configured. See the following: </p><pre><code>poller:\n  inventory: |\n    address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n    10.0.101.22,,2c,public,,,42000,,,\n</code></pre> </li> <li> <p>Load <code>values.yaml</code> file into SC4SNMP using the following command: </p> </li> </ol> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <ol> <li>Verify if the records appeared in Splunk using the following command: </li> </ol> <pre><code>index=\"netops\" sourcetype=\"sc4snmp:event\"\n</code></pre> <pre><code>| mpreview index=\"netmetrics\" | search sourcetype=\"sc4snmp:metric\"\n</code></pre> <p>NOTE: Before polling starts, SC4SNMP must perform the SNMP WALK process on the device. It is run the first time after configuring the new device, and then during the run time in every <code>walk_interval</code>.  Its purpose is to gather all the data and provide meaningful context for the polling records. For example, it might report that your device is so large that the walk takes too long, so the scope of walking needs to be limited. In such cases, enable the small walk. See walk takes too much time. When the walk finishes, events appear in Splunk.</p>"},{"location":"microk8s/sc4snmp-installation/#next-steps","title":"Next Steps","text":"<p>A good way to start with SC4SNMP polling is to follow the Step by Step guide for polling. Advanced configuration of polling is available in the Poller configuration section. The SNMP data format is explained in the SNMP data format section.</p> <p>For advanced trap configuration, see the Traps configuration section.</p>"},{"location":"microk8s/sc4snmp-installation/#uninstall-splunk-connect-for-snmp","title":"Uninstall Splunk Connect for SNMP","text":"<p>To uninstall SC4SNMP run the following commands:</p> <pre><code> microk8s helm3 uninstall snmp -n sc4snmp\n microk8s kubectl delete pvc --all -n sc4snmp\n</code></pre> <p>Example of pods terminating:</p> <pre><code>NAME                                                          READY   STATUS        RESTARTS        AGE\nsnmp-mibserver-bb8994c64-twk42                                1/1     Terminating   2 (5h21m ago)   46h\nsnmp-splunk-connect-for-snmp-worker-sender-7f5557678b-psj97   1/1     Terminating   1 (5h21m ago)   22h\nsnmp-splunk-connect-for-snmp-worker-trap-dfcc487c-lh2dl       1/1     Terminating   1 (5h21m ago)   22h\nsnmp-splunk-connect-for-snmp-worker-trap-dfcc487c-5z5sq       1/1     Terminating   1 (5h21m ago)   22h\nsnmp-splunk-connect-for-snmp-trap-684d57dc8d-722tv            1/1     Terminating   1 (5h21m ago)   22h\nsnmp-splunk-connect-for-snmp-trap-684d57dc8d-z68lb            1/1     Terminating   1 (5h21m ago)   22h\n</code></pre>"},{"location":"microk8s/sc4snmp-installation/#restart-splunk-connect-for-snmp","title":"Restart Splunk Connect for SNMP","text":"<p>First run the command to uninstall SC4SNMP, wait until all pods are removed, then use the command to install sc4snmp again.</p> <pre><code> microk8s helm3 uninstall snmp -n sc4snmp\n microk8s kubectl delete pvc --all -n sc4snmp\n microk8s helm3 install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre>"},{"location":"microk8s/sck-installation/","title":"Install Splunk OpenTelemetry Collector for Kubernetes","text":""},{"location":"microk8s/sck-installation/#splunk-opentelemetry-collector-for-kubernetes-installation","title":"Splunk OpenTelemetry Collector for Kubernetes installation","text":"<p>Splunk OpenTelemetry Collector for Kubernetes is not required for SC4SNMP installation. However, Splunk OpenTelemetry Collector for Kubernetes sends logs and metrics from a k8s cluster to a Splunk instance, which makes SC4SNMP easier to debug.  You can do the same using the <code>microk8s kubectl logs</code> command on instances you are interested in, but if you are not proficient in Kubernetes, Splunk OpenTelemetry Collector for Kubernetes is recommended. </p> <p>The following steps are sufficient for a Splunk OpenTelemetry Collector installation for the SC4SNMP project with Splunk Enterprise/Enterprise Cloud. In order to learn more about Splunk OpenTelemetry Collector, visit Splunk OpenTelemetry Collector.</p>"},{"location":"microk8s/sck-installation/#offline-installation","title":"Offline installation","text":"<p>For offline installation instructions see Splunk OpenTelemetry Collector for Kubernetes offline installation.</p>"},{"location":"microk8s/sck-installation/#add-splunk-opentelemetry-collector-repository-to-helm","title":"Add Splunk OpenTelemetry Collector repository to HELM","text":"<pre><code>microk8s helm3 repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart\n</code></pre>"},{"location":"microk8s/sck-installation/#install-splunk-opentelemetry-collector-with-helm-for-a-splunk-platform","title":"Install Splunk OpenTelemetry Collector with HELM for a Splunk Platform","text":"<p>In order to run Splunk OpenTelemetry Collector on your environment, replace <code>&lt;&gt;</code> variables based on the following description:  </p><pre><code>microk8s helm3 upgrade --install sck \\\n  --set=\"clusterName=&lt;cluster_name&gt;\" \\\n  --set=\"splunkPlatform.endpoint=&lt;splunk_endpoint&gt;\" \\\n  --set=\"splunkPlatform.insecureSkipVerify=&lt;insecure_skip_verify&gt;\" \\\n  --set=\"splunkPlatform.token=&lt;splunk_token&gt;\" \\\n  --set=\"logsEngine=otel\" \\\n  --set=\"splunkPlatform.metricsEnabled=true\" \\\n  --set=\"splunkPlatform.metricsIndex=em_metrics\" \\\n  --set=\"splunkPlatform.index=em_logs\" \\\n  splunk-otel-collector-chart/splunk-otel-collector\n</code></pre>"},{"location":"microk8s/sck-installation/#variables-description","title":"Variables description","text":"Placeholder Description Example splunk_endpoint host address of splunk instance https://endpoint.example.com:8088/services/collector insecure_skip_verify is insecure ssl allowed false splunk_token Splunk HTTP Event Collector token 450a69af-16a9-4f87-9628-c26f04ad3785 cluster_name name of the cluster my-cluster <p>See the following example of a correctly filled up command:  </p><pre><code>microk8s helm3 upgrade --install sck \\\n  --set=\"clusterName=my-cluster\" \\\n  --set=\"splunkPlatform.endpoint=https://endpoint.example.com/services/collector\" \\\n  --set=\"splunkPlatform.insecureSkipVerify=false\" \\\n  --set=\"splunkPlatform.token=4d22911c-18d9-4706-ae7b-dd1b976ca6f7\" \\\n  --set=\"splunkPlatform.metricsEnabled=true\" \\\n  --set=\"splunkPlatform.metricsIndex=em_metrics\" \\\n  --set=\"splunkPlatform.index=em_logs\" \\\n  splunk-otel-collector-chart/splunk-otel-collector\n</code></pre>"},{"location":"microk8s/sck-installation/#install-splunk-opentelemetry-collector-with-helm-for-splunk-observability-for-kubernetes","title":"Install Splunk OpenTelemetry Collector with HELM for Splunk Observability for Kubernetes","text":"<p>To run Splunk OpenTelemetry Collector on your environment, replace the <code>&lt;&gt;</code> variables based on the following description: </p> <pre><code>microk8s helm3 upgrade --install sck\n--set=\"clusterName=&lt;cluster_name&gt;\"\n--set=\"splunkObservability.realm=&lt;realm&gt;\"\n--set=\"splunkObservability.accessToken=&lt;token&gt;\"\n--set=\"splunkObservability.ingestUrl=&lt;ingest_url&gt;\"\n--set=\"splunkObservability.apiUrl=&lt;api_url&gt;\"\n--set=\"splunkObservability.metricsEnabled=true\"\n--set=\"splunkObservability.tracesEnabled=false\"\n--set=\"splunkObservability.logsEnabled=false\"\nsplunk-otel-collector-chart/splunk-otel-collector\n</code></pre>"},{"location":"microk8s/sck-installation/#variables-description_1","title":"Variables description","text":"Placeholder Description Example cluster_name name of the cluster my_cluster realm Realm obtained from the Splunk Observability Cloud environment us0 token Token obtained from the Splunk Observability Cloud environment BCwaJ_Ands4Xh7Nrg ingest_url Ingest URL from the Splunk Observability Cloud environment https://ingest..signalfx.com api_url API URL from the Splunk Observability Cloud environment https://api..signalfx.com <p>See the following example of a correctly filled up command:  </p><pre><code>microk8s helm3 upgrade --install sck\n--set=\"clusterName=my_cluster\"\n--set=\"splunkObservability.realm=us0\"\n--set=\"splunkObservability.accessToken=BCwaJ_Ands4Xh7Nrg\"\n--set=\"splunkObservability.ingestUrl=https://ingest..signalfx.com\"\n--set=\"splunkObservability.apiUrl=https://api..signalfx.com\"\n--set=\"splunkObservability.metricsEnabled=true\"\n--set=\"splunkObservability.tracesEnabled=false\"\n--set=\"splunkObservability.logsEnabled=false\"\nsplunk-otel-collector-chart/splunk-otel-collector\n</code></pre>"},{"location":"microk8s/splunk-requirements/","title":"Splunk Requirements","text":""},{"location":"microk8s/splunk-requirements/#prerequisites-for-the-splunk-connect-for-snmp","title":"Prerequisites for the Splunk Connect for SNMP","text":"<p>See the following prerequisites for the Splunk Connect for SNMP.</p>"},{"location":"microk8s/splunk-requirements/#requirements-for-splunk-enterprise-or-enterprise-cloud","title":"Requirements for Splunk Enterprise or Enterprise Cloud","text":"<ol> <li>Manually create the following indexes in Splunk:</li> </ol> <ul> <li>Indexes to store Splunk Connect for SNMP logs and metrics: <ul> <li>em_metrics (metrics type)</li> <li>em_logs (event type)</li> </ul> </li> <li>Destination indexes for forwarding SNMP data: <ul> <li>netmetrics (metrics type)</li> <li>netops (event type)</li> </ul> </li> </ul> <p>Note: <code>netmetrics</code> and <code>netops</code> are the default names of SC4SNMP indexes. You can use the index names of your choice and reference it in the <code>values.yaml</code> file later on. See SC4SNMP Parameters for details.</p> <ol> <li>Create or obtain a new Splunk HTTP Event Collector token and the correct HTTPS endpoint.</li> <li>Verify the token using curl. The endpoint must use a publicly trusted certificate authority.</li> <li>Use the shared IP address for SNMP traps. Simple and POC deployments will use the same IP address as the host server. For an HA deployment, use the management interface and the IP address of each cluster member. </li> <li>Obtain the IP address of an internal DNS server that can resolve the Splunk Endpoint.</li> </ol>"},{"location":"microk8s/splunk-requirements/#requirements-splunk-infrastructure-monitoring","title":"Requirements (Splunk Infrastructure Monitoring)","text":"<p>Obtain the following from your Splunk Observability Cloud environment:</p> <ol> <li>Realm</li> <li>Token</li> </ol>"},{"location":"microk8s/upgrade/","title":"Upgrade SC4SNMP","text":""},{"location":"microk8s/upgrade/#upgrading-sc4snmp","title":"Upgrading SC4SNMP","text":"<p>See the following to update SC4SNMP.</p>"},{"location":"microk8s/upgrade/#upgrade-to-the-latest-version","title":"Upgrade to the latest version","text":"<p>To upgrade SC4SNMP to the latest version, simply run the following command:</p> <pre><code>microk8s helm3 repo update\n</code></pre> <p>Afterwards, run:</p> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <p>SC4SNMP will be upgraded to the newest version. You can see the latest version after running the following command: </p><pre><code>microk8s helm3 search repo snmp\n</code></pre> <p>The following should be the output:</p> <pre><code>NAME                                            CHART VERSION   APP VERSION DESCRIPTION                           \nsplunk-connect-for-snmp/splunk-connect-for-snmp 1.6.2           1.6.2       A Helm chart for SNMP Connect for SNMP\n</code></pre> <p>In this case, the latest version is <code>1.6.2</code> and it will be installed after running the <code>helm3 upgrade</code> command.</p>"},{"location":"microk8s/upgrade/#upgrade-to-a-specific-version","title":"Upgrade to a specific version","text":"<p>Alternatively, you can install one of the previous versions, or a development one. You can list all the previous versions using:</p> <pre><code>microk8s helm3 search repo snmp --versions\n</code></pre> <p>And all the development versions using:</p> <pre><code>microk8s helm3 search repo snmp --devel\n</code></pre> <p>To upgrade your SC4SNMP instance to any of the listed versions, run <code>helm3 upgrade</code> with the <code>--version</code> flag:</p> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace --version &lt;VERSION&gt;\n</code></pre> <p>See the following example:</p> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace --version 1.6.3-beta.13\n</code></pre>"},{"location":"microk8s/configuration/configuring-groups/","title":"Configuring Groups","text":""},{"location":"microk8s/configuration/configuring-groups/#configuring-groups","title":"Configuring Groups","text":"<p>It is common to configure whole groups of devices instead of just single ones.  SC4SNMP allows both types of configuration. A group consists of many hosts. Each of them is configured in the <code>values.yaml</code>  file, in the <code>scheduler</code> section. After configuring a group, its name can be used in the <code>address</code> field in the inventory record. All settings specified in the inventory record will be assigned to hosts from the given group,  unless specific host configuration overrides it.</p> <ul> <li>See the Scheduler Configuration page for group examples and documentation. </li> <li>See the Poller Configuration page for information about groups in the inventory. </li> </ul> <p>If the host is configured in the group and both the group and the single host are included in the inventory, the configuration for the single host will be ignored in favor of the group configuration. See the following example: </p> <pre><code>scheduler:\n  groups: |\n    example_group_1:\n      - address: 10.202.4.202\n        port: 161\n      - address: 63.2.40.0\n        port: 161\n</code></pre> <pre><code>poller:\n    inventory: |\n      address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n      example_group_1,,2c,public,,,2000,my_profile2,,\n      10.202.4.202,,2c,public,,,2000,my_profile1,,\n</code></pre> <p>If the specific host from the group has to be configured separately, first it must be deleted from the group configuration, and then it can be inserted as a new record in the inventory. See the following example:</p> <pre><code>scheduler:\n  groups: |\n    example_group_1:\n      - address: 63.2.40.0\n        port: 161\n</code></pre> <pre><code>poller:\n    inventory: |\n      address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n      example_group_1,,2c,public,,,2000,my_profile2,,\n      10.202.4.202,,2c,public,,,2000,my_profile1,,\n</code></pre>"},{"location":"microk8s/configuration/configuring-profiles/","title":"Configuring Profiles","text":""},{"location":"microk8s/configuration/configuring-profiles/#configuring-profiles","title":"Configuring profiles","text":"<p>Profiles are the part of configuration where you can specify what you want to poll, and then assign them to the device.  The definition of profile can be found in the <code>values.yaml</code> file under the <code>scheduler</code> section.</p> <p>See the following instructions on how to use profiles: Update Inventory and Profile. </p> <p>There are two types of profiles in general:</p> <ol> <li>Static profile: Polling starts when the profile is added to the <code>profiles</code> field in the <code>inventory</code> of the device.</li> <li> <p>Smart profile: Polling starts when configured conditions are fulfilled, and the device to poll from has <code>smart_profiles</code> enabled in inventory. Smart profiles are useful when you have many devices of the same kind, and you do not want to configure each of them individually with static profiles.</p> <p>In order to configure smart profile, do the following:</p> <ol> <li>Choose one of the fields polled from the device, most commonly sysDescr. </li> <li>Set the filter to match all the devices of this kind.</li> <li>Set up polling of the profile by enabling the smart profiles for the devices that you want to be polled.</li> </ol> </li> </ol> <p>The profile template looks like the following:</p> <pre><code>scheduler:\n    profiles: |\n      #Name of profile\n      basev1:\n        # Define frequency for profile\n        frequency: 100\n        #Define condition\n        condition:\n          # Define type of condition. Allowed value field, base and walk\n          type: field\n          field: \"SNMPv2-MIB.sysDescr\"\n          # Define paterns\n          patterns:\n            - '.*STRING_TO_BE_MATCHED.*'\n        #Define varbinds to query\n        varBinds:\n          # Syntax: [ \"MIB-Component\", \"MIB object name\"[Optional], \"MIB index number\"[Optional]]\n          - ['SNMPv2-MIB']\n          - ['SNMPv2-MIB', 'sysName']\n          - ['SNMPv2-MIB', 'sysUpTime',0]\n</code></pre> <p>In the following example, two profiles are configured. One is smart, and the other one is static:</p> <pre><code>scheduler:\n    profiles: |\n      smart_profile:\n        frequency: 100\n        condition:\n          type: field\n          field: \"SNMPv2-MIB.sysDescr\"\n          patterns:\n            - '.*linux.*'\n        varBinds:\n          - ['SNMPv2-MIB']\n          - ['SNMPv2-MIB', 'sysName']\n          - ['SNMPv2-MIB', 'sysUpTime',0]\n      static_profile:\n        frequency: 300\n        varBinds:\n          - ['IP-MIB']\n</code></pre> <p>If you only want to enable the option of <code>static_profile</code> polling for the host <code>10.202.4.202</code>, you would configure a similar inventory:</p> <pre><code>poller:\n    inventory: |\n      address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n      10.202.4.202,,2c,public,,,2000,static_profile,f,\n</code></pre> <p>If you want to enable checking the <code>10.202.4.202</code> device against smart profiles, you need to set <code>smart_profiles</code> to <code>t</code>:</p> <pre><code>poller:\n    inventory: |\n      address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n      10.202.4.202,,2c,public,,,2000,,t,\n</code></pre> <p>Afterwards, if the device <code>sysDescr</code> matches the <code>'.*linux.*'</code> filter, the <code>smart_profile</code> profile will be polled.</p>"},{"location":"microk8s/configuration/configuring-profiles/#varbinds-configuration","title":"varBinds configuration","text":"<p><code>VarBinds</code> is short name for \u201cvariable binding\u201d in the SNMP. It is the combination of an Object Identifier (OID) and a value.  <code>varBinds</code> are used for defining what OIDs should be requested from SNMP Agents. <code>varBinds</code> is a required  subsection of each profile. The syntax configuration of <code>varBinds</code> looks like the following:</p> <p>[ \u201cMIB-Component\u201d, \u201cMIB object\u201d[Optional], \u201cMIB index number\u201d[Optional]]</p> <ul> <li><code>MIB-Component</code>: The SNMP MIB itself consists of distinct component MIBs, each of which refers to a specific  collection of management information that is part of the overall SNMP MIB, for example, <code>SNMPv2-MIB</code>.   If only the <code>MIB-Component</code> is set, then the SC4SNMP will get the whole subtree.</li> <li><code>MIB object</code>:  The SNMP MIB stores only simple data types: scalars and two-dimensional arrays of scalars,   called tables. The keywords SYNTAX, ACCESS, and DESCRIPTION as well as other keywords such as STATUS and   INDEX are used to define the SNMP MIB managed objects. </li> <li><code>MIB index number</code>: Define the index number for a given MIB Object, for example,<code>0</code>.</li> </ul> <p>See the following example: </p><pre><code>  varBinds:\n    # Syntax: [ \"MIB-Component\", \"MIB object name\"[Optional], \"MIB index number\"[Optional]]\n    - ['SNMPv2-MIB']\n    - ['SNMPv2-MIB', 'sysName']\n    - ['SNMPv2-MIB', 'sysUpTime',0]\n</code></pre>"},{"location":"microk8s/configuration/configuring-profiles/#static-profile-configuration","title":"Static Profile configuration","text":"<p>Static Profile is used when a list of profiles is defined in the <code>poller</code>  service Inventory configuration. Static Profiles are executed  even if the SmartProfile flag in inventory is set to false.  To configure Static Profile, the following value needs to be set in the <code>profiles</code> section:</p> <ul> <li>Define <code>ProfileName</code> as a subsection key in <code>profiles</code>.</li> <li>Define <code>frequency</code> as the interval between SNMP execution in seconds.  </li> <li>Define <code>varBinds</code> as variable bindings to query. </li> </ul> <p>See the following example: </p><pre><code>scheduler:\n  profiles: |\n    static_profile_example:\n      frequency: 20\n      varBinds:\n        - ['SNMPv2-MIB']\n        - ['SNMPv2-MIB', 'sysName']\n        - ['SNMPv2-MIB', 'sysUpTime',0]\n</code></pre>"},{"location":"microk8s/configuration/configuring-profiles/#particular-kinds-of-static-profiles","title":"Particular kinds of static profiles","text":"<p>Sometimes static profiles have additional functionalities to be used in specific scenarios. </p>"},{"location":"microk8s/configuration/configuring-profiles/#walk-profile","title":"WALK profile","text":"<p>If you would like to limit the scope of the walk, you should set one of the profiles in the inventory to point to the profile  definition of the <code>walk</code> type: </p><pre><code>scheduler:\n    profiles: |\n      small_walk:\n        condition: \n          type: \"walk\"\n        varBinds:\n          - ['UDP-MIB']\n</code></pre> This profile should be placed in the profiles section of the inventory definition. It will be executed with the frequency  defined in <code>walk_interval</code> field from <code>inventory</code>. If multiple profiles of type <code>walk</code> were placed in profiles, the last one will be used.  <p>See the following example on how to use <code>walk</code> in profiles:</p> <pre><code>poller:\n  inventory: |\n    address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n    10.202.4.202,,2c,public,,,2000,small_walk,,\n</code></pre> <p>Info</p> <p>When small walk is configured, <code>SNMPv2-MIB</code> is polled by default (we need it to create the state of the device in the database). For example, if you used <code>small_walk</code> from the previous example, you will only be able to poll <code>UDP-MIB</code> and <code>SNMPv2-MIB</code> OIDs.</p>"},{"location":"microk8s/configuration/configuring-profiles/#smartprofile-configuration","title":"SmartProfile configuration","text":"<p>SmartProfile is executed when the SmartProfile flag in the inventory is set to true and the conditions defined in profile match.  See Inventory configuration for more information.</p> <p>To configure SmartProfile, the following values needs to be set in the <code>profiles</code> section:</p> <ul> <li>For<code>ProfileName</code>, define it as a subsection key in <code>profiles</code>. </li> <li>For<code>frequency</code>, define it as the interval between SNMP execution in seconds.</li> <li>For <code>condition</code>, define the conditions to match the profile. </li> <li>For <code>type</code>, define it as the key for the <code>condition</code> section that defines the type of condition. The allowed    values are <code>base</code> or <code>field</code> (<code>walk</code> type is also allowed here, but it is not part of smart profiles).</li> <li>The <code>base</code> type of condition will be executed when <code>SmartProfile</code> in inventory is set to true.</li> <li>The<code>field</code> type of condition will be executed if it matches <code>pattern</code> for the defined <code>field</code>. Supported fields are:<ul> <li>\u201cSNMPv2-MIB.sysDescr\u201d</li> <li>\u201cSNMPv2-MIB.sysObjectID\u201d</li> </ul> </li> <li>For <code>field</code>, define the field name for the field condition type. </li> <li>For<code>pattern</code>, define the list of regular expression patterns for the MIB object field defined in the <code>field</code> section, for example:<ul> <li>\u201d.linux.\u201c</li> </ul> </li> <li>For <code>varBinds</code>, define variable bindings to query. </li> </ul> <p>See the following example of a <code>base</code> type profile: </p><pre><code>scheduler:\n    profiles: |\n      SmartProfile_base_example:\n        frequency: 100\n        condition: \n          type: \"base\"\n        varBinds:\n          - ['SNMPv2-MIB']\n          - ['SNMPv2-MIB', 'sysName']\n</code></pre> <p>See the following example of a <code>field</code>  type profile, also called an automatic profile: </p><pre><code>scheduler:\n    profiles: |\n      SmartProfile_field_example:\n        frequency: 100\n        condition: \n          type: \"field\"\n          field: \"SNMPv2-MIB.sysDescr\"\n          patterns:\n            - '.*STRING_TO_BE_MATCHED.*'\n        varBinds:\n          - ['SNMPv2-MIB']\n          - ['SNMPv2-MIB', 'sysName']\n</code></pre> <p>Info</p> <p>Be aware that profile changes may not be reflected immediately. It can take up to 1 minute for changes to propagate.  In case you changed the frequency, or a profile type, the change will be reflected only after the next walk. There is also a 5 minute time to live (TTL) for an inventory pod. SC4SNMP allows one inventory upgrade and then it  block updates for the next 5 minutes.</p>"},{"location":"microk8s/configuration/configuring-profiles/#conditional-profiles","title":"Conditional profiles","text":"<p>There is a way to not explicitly list what SNMP objects you want to poll, but, instead, only give the conditions that must  be fulfilled to qualify an object for polling.</p> <p>See the following example of a conditional profile:</p> <pre><code>IF_conditional_profile:\n  frequency: 30\n  conditions:\n    - field: IF-MIB.ifAdminStatus\n      operation: \"equals\" \n      value: \"up\"\n    - field: IF-MIB.ifOperStatus\n      operation: \"equals\"\n      value: \"up\"\n  varBinds:\n    - [ 'IF-MIB', 'ifDescr' ]\n    - [ 'IF-MIB', 'ifAlias' ]\n    - [ 'IF-MIB', 'ifInErrors' ]\n    - [ 'IF-MIB', 'ifOutDiscards' ]\n</code></pre> <p>When the such profile is defined and added to a device in an inventory, it will poll all interfaces where <code>ifAdminStatus</code> and <code>ifOperStatus</code> is up. Conditional profiles are being evaluated during the walk process (on every <code>walk_interval</code>), and, if the status changes in between, the scope of the conditional profile will not be modified. Therefore, status  changes are only implemented when walk_interval is executed.</p> <p>See the following operations that can be used in conditional profiles: </p> <ol> <li><code>equals</code>: the value gathered from <code>field</code> is equal to the<code>value</code>.</li> <li><code>gt</code>: the value gathered from <code>field</code> is bigger than <code>value</code> (works only for numeric values).</li> <li><code>lt</code>: the value gathered from <code>field</code> is smaller than <code>value</code> (works only for numeric values).</li> <li><code>in</code>: the value gathered from <code>field</code> is equal to one of the elements provided in <code>value</code>, for example:</li> </ol> <pre><code>conditions:\n  - field: IF-MIB.ifAdminStatus\n    operation: \"in\"\n    value: \n      - \"down\"\n      - 0\n</code></pre> <ol> <li><code>regex</code>: value gathered from <code>field</code> match the pattern provided in <code>value</code>.  You can add options for regular expression after <code>/</code>. Possible options match ones used in mongodb regex operator, for example: </li> </ol> <pre><code>conditions:\n  - field: IF-MIB.ifAdminStatus\n    operation: \"regex\"\n    value: \".own/i\"\n</code></pre> <p>To negate an operation you can add the flag <code>negate_operation: \"true\"</code> to the specified <code>field</code>, for example:  </p><pre><code>conditions:\n    - field: IF-MIB.ifAdminStatus\n      operation: \"equals\" \n      value: \"up\"\n      negate_operation: \"true\"\n</code></pre> This will negate the operator specified in <code>operation</code>. See the following:  <ol> <li><code>negate_operation + equals</code>: value gathered from <code>field</code> is NOT equal to <code>value</code>.</li> <li><code>negate_operation + gt</code>: value gathered from <code>field</code> is SMALLER or EQUAL to <code>value</code> (works only for numeric values).</li> <li><code>negate_operation + lt</code>: value gathered from <code>field</code> is BIGGER or EQUAL to <code>value</code> (works only for numeric values).</li> <li><code>negate_operation + in</code>: value gathered from <code>field</code> is NOT equal to any of the elements provided in <code>value</code>.</li> <li><code>negate_operation + regex</code>: value gathered from <code>field</code> is NOT matching the pattern provided in <code>value</code>. </li> </ol> <p>The <code>field</code> parameter in <code>conditions</code> must fulfill the pattern <code>MIB-family.field</code>. The field must represent a textual value (rather than a metric one). See snmp data format for more information. </p> <p>You have to explicitly define <code>varBinds</code> (not only the MIB family but also the field to poll). See the following incorrect example: </p> <pre><code>varBinds:\n- [ 'IF-MIB' ]\n</code></pre>"},{"location":"microk8s/configuration/configuring-profiles/#custom-translations","title":"Custom translations","text":"<p>If the user wants to use custom names/translations of MIB names, it can be configured under the <code>customTranslations</code> section under scheduler config. Translations are grouped by the MIB family. In the following example, <code>IF-MIB.ifInDiscards</code> will be translated to <code>IF-MIB.myCustomName1</code>: </p><pre><code>scheduler:\n    customTranslations:\n      IF-MIB:\n        ifInDiscards: myCustomName1\n        ifOutErrors: myCustomName2\n      SNMPv2-MIB:\n        sysDescr: myCustomName3\n</code></pre>"},{"location":"microk8s/configuration/coredns-configuration/","title":"CoreDNS","text":""},{"location":"microk8s/configuration/coredns-configuration/#configuration-of-coredns-in-microk8s-to-use-different-nameservers-for-different-domains-and-ip-ranges","title":"Configuration of CoreDNS in microk8s to use different nameservers for different domains and ip ranges","text":"<p>In MicroK8s, CoreDNS is enabled by running the following command: <code>microk8s enable dns</code>.</p> <p>Alternatively, you can specify a list of DNS servers by running the command: <code>microk8s enable dns:8.8.8.8,1.1.1.1</code>.</p> <p>The servers in the provided list are expected to be capable of resolving the same addresses.  If one of these servers is unreachable, another one is used.  If the requirement is to use different DNS servers for various domains or different IP ranges in the case of reverse lookup, the configuration differs.</p> <p>Before executing <code>microk8s enable dns</code>, the first step is to edit <code>coredns.yaml</code>, located inside the MicroK8s installation folder. An example path is: <code>/var/snap/microk8s/common/addons/core/addons/dns/coredns.yaml</code>.</p> <p>Inside <code>coredns.yaml</code>, there is a complete configuration for the CoreDNS deployment. The only section that requires editing is the ConfigMap:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io/mode: EnsureExists\n    k8s-app: kube-dns\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health {\n          lameduck 5s\n        }\n        ready\n        log . {\n          class error\n        }\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n          pods insecure\n          fallthrough in-addr.arpa ip6.arpa\n        }\n        prometheus :9153\n        forward . $NAMESERVERS\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\n</code></pre> <p>Changes should be made in <code>data.Corefile</code> within this ConfigMap. Presented documentation explains basic configuration.  For more details, refer to the official CoreDNS documentation.</p> <p>Updated ConfigMap: </p><pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io/mode: EnsureExists\n    k8s-app: kube-dns\ndata:\n  Corefile: |\n      .:53 {\n        errors\n        health {\n          lameduck 5s\n        }\n        ready\n        log . {\n          class error\n        }\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n          pods insecure\n          fallthrough in-addr.arpa ip6.arpa\n        }\n        prometheus :9153\n        forward . $NAMESERVERS\n        cache 1\n        loop\n        reload\n        loadbalance\n      }\n      dummyhost.com:53 {\n      errors\n        health {\n          lameduck 5s\n        }\n        ready\n        log . {\n          class error\n        }\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n          pods insecure\n          fallthrough in-addr.arpa ip6.arpa\n        }\n        prometheus :9153\n        forward . 4.3.2.1\n        cache 1\n        loop\n        reload\n        loadbalance\n      }\n      2.1.in-addr.arpa:53 {\n       errors\n        health {\n          lameduck 5s\n        }\n        ready\n        log . {\n          class error\n        }\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n          pods insecure\n          fallthrough in-addr.arpa ip6.arpa\n        }\n        prometheus :9153\n        forward . 4.3.2.1\n        cache 1\n        loop\n        reload\n        loadbalance\n      }\n</code></pre> <p>Two server blocks, <code>dummyhost.com:53</code> and <code>2.1.in-addr.arpa:53</code>, have been added.</p> <p>The <code>dummyhost.com:53</code> server block is used to resolve all hosts within the <code>dummyhost.com</code> domain.  The DNS server used for these hosts is specified in the forward plugin as <code>4.3.2.1</code>.  Additional information about the forward plugin can be found in the official CoreDNS documentation.</p> <p>The <code>2.1.in-addr.arpa:53</code> server block is added for reverse DNS lookup for all devices in the IPv4 range <code>1.2.0.0/16</code>.  The DNS server is the same as in the <code>dummyhost.com:53</code> server block.</p> <p>All other DNS requests will be handled by the <code>8.8.8.8</code> server if <code>microk8s enable dns</code> is run without providing a list of DNS servers.  Alternatively, one of the servers provided in the list will be used in the case of running with the list of servers,  i.e., <code>microk8s enable dns:8.8.8.8,1.1.1.1</code>.</p>"},{"location":"microk8s/configuration/deployment-configuration/","title":"Deployment","text":""},{"location":"microk8s/configuration/deployment-configuration/#deployment-configuration","title":"Deployment Configuration","text":"<p>The <code>values.yaml</code> is the main point of SC4SNMP management. You can check all the default values of Helm dependencies using the following command:</p> <pre><code>microk8s helm3 inspect values splunk-connect-for-snmp/splunk-connect-for-snmp &gt; values.yaml\n</code></pre> <p>The whole file is divided into the following parts:</p> <p>To configure the endpoint for sending SNMP data:</p> <ul> <li><code>splunk</code> - in case you use Splunk Enterprise/Cloud.</li> <li><code>sim</code> - in case you use Splunk Observability Cloud. For more details see sim configuration.</li> </ul> <p>For polling purposes:</p> <ul> <li><code>scheduler</code> - For more details see scheduler configuration.</li> <li><code>poller</code> - For more details see poller configuration.</li> </ul> <p>For traps receiving purposes:</p> <ul> <li><code>traps</code> - For more details see trap configuration.</li> </ul> <p>Shared components:</p> <ul> <li><code>inventory</code> - For more details see inventory configuration.</li> <li><code>mibserver</code> - For more details see mibserver configuration.</li> <li><code>mongodb</code> - For more details see mongo configuration.</li> <li><code>redis</code> - For more details see redis configuration.</li> <li><code>ui</code> - For more details see ui configuration.</li> <li><code>worker</code> - For more details see worker configuration.</li> </ul>"},{"location":"microk8s/configuration/deployment-configuration/#shared-values","title":"Shared values","text":"<p>All the components have the following <code>resources</code> field for adjusting memory resources:</p> <p></p><pre><code>  resources:\n    limits:\n      cpu: 1000m\n      memory: 2Gi\n    requests:\n      cpu: 1000m\n      memory: 2Gi\n</code></pre> For more information about the concept of <code>resources</code>, see the kuberentes documentation. For more information about scaling resources see the scaling with microk8s. <p>There is an option to create common annotations across all services. It can be set by:</p> <pre><code>commonAnnotations:\n  annotation_key: annotation_value\n</code></pre>"},{"location":"microk8s/configuration/mongo-configuration/","title":"MongoDB","text":""},{"location":"microk8s/configuration/mongo-configuration/#mongo-db-configuration","title":"Mongo DB Configuration","text":"<p>Mongo DB is used as the database for keeping schedules.</p>"},{"location":"microk8s/configuration/mongo-configuration/#mongodb-configuration-file","title":"MongoDB configuration file","text":"<p>MongoDB configuration is kept in the <code>values.yaml</code> file in the <code>mongodb</code> section. <code>values.yaml</code> is used during the installation process for configuring kubernetes values.</p> <p>See the following example: </p><pre><code>mongodb:\n  #Architecture, Architecture for Mongo deployments is immutable to move from standalone to replicaset will require a uninstall.\n  # \"replicaset\" for HA or multi node deployments\n  # \"standalone\" for single node non HA\n  #architecture: \"standalone\"\n  pdb:\n    create: true\n  #The following requests and limits are appropriate starting points\n  #For productions deployments\n  resources: \n    limits:\n      cpu: 2\n      memory: 2Gi\n    requests:\n      cpu: 750m\n      memory: 512Mi    \n  persistence:\n    storageClass: \"microk8s-hostpath\"\n  volumePermissions:\n    enabled: true\n</code></pre> <p>It is recommended not to change this setting. If it is necessary to change it, see MongoDB on Kubernetes. </p>"},{"location":"microk8s/configuration/poller-configuration/","title":"Poller","text":""},{"location":"microk8s/configuration/poller-configuration/#poller-configuration","title":"Poller Configuration","text":"<p>Poller is a service which is responsible for querying  SNMP devices using the SNMP GET and WALK functionalities. Poller executes two main types of tasks:</p> <ul> <li> <p>The Walk task executes SNMP walk. SNMP walk is an SNMP application that uses SNMP GETNEXT requests to  collect SNMP data from the network and infrastructure of SNMP-enabled devices, such as switches and routers. It is a time-consuming task, which may overload the SNMP device when executed too often. It is used by the SC4SNMP to collect and push all OID values, which the provided ACL has access to. </p> </li> <li> <p>The Get task is a lightweight task that queries a subset of OIDs defined by the customer. This task monitors OIDs, such as memory or CPU utilization.  </p> </li> </ul> <p>Poller has an <code>inventory</code>, which defines what and how often SC4SNMP has to poll.</p>"},{"location":"microk8s/configuration/poller-configuration/#poller-configuration-file","title":"Poller configuration file","text":"<p>The poller configuration is kept in a <code>values.yaml</code> file in the <code>poller</code> section. <code>values.yaml</code> is used during the installation process for configuring Kubernetes values.</p> <p>See the following poller example configuration: </p><pre><code>poller:\n  usernameSecrets:\n   - sc4snmp-hlab-sha-aes\n   - sc4snmp-hlab-sha-des\n  logLevel: \"WARN\"\n  inventory: |\n    address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n    10.202.4.202,,2c,public,,,2000,,,\n</code></pre> <p>Info</p> <p>The header\u2019s line (<code>address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete</code>) is necessary for the correct execution of SC4SNMP. Do not remove it.</p>"},{"location":"microk8s/configuration/poller-configuration/#ipv6-hostname-resolution","title":"IPv6 hostname resolution","text":"<p>When IPv6 is enabled and device is dual stack, the hostname resolution will try to resolve the name to the IPv6 address first, then to the IPv4 address.</p>"},{"location":"microk8s/configuration/poller-configuration/#define-log-level","title":"Define log level","text":"<p>The log level for poller can be set by changing the value for the key <code>logLevel</code>. The allowed values are: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code> or <code>FATAL</code>.  The default value is <code>INFO</code>.</p>"},{"location":"microk8s/configuration/poller-configuration/#define-usernamesecrets","title":"Define usernameSecrets","text":"<p>Secrets are required to run SNMPv3 polling. To add v3 authentication details, create the k8s Secret object: SNMPv3 Configuration, and put its name in <code>poller.usernameSecrets</code>.</p>"},{"location":"microk8s/configuration/poller-configuration/#append-oid-index-part-to-the-metrics","title":"Append OID index part to the metrics","text":"<p>Not every SNMP metric object is structured with its index as a one of the field values. We can append the index part of OID with:</p> <pre><code>poller:\n  metricsIndexingEnabled: true\n</code></pre> <p>So the following change will make this metric object (derived from the OID <code>1.3.6.1.2.1.6.20.1.4.0.0.443</code>)</p> <pre><code>{\n   frequency: 5\n   metric_name:sc4snmp.TCP-MIB.tcpListenerProcess: 309\n   mibIndex: 0.0.443\n   profiles: generic_switch\n}\n</code></pre> <p>out of this object: </p><pre><code>{\n   frequency: 5\n   metric_name:sc4snmp.TCP-MIB.tcpListenerProcess: 309\n   profiles: generic_switch\n}\n</code></pre>"},{"location":"microk8s/configuration/poller-configuration/#disable-automatic-polling-of-base-profiles","title":"Disable automatic polling of base profiles","text":"<p>There are two profiles that are being polled by default, so that even without any configuration set up, you can see the data in Splunk. You can disable it with the following <code>pollBaseProfiles</code> parameter:</p> <pre><code>poller:\n  pollBaseProfiles: false\n</code></pre>"},{"location":"microk8s/configuration/poller-configuration/#configure-inventory","title":"Configure inventory","text":"<p>To update inventory, see Update Inventory and Profile.</p> <p>The <code>inventory</code> section in <code>poller</code> has the following fields to configure:</p> Field Description Default Required <code>address</code> The IP address which SC4SNMP should collect data from, or name of the group of hosts. General information about groups can be found on the Configuring Groups page. YES <code>port</code> SNMP listening port. <code>161</code> NO <code>version</code> SNMP version, the allowed values are <code>1</code>, <code>2c</code>, or <code>3</code>. YES <code>community</code> SNMP community string, this field is required when the <code>version</code> is <code>1</code> or <code>2c</code>. NO <code>secret</code> The reference to the secret from <code>poller.usernameSecrets</code> that should be used to poll from the device. NO <code>security_engine</code> The security engine ID required by SNMPv3. If it is not provided for version <code>3</code>, it will be autogenerated. NO <code>walk_interval</code> The interval in seconds for SNMP walk. This value needs to be between <code>1800</code> and <code>604800</code>. <code>42000</code> NO <code>profiles</code> A list of SNMP profiles used for the device. More than one profile can be added by a semicolon separation, for example, <code>profile1;profile2</code>. For more information about profiles, see Profile Configuration. NO <code>smart_profiles</code> Enables smart profiles. Its allowed values are <code>true</code> or <code>false</code>. <code>true</code> NO <code>delete</code> A flag that defines if the inventory should be deleted from the scheduled tasks for WALKs and GETs. Its allowed value are <code>true</code>or <code>false</code>. The default value is <code>false</code>. <code>false</code> NO <p>See the following example: </p><pre><code>poller:\n    inventory: |\n      address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n      10.202.4.202,,2c,public,,,2000,my_profile1,,\n      example_group_1,,2c,public,,,2000,my_profile2;my_profile3,,\n</code></pre>"},{"location":"microk8s/configuration/poller-configuration/#update-inventory","title":"Update Inventory","text":"<p>Adding new devices for <code>values.yaml</code> is resource expensive, and can impact performance. As it interacts with hardware networking devices, the updating process requires several checks before applying changes. SC4SNMP was designed to prevent changes in inventory tasks  more often than every 5 minutes.</p> <p>To apply inventory changes in <code>values.yaml</code>, the following steps need to be executed:</p> <ol> <li>Edit <code>values.yaml</code> </li> <li>Check if the inventory pod is still running using the following execute command:</li> </ol> <pre><code>microk8s kubectl -n sc4snmp get pods | grep inventory\n</code></pre> <p>If the command return pods, wait and continue to execute the command again, until the inventory job finishes. </p> <p>If you really need to apply changes immediately, you can get around the limitation by deleting the inventory job using the following command:</p> <pre><code>microk8s kubectl delete job/snmp-splunk-connect-for-snmp-inventory -n sc4snmp\n</code></pre> <p>After running this command, you can proceed with upgrading without a need to wait.</p> <ol> <li>Run upgrade command :</li> </ol> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <p>Info</p> <p>If you decide to change the frequency of the profile without changing the inventory data, the change will be reflected after  the next walk process for the host. The walk happens every <code>walk_interval</code>, or during any change in inventory.</p>"},{"location":"microk8s/configuration/poller-configuration/#upgrade-with-the-csv-file","title":"Upgrade with the csv file","text":"<p>You can update inventory by making changes outside of the <code>values.yaml</code>. It can be put into a separate csv file and upgraded using <code>--set-file poller.inventory=&lt;path_to_file&gt;</code>.</p> <p>See the following example of an CSV file configuration:</p> <pre><code>address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n10.202.4.202,,2c,public,,,3000,my_profile,,\n</code></pre> <p>See the following example of an upgrade command with a CSV file:</p> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml --set-file poller.inventory=inventory.csv splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre>"},{"location":"microk8s/configuration/redis-configuration/","title":"Redis","text":""},{"location":"microk8s/configuration/redis-configuration/#redis-configuration","title":"Redis configuration","text":"<p>Recently, RabbitMQ was replaced with Redis as a queue service and periodic task database. The reason for this is to increase SC4SNMP performance and protect against bottlenecks.</p> <p>Redis both manages periodic tasks and queues the SC4SNMP service. It queues tasks like SNMP Walk and Poll.  </p>"},{"location":"microk8s/configuration/redis-configuration/#redis-configuration-file","title":"Redis configuration file","text":"<p>Redis configuration is kept in the <code>values.yaml</code> file in the <code>redis</code> section. <code>values.yaml</code> is used during the installation process to configure Kubernetes values.</p> <p>To edit the configuration, see Redis on Kubernetes.</p>"},{"location":"microk8s/configuration/scheduler-configuration/","title":"Scheduler","text":""},{"location":"microk8s/configuration/scheduler-configuration/#scheduler-configuration","title":"Scheduler configuration","text":"<p>The scheduler is a service that manages schedules for SNMP WALKs and GETs. The definitions of the schedules are stored in MongoDB. </p>"},{"location":"microk8s/configuration/scheduler-configuration/#scheduler-configuration-file","title":"Scheduler configuration file","text":"<p>Scheduler configuration is kept in <code>values.yaml</code> file, in the section <code>scheduler</code>. <code>values.yaml</code> is used during the installation process to configure Kubernetes values.</p> <p>See the following example:  </p><pre><code>scheduler:\n  logLevel: \"WARN\"\n  profiles: |\n    test_profile:\n      frequency: 5 \n      condition: \n        type: \"field\" \n        field: \"SNMPv2-MIB.sysDescr\" \n        patterns: \n          - \"^.*\"\n      varBinds:\n          # Syntax: [ \"MIB-Component\", \"MIB object name\"[Optional], \"MIB index number\"[Optional]]\n        - [\"SNMPv2-MIB\", \"sysDescr\",0]\n</code></pre>"},{"location":"microk8s/configuration/scheduler-configuration/#define-log-level","title":"Define log level","text":"<p>The log level for the scheduler can be set by changing the value for the <code>logLevel</code> key. The allowed values are <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, or <code>ERROR</code>.  The default value is <code>WARNING</code>.</p>"},{"location":"microk8s/configuration/scheduler-configuration/#define-resource-requests-and-limits","title":"Define resource requests and limits","text":"<p>To change the resource requests ad limits for cpu and memory, modify the <code>resources</code> section of the scheduler configuration.</p> <pre><code>scheduler:\n  # The following resource specification is appropriate for most deployments to scale the\n  # Larger inventories may require more memory but should not require additional cpu\n  resources:\n    limits:\n        cpu: 1\n        memory: 1Gi\n    requests:\n      cpu: 200m\n      memory: 128Mi\n</code></pre>"},{"location":"microk8s/configuration/scheduler-configuration/#define-groups-of-hosts","title":"Define groups of hosts","text":"<p>For more information on when to use groups, see Configuring Groups.</p> <p>See the following example of group configuration: </p><pre><code>scheduler:\n  groups: |\n    example_group_1:\n      - address: 123.0.0.1\n        port: 161\n      - address: 178.8.8.1\n        port: 999\n      - address: 12.22.23\n        port: 161\n        community: 'private'\n    example_group_2:\n      - address: 103.0.0.1\n        port: 1161\n        version: '3'\n        secret: 'my_secret'\n      - address: 178.80.8.1\n        port: 999\n</code></pre> <p>The one obligatory field for the host configuration is <code>address</code>. If <code>port</code> is not configured its default value is <code>161</code>.  Other fields that can be modified are: <code>community</code>, <code>secret</code>, <code>version</code>, and <code>security_engine</code>. However, if they remain unspecified in the host configuration, they will be derived from the inventory record. </p>"},{"location":"microk8s/configuration/scheduler-configuration/#define-the-expiration-time-for-tasks","title":"Define the expiration time for tasks","text":"<p>Define the time, in seconds, when polling or walk tasks will be revoked if they have not been picked up by the worker. See the celery documentation for more details. </p><pre><code>scheduler:\n  tasksExpiryTime: 300\n</code></pre>"},{"location":"microk8s/configuration/sim-configuration/","title":"Splunk Infrastructure Monitoring","text":""},{"location":"microk8s/configuration/sim-configuration/#otel-and-splunk-observability-cloud-configuration","title":"OTEL and Splunk Observability Cloud configuration","text":"<p>Splunk OpenTelemetry Collector is a component that provides an option to send metrics to Splunk Observability Cloud. In order to use it, you must set <code>enabled</code> flag in <code>values.yaml</code> to <code>true</code>:</p> <pre><code>sim:\n  # sim must be enabled if you want to use SignalFx\n  enabled: true\n</code></pre>"},{"location":"microk8s/configuration/sim-configuration/#token-and-realm","title":"Token and realm","text":"<p>You need to specify Splunk Observability Cloud token and realm. There are two ways of configuring them:</p> <ol> <li>Pass those in a plain text using <code>values.yaml</code>, so at the end, the sim element looks like the following:</li> </ol> <pre><code>sim:\n  enabled: true\n  signalfxToken: BCwaJ_Ands4Xh7Nrg\n  signalfxRealm: us0\n</code></pre> <ol> <li>Alternatively, create the microk8s secret by yourself and pass its name to the <code>values.yaml</code> file. Use the following command to create it:</li> </ol> <pre><code>microk8s kubectl create -n &lt;namespace&gt; secret generic &lt;secretname&gt; \\\n  --from-literal=signalfxToken=&lt;signalfxToken&gt; \\\n  --from-literal=signalfxRealm=&lt;signalfxRealm&gt;\n</code></pre> <p>Modify <code>sim.secret</code> section of <code>values.yaml</code>. Disable the creation of the secret with <code>sim.secret.create</code> and provide the <code>&lt;secretname&gt;</code>, matching the one from the previous step. Pass it using <code>sim.secret.name</code>. For example, for <code>&lt;secretname&gt;</code>=<code>signalfx</code>, the <code>sim</code> section would look like the following:</p> <pre><code>sim:\n  secret:\n    create: false\n    name: signalfx\n</code></pre> <p>Info</p> <p>After the initial installation, if you change <code>sim.signalfxToken</code> and/or <code>sim.signalfxRealm</code> and no <code>sim.secret.name</code> is given,  the <code>sim</code> pod will sense the update by itself (after <code>helm3 upgrade</code> command) and trigger the recreation. But, when you edit secret created outside of <code>values.yaml</code> (given by <code>sim.secret.name</code>), you need to roll out the deployment by yourself or delete the pod to update the data.</p>"},{"location":"microk8s/configuration/sim-configuration/#define-annotations","title":"Define annotations","text":"<p>In case you need to append some annotations to the <code>sim</code> service, you can do it by setting <code>sim.service.annotations</code>, for example:</p> <pre><code>sim:\n  service:\n    annotations:\n      annotation_key: annotation_value\n</code></pre>"},{"location":"microk8s/configuration/sim-configuration/#verify-the-deployment","title":"Verify the deployment","text":"<p>After executing <code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace</code>, the sim pod should be up and running:</p> <pre><code>splunker@ip-10-202-13-233:~$ microk8s kubectl get pods -n sc4snmp\nNAME                                                      READY   STATUS    RESTARTS   AGE\nsnmp-splunk-connect-for-snmp-scheduler-7ddbc8d75-bljsj        1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-worker-poller-57cd8f4665-9z9vx   1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-worker-sender-5c44cbb9c5-ppmb5   1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-worker-trap-549766d4-28qzh       1/1     Running   0          133m\nsnmp-mibserver-7f879c5b7c-hz9tz                               1/1     Running   0          133m\nsnmp-mongodb-869cc8586f-vvr9f                                 2/2     Running   0          133m\nsnmp-redis-master-0                                           1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-trap-78759bfc8b-79m6d            1/1     Running   0          99m\nsnmp-splunk-connect-for-snmp-sim-59b89747f-kn6tf              1/1     Running   0          32s\n</code></pre>"},{"location":"microk8s/configuration/snmp-data-format/","title":"SNMP data format","text":""},{"location":"microk8s/configuration/snmp-data-format/#snmp-data-format","title":"SNMP Data Format","text":"<p>SC4SNMP classifies SNMP data elements as metrics or textual fields. Metric types are usually the indicators worth monitoring,  which change dynamically, while textual fields are helpful context to understand what an SNMP object means.</p> <p>SC4SNMP classifies the data element as a metric when its type is one of the following:</p> <ul> <li><code>Unsigned</code></li> <li><code>Counter</code></li> <li><code>TimeTicks</code></li> <li><code>Gauge</code></li> <li><code>Integer</code></li> </ul> <p>Every other type is interpreted as a field value.</p> <p>Sometimes, the MIB file indicates a field as an <code>INTEGER</code>, but there is also some mapping defined. See the following<code>IF-MIB.ifOperStatus</code> example:</p> <pre><code>ifOperStatus OBJECT-TYPE\n    SYNTAX  INTEGER {\n                up(1),        -- ready to pass packets\n                down(2),\n                testing(3),   -- in some test mode\n                unknown(4),   -- status can not be determined\n                              -- for some reason.\n                dormant(5),\n                notPresent(6),    -- some component is missing\n                lowerLayerDown(7) -- down due to state of\n                                  -- lower-layer interface(s)\n            }\n</code></pre> <p>Here a numeric value is expected, but actually what SNMP Agents ends up receiving from the device is a <code>string</code> value, like <code>up</code>. To avoid setting textual value as a metric, SC4SNMP does an additional check and tries to cast the numeric value to float. If the check fails, the value is classified as a textual field.</p> <p>See the following simple example. You just added a device and did not configure anything special. The data from a walk in Splunk\u2019s metrics index is:</p> <pre><code>{\n   ifAdminStatus: up\n   ifDescr: GigabitEthernet1\n   ifIndex: 1\n   ifOperStatus: up\n   ifPhysAddress: 0a:aa:ef:53:67:15\n   ifType: ethernetCsmacd\n   metric_name:sc4snmp.IF-MIB.ifInDiscards: 0\n   metric_name:sc4snmp.IF-MIB.ifInErrors: 0\n   metric_name:sc4snmp.IF-MIB.ifInOctets: 3873878708\n   metric_name:sc4snmp.IF-MIB.ifInUcastPkts: 47512921\n   metric_name:sc4snmp.IF-MIB.ifInUnknownProtos: 0\n   metric_name:sc4snmp.IF-MIB.ifLastChange: 454107556\n   metric_name:sc4snmp.IF-MIB.ifMtu: 1500\n   metric_name:sc4snmp.IF-MIB.ifOutDiscards: 0\n   metric_name:sc4snmp.IF-MIB.ifOutErrors: 0\n   metric_name:sc4snmp.IF-MIB.ifOutOctets: 1738565177\n   metric_name:sc4snmp.IF-MIB.ifOutUcastPkts: 44295751\n   metric_name:sc4snmp.IF-MIB.ifSpeed: 1000000000\n}\n</code></pre> <p>You can see a textual part:</p> <pre><code>   ifAdminStatus: up\n   ifDescr: GigabitEthernet1\n   ifIndex: 1\n   ifOperStatus: up\n   ifPhysAddress: 0a:aa:ef:53:67:15\n   ifType: ethernetCsmacd\n</code></pre> <p>And a metric one: </p><pre><code>   metric_name:sc4snmp.IF-MIB.ifInDiscards: 0\n   metric_name:sc4snmp.IF-MIB.ifInErrors: 0\n   metric_name:sc4snmp.IF-MIB.ifInOctets: 3873878708\n   metric_name:sc4snmp.IF-MIB.ifInUcastPkts: 47512921\n   metric_name:sc4snmp.IF-MIB.ifInUnknownProtos: 0\n   metric_name:sc4snmp.IF-MIB.ifLastChange: 454107556\n   metric_name:sc4snmp.IF-MIB.ifMtu: 1500\n   metric_name:sc4snmp.IF-MIB.ifOutDiscards: 0\n   metric_name:sc4snmp.IF-MIB.ifOutErrors: 0\n   metric_name:sc4snmp.IF-MIB.ifOutOctets: 1738565177\n   metric_name:sc4snmp.IF-MIB.ifOutUcastPkts: 44295751\n   metric_name:sc4snmp.IF-MIB.ifSpeed: 1000000000\n</code></pre>"},{"location":"microk8s/configuration/snmp-data-format/#to-which-splunk-index-will-my-data-go","title":"To which Splunk index will my data go?","text":""},{"location":"microk8s/configuration/snmp-data-format/#metric-index","title":"Metric index","text":"<p>The rule is, if we poll a profile with AT LEAST one metric value, it will go to the metric index and will be enriched with all the textual fields you have for the object. For example, when polling:</p> <pre><code>profile_with_one_metric:\n  frequency: 100\n  varBinds:\n    - ['IF-MIB', 'ifOutUcastPkts']\n    - ['IF-MIB', 'ifInUcastPkts']\n</code></pre> <p>The record that you will see in Splunk <code>| mpreview index=net*</code> for the same case as the previous one would be:</p> <pre><code>   ifAdminStatus: up\n   ifDescr: GigabitEthernet1\n   ifIndex: 1\n   ifOperStatus: up\n   ifPhysAddress: 0a:aa:ef:53:67:15\n   ifType: ethernetCsmacd\n   metric_name:sc4snmp.IF-MIB.ifOutUcastPkts: 44295751\n   metric_name:sc4snmp.IF-MIB.ifInUcastPkts: 47512921\n</code></pre> <p>Only fields specified in <code>varBinds</code> are actively polled from the device. In the case of the previous <code>profile_with_one_metric</code>, the textual fields <code>ifAdminStatus</code>, <code>ifDescr</code>, <code>ifIndex</code>, <code>ifOperStatus</code> and <code>ifPhysAddress</code> are taken from the database cache. This is updated on every walk process. This is fine in most cases, as values such as MAC address, interface type, or interface status should not change frequently if at all. </p> <p>If you want to keep <code>ifOperStatus</code> and <code>ifAdminStatus</code> up to date all the time, define profile using the following example:</p> <pre><code>profile_with_one_metric:\n  frequency: 100\n  varBinds:\n    - ['IF-MIB', 'ifOutUcastPkts']\n    - ['IF-MIB', 'ifInUcastPkts']\n    - ['IF-MIB', 'ifOperStatus']\n    - ['IF-MIB', 'ifAdminStatus']\n</code></pre> <p>The result in Splunk will look the same, but <code>ifOperStatus</code> and <code>ifAdminStatus</code> will be actively polled.</p>"},{"location":"microk8s/configuration/snmp-data-format/#event-index","title":"Event index","text":"<p>It is possible to create an event without a single metric value. In such scenario, it will go to an event index. See the following example of profile under that scenario:</p> <pre><code>profile_with_only_textual_fields:\n  frequency: 100\n  varBinds:\n    - ['IF-MIB', 'ifDescr']\n    - ['IF-MIB', 'ifName']\n    - ['IF-MIB', 'ifOperStatus']\n</code></pre> <p>In the following example, no additional enrichment will be done. The events in event index <code>index=netops</code> of Splunk would look like:</p> <pre><code>{ [-]\n   IF-MIB.ifDescr: { [-]\n     name: IF-MIB.ifDescr\n     oid: 1.3.6.1.2.1.2.2.1.2.5\n     time: 1676302789.9729967\n     type: f\n     value: VirtualPortGroup0\n   }\n   IF-MIB.ifName: { [-]\n     name: IF-MIB.ifName\n     oid: 1.3.6.1.2.1.31.1.1.1.1.5\n     time: 1676302789.6655216\n     type: f\n     value: Vi0\n   }\n   IF-MIB.ifOperStatus: { [-]\n     name: IF-MIB.ifOperStatus\n     oid: 1.3.6.1.2.1.2.2.1.8.5\n     time: 1676302789.6655502\n     type: g\n     value: up\n   }\n}\n</code></pre>"},{"location":"microk8s/configuration/snmpv3-configuration/","title":"SNMPv3 configuration","text":""},{"location":"microk8s/configuration/snmpv3-configuration/#snmpv3-user-configuration","title":"SNMPv3 user configuration","text":"<p>Configuration of SNMP v3, when supported by the monitored devices, is the most secure choice available for authentication and data privacy. Each set of credentials will be stored as \u201cSecret\u201d objects in k8s, and will be referenced in <code>values.yaml</code>. This allows the secret to be created once, including automation by third-party password managers, then consumed without storing sensitive data in plain text.</p> <pre><code># &lt;secretname&gt;=Arbitrary name of the secret often the same as the username or prefixed with \"sc4snmp-\"\n# &lt;namespace&gt;=Namespace used to install sc4snmp\n# &lt;username&gt;=the SNMPv3 Username\n# &lt;key&gt;=key note must be at least 8 char long subject to target limitations\n# &lt;authProtocol&gt;=One of SHA (SHA1) or MD5 \n# &lt;privProtocol&gt;=One of AES or DES \n# Note MD5 and DES are considered insecure but must be supported for standards compliance\nmicrok8s kubectl create -n &lt;namespace&gt; secret generic &lt;secretname&gt; \\\n  --from-literal=userName=&lt;username&gt; \\\n  --from-literal=authKey=&lt;key&gt; \\\n  --from-literal=privKey=&lt;key&gt; \\\n  --from-literal=authProtocol=&lt;authProtocol&gt; \\\n  --from-literal=privProtocol=&lt;privProtocol&gt; \n</code></pre> <p>Configured credentials can be used in poller and trap services.  In service configuration, <code>secretname</code> needs to be provided. </p>"},{"location":"microk8s/configuration/step-by-step-poll/","title":"Step by Step polling example","text":""},{"location":"microk8s/configuration/step-by-step-poll/#an-example-of-a-polling-scenario","title":"An example of a polling scenario","text":"<p>In the following example, there are 4 hosts you want to poll from: </p> <ol> <li><code>10.202.4.201:161</code></li> <li><code>10.202.4.202:161</code></li> <li><code>10.202.4.203:161</code></li> <li><code>10.202.4.204:163</code></li> </ol> <p>To retrieve data from the device efficiently, first determine the specific data needed. Instead of walking through  the entire <code>1.3.6.1</code>, limit the walk to poll only the necessary data. Configure the <code>IF-MIB</code> family for interfaces and  the <code>UCD-SNMP-MIB</code> for CPU-related statistics. In the <code>scheduler</code> section of <code>values.yaml</code>, define the target group and  establish the polling parameters, known as the profile, to gather the desired data precisely. See the following example: </p> <pre><code>scheduler:\n  logLevel: \"INFO\"\n  profiles: |\n    small_walk:\n      condition:\n        type: \"walk\"\n      varBinds:\n        - [\"IF-MIB\"]\n        - [\"UCD-SNMP-MIB\"]\n    switch_profile:\n      frequency: 60\n      varBinds:\n        - [\"IF-MIB\", \"ifDescr\"]\n        - [\"IF-MIB\", \"ifAdminStatus\"]\n        - [\"IF-MIB\", \"ifOperStatus\"]\n        - [\"IF-MIB\", \"ifName\"]\n        - [\"IF-MIB\", \"ifAlias\"]\n        - [\"IF-MIB\", \"ifIndex\"]\n        - [\"IF-MIB\", \"ifInDiscards\"]\n        - [\"IF-MIB\", \"ifInErrors\"]\n        - [\"IF-MIB\", \"ifInOctets\"]\n        - [\"IF-MIB\", \"ifOutDiscards\"]\n        - [\"IF-MIB\", \"ifOutErrors\"]\n        - [\"IF-MIB\", \"ifOutOctets\"]\n        - [\"IF-MIB\", \"ifOutQLen\"]\n        - [\"UCD-SNMP-MIB\"]\n  groups: |\n    switch_group:\n      - address: 10.202.4.201\n      - address: 10.202.4.202\n      - address: 10.202.4.203\n      - address: 10.202.4.204\n        port: 163\n</code></pre> <p>It is required to pass the proper instruction of what to do for the SC4SNMP instance. To do this, append a new row to <code>poller.inventory</code>:</p> <pre><code>poller:\n  logLevel: \"WARN\"\n  inventory: |\n    address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n    switch_group,,2c,public,,,2000,small_walk;switch_profile,,\n</code></pre> <p>The provided example configuration will make:</p> <ol> <li>Walk devices from <code>switch_group</code> with <code>IF-MIB</code> and <code>UCD-SNMP-MIB</code> every 2000 seconds.</li> <li>Poll specific <code>IF-MIB</code> fields and the whole <code>UCD-SNMP-MIB</code> every 60 seconds.</li> </ol> <p>Info</p> <p>You can also limit the walk profile even more if you want to enhance the performance.</p> <p>It makes sense to put the textual values in the walk that are not required to be constantly monitored, and monitor only the metrics you are interested in:</p> <pre><code>small_walk:\n  condition:\n    type: \"walk\"\n  varBinds:\n    - [\"IF-MIB\", \"ifDescr\"]\n    - [\"IF-MIB\", \"ifAdminStatus\"]\n    - [\"IF-MIB\", \"ifOperStatus\"]\n    - [\"IF-MIB\", \"ifName\"]\n    - [\"IF-MIB\", \"ifAlias\"]\n    - [\"IF-MIB\", \"ifIndex\"]\nswitch_profile:\n  frequency: 60\n  varBinds:\n    - [\"IF-MIB\", \"ifInDiscards\"]\n    - [\"IF-MIB\", \"ifInErrors\"]\n    - [\"IF-MIB\", \"ifInOctets\"]\n    - [\"IF-MIB\", \"ifOutDiscards\"]\n    - [\"IF-MIB\", \"ifOutErrors\"]\n    - [\"IF-MIB\", \"ifOutOctets\"]\n    - [\"IF-MIB\", \"ifOutQLen\"]\n</code></pre> <p>Afterwards, every metric object will be enriched with the textual values gathered from a walk process. See here for more information about SNMP format.</p> <p>Now you are ready to reload SC4SNMP. Run the following <code>helm3 upgrade</code> command:</p> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <p>See the new pod with the following <code>Running</code> -&gt; <code>Completed</code> state command:</p> <pre><code>microk8s kubectl get pods -n sc4snmp -w\n</code></pre> <p>See the following example output: </p><pre><code>NAME                                                          READY   STATUS    RESTARTS   AGE\nsnmp-splunk-connect-for-snmp-worker-sender-5bc5cf864b-cwmfw   1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-worker-poller-76dcfb5896-d55pd   1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-worker-trap-68fb6476db-zl9rb     1/1     Running   0          5h52m\nsnmp-mibserver-58b558f5b4-zqf85                               1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-scheduler-57c5878444-k4qv4       1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-worker-poller-76dcfb5896-bzgrm   1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-trap-6cb76fcb49-l62f9            1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-trap-6cb76fcb49-d7c88            1/1     Running   0          5h52m\nsnmp-mongodb-869cc8586f-kw67q                                 2/2     Running   0          5h52m\nsnmp-redis-master-0                                           1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-inventory-g4bs7                  1/1     Running   0          3s\nsnmp-splunk-connect-for-snmp-inventory-g4bs7                  0/1     Completed   0          5s\nsnmp-splunk-connect-for-snmp-inventory-g4bs7                  0/1     Completed   0          6s\nsnmp-splunk-connect-for-snmp-inventory-g4bs7                  0/1     Completed   0          7s\n</code></pre> <p>Check the pod\u2019s logs to make sure everything was reloaded correctly, using the following command:</p> <pre><code>microk8s kubectl logs -f snmp-splunk-connect-for-snmp-inventory-g4bs7  -n sc4snmp\n</code></pre> <p>See the following example output:</p> <pre><code>Successfully connected to redis://snmp-redis-headless:6379/0\nSuccessfully connected to redis://snmp-redis-headless:6379/1\nSuccessfully connected to mongodb://snmp-mongodb:27017\nSuccessfully connected to http://snmp-mibserver/index.csv\n{\"message\": \"Loading inventory from /app/inventory/inventory.csv\", \"time\": \"2022-09-05T14:30:30.605420\", \"level\": \"INFO\"}\n{\"message\": \"New Record address='10.202.4.201' port=161 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\", \"time\": \"2022-09-05T14:30:30.607641\", \"level\": \"INFO\"}\n{\"message\": \"New Record address='10.202.4.202' port=161 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\", \"time\": \"2022-09-05T14:30:30.607641\", \"level\": \"INFO\"}\n{\"message\": \"New Record address='10.202.4.203' port=161 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\", \"time\": \"2022-09-05T14:30:30.607641\", \"level\": \"INFO\"}\n{\"message\": \"New Record address='10.202.4.204' port=163 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\", \"time\": \"2022-09-05T14:30:30.607641\", \"level\": \"INFO\"}\n</code></pre> <p>In some time (depending on how long the walk takes), we will see events using the following query:</p> <pre><code>| mpreview index=netmetrics | search profiles=switch_profile\n</code></pre> <p>When groups are used, we can also use querying by the group name, for example:</p> <pre><code>| mpreview index=netmetrics | search group=switch_group\n</code></pre> <p>Querying by profiles/group in Splunk is only possible in the metrics index. Every piece of data being sent by SC4SNMP is formed based on the MIB file\u2019s definition of the SNMP object\u2019s index. The object is forwarded to an event  index only if it does not have any metric value inside.</p> <p>The following is a Splunk <code>raw</code> metrics example:</p> <pre><code>{\n   \"frequency\":\"60\",\n   \"group\":\"switch_group\",\n   \"ifAdminStatus\":\"up\",\n   \"ifAlias\":\"1\",\n   \"ifDescr\":\"lo\",\n   \"ifIndex\":\"1\",\n   \"ifName\":\"lo\",\n   \"ifOperStatus\":\"up\",\n   \"ifPhysAddress\":\"1\",\n   \"ifType\":\"softwareLoopback\",\n   \"profiles\":\"switch_profile\",\n   \"metric_name:sc4snmp.IF-MIB.ifInDiscards\":21877,\n   \"metric_name:sc4snmp.IF-MIB.ifInErrors\":21840,\n   \"metric_name:sc4snmp.IF-MIB.ifInNUcastPkts\":14152789,\n   \"metric_name:sc4snmp.IF-MIB.ifInOctets\":1977814270,\n   \"metric_name:sc4snmp.IF-MIB.ifInUcastPkts\":220098191,\n   \"metric_name:sc4snmp.IF-MIB.ifInUnknownProtos\":1488029,\n   \"metric_name:sc4snmp.IF-MIB.ifLastChange\":124000001,\n   \"metric_name:sc4snmp.IF-MIB.ifMtu\":16436,\n   \"metric_name:sc4snmp.IF-MIB.ifOutDiscards\":21862,\n   \"metric_name:sc4snmp.IF-MIB.ifOutErrors\":21836,\n   \"metric_name:sc4snmp.IF-MIB.ifOutNUcastPkts\":14774727,\n   \"metric_name:sc4snmp.IF-MIB.ifOutOctets\":1346799625,\n   \"metric_name:sc4snmp.IF-MIB.ifOutQLen\":4294967295,\n   \"metric_name:sc4snmp.IF-MIB.ifOutUcastPkts\":74003841,\n   \"metric_name:sc4snmp.IF-MIB.ifSpeed\":10000000\n}\n</code></pre> <p>or</p> <pre><code>{\n   \"frequency\":\"60\",\n   \"group\":\"switch_group\",\n   \"laNames\":\"Load-1\",\n   \"profiles\":\"switch_profile\",\n   \"metric_name:sc4snmp.UCD-SNMP-MIB.laIndex\":1\n}\n</code></pre>"},{"location":"microk8s/configuration/trap-configuration/","title":"Traps","text":""},{"location":"microk8s/configuration/trap-configuration/#trap-configuration","title":"Trap Configuration","text":"<p>A trap service is a simple server that can handle SNMP traps sent by SNMP devices, such as routers or switches.   </p>"},{"location":"microk8s/configuration/trap-configuration/#trap-configuration-file","title":"Trap configuration file","text":"<p>The trap configuration is kept in the <code>values.yaml</code> file in section traps. <code>values.yaml</code> is used during the installation process for configuring Kubernetes values.</p> <p>See the following trap example configuration: </p><pre><code>traps:\n  communities:\n    1:\n      - public \n    2c:\n      - public\n      - homelab\n  usernameSecrets:\n    - secretv3\n    - sc4snmp-homesecure-sha-des\n\n  # Overrides the logLevel tag whose default is the chart\n  logLevel: \"WARN\"\n  # replicas: Number of replicas for trap container should be 2x number of nodes\n  replicas: 2\n  #loadBalancerIP: The IP address in the metallb pool\n  loadBalancerIP: 10.202.4.202\n  resources: \n    limits:\n      cpu: 500m\n      memory: 512Mi\n    requests:\n      cpu: 200m\n      memory: 256Mi  \n</code></pre>"},{"location":"microk8s/configuration/trap-configuration/#define-communities","title":"Define communities","text":"<p><code>communities</code> defines a version of an SNMP protocol and an SNMP community string, which should be used.  The <code>communities</code> key is split by protocol version, with <code>1</code> and <code>2c</code> as supported values. Under the <code>version</code> section, you can define the SNMP community string.</p> <p>See the following example:  </p><pre><code>traps:\n  communities:\n    1:\n      - public \n    2c:\n      - public\n      - homelab\n</code></pre>"},{"location":"microk8s/configuration/trap-configuration/#configure-user-secrets-for-snmpv3","title":"Configure user secrets for SNMPv3","text":"<p>The <code>usernameSecrets</code> key in the <code>traps</code> section defines the SNMPv3 secrets for the trap messages sent by the SNMP device.  <code>usernameSecrets</code> defines which secrets in \u201cSecret\u201d objects in k8s should be used, as a value it needs the name of \u201cSecret\u201d objects.  For more information on how to define the \u201cSecret\u201d object for SNMPv3, see SNMPv3 Configuration.</p> <p>See the following example: </p><pre><code>traps:\n    usernameSecrets:\n      - sc4snmp-homesecure-sha-aes\n      - sc4snmp-homesecure-sha-des\n</code></pre>"},{"location":"microk8s/configuration/trap-configuration/#define-security-engines-id-for-snmpv3","title":"Define security engines ID for SNMPv3","text":"<p>SNMPv3 TRAPs require the configuration of SNMP Engine ID of the TRAP sending application for the USM users table of  the TRAP receiving application for each USM user. The SNMP Engine ID is usually unique for the device, and the SC4SNMP  as a trap receiver has to be aware of which security engine IDs to accept. Define all of them under <code>traps.securityEngineId</code>  in <code>values.yaml</code>.</p> <p>By default, it is set to a one-element list: <code>[80003a8c04]</code>, for example: </p> <pre><code>traps:\n    securityEngineId: \n      - \"80003a8c04\"\n</code></pre> <p>The <code>securityEngineID</code> is a substitute of the <code>-e</code> variable in <code>snmptrap</code>. The following is an example of an SNMPv3 trap:</p> <pre><code>snmptrap -v3 -e 80003a8c04 -l authPriv -u snmp-poller -a SHA -A PASSWORD1 -x AES -X PASSWORD1 10.202.13.233 '' 1.3.6.1.2.1.2.2.1.1.1\n</code></pre>"},{"location":"microk8s/configuration/trap-configuration/#updating-trap-configuration","title":"Updating trap configuration","text":"<p>If you need to update part of the traps configuration, you can do it by editing the <code>values.yaml</code> and then running the following command to restart the pod deployment: </p><pre><code>microk8s kubectl rollout restart deployment snmp-splunk-connect-for-snmp-trap -n sc4snmp\n</code></pre> <p>Info</p> <p>The name of the deployment can differ based on the helm installation name.  This can be checked with the following command:  </p><pre><code>microk8s kubectl get deployments -n sc4snmp\n</code></pre>"},{"location":"microk8s/configuration/trap-configuration/#define-external-gateway-for-traps","title":"Define external gateway for traps","text":"<p>If you use SC4SNMP on a single machine, configure <code>loadBalancerIP</code>. <code>loadBalancerIP</code> is the IP address in the metallb pool.  See the following example:</p> <p></p><pre><code>traps:\n  loadBalancerIP: 10.202.4.202\n</code></pre> If you have enabled the Ipv6 you need to pass IP addresses for both IPv4 and IPv6. See the following example: <pre><code>traps:\n  loadBalancerIP: 10.202.4.202,2001:0DB8:AC10:FE01:0000:0000:0000:0001\n</code></pre> <p>If you want to use the SC4SNMP trap receiver in K8S cluster, configure <code>NodePort</code> instead. Use the following configuration:</p> <pre><code>traps:\n  service: \n    type: NodePort\n    externalTrafficPolicy: Cluster\n    nodePort: 30000\n</code></pre> <p>Using this method, the SNMP trap will always be forwarded to one of the trap receiver pods listening on port 30000 (like in the example above, you can configure to any other port). So, it does not matter that IP address of which node you use.  Adding nodePort will make it end up in the correct place everytime. </p> <p>A good practice is to create an IP floating address/Anycast pointing to the healthy nodes, so the traffic is forwarded in case of the failover. To do this, create an external LoadBalancer that balances the traffic between nodes.</p>"},{"location":"microk8s/configuration/trap-configuration/#define-number-of-traps-server-replica","title":"Define number of traps server replica","text":"<p><code>replicaCount</code> defines that the number of replicas per trap container should be 2 times the number of nodes. </p><pre><code>traps:\n  #For production deployments the value should be at least 2x the number of nodes\n  # Minimum 2 for a single node\n  # Minimum 6 for multi-node HA\n  replicaCount: 2\n</code></pre>"},{"location":"microk8s/configuration/trap-configuration/#define-log-level","title":"Define log level","text":"<p>The log level for trap can be set by changing the value for the <code>logLevel</code> key. The allowed values are<code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, or <code>ERROR</code>.  The default value is <code>WARNING</code>.</p>"},{"location":"microk8s/configuration/trap-configuration/#define-annotations","title":"Define annotations","text":"<p>In case you need to append some annotations to the <code>trap</code> service, you can do so by setting <code>traps.service.annotations</code>, for example:</p> <pre><code>traps:\n  service:\n    annotations:\n      annotation_key: annotation_value\n</code></pre>"},{"location":"microk8s/configuration/trap-configuration/#aggregate-traps","title":"Aggregate traps","text":"<p>In case you want to see traps events collected as one event inside Splunk, you can enable it by setting <code>traps.aggregateTrapsEvents</code>, for example: </p><pre><code>traps:\n  aggregateTrapsEvents: \"true\"\n</code></pre> Then the upgrade command can be executed."},{"location":"microk8s/configuration/values-params-description/","title":"Configurable values","text":""},{"location":"microk8s/configuration/values-params-description/#configurable-values","title":"Configurable values","text":""},{"location":"microk8s/configuration/values-params-description/#image-section","title":"Image Section","text":"<p>Detailed documentation about configuring images section can be found in kubernetes documentation. Below are the most common options:</p> Variable Description Example <code>repository</code> Defines a registry from which container image is downloaded <code>ghcr.io/splunk/splunk-connect-for-snmp/container</code> <code>tag</code> Defines different versions of images to pull <code>1.21.1</code> <code>pullPolicy</code> Defines when kubelet attempts to pull the specified image <code>Always</code>"},{"location":"microk8s/configuration/values-params-description/#ui-section","title":"UI section","text":"<p>Detailed documentation about configuring UI can be found in Enable GUI.</p> Variable Description Default <code>enable</code> Enabling GUI for user <code>false</code> <code>frontEnd</code> Section with configuration for frontEnd image <code>backEnd</code> Section with configuration for backEnd image <code>NodePort</code> Port number for accessing UI <code>frontEnd - 30001</code>, <code>backend - 30002</code> <code>image</code> Refer to Image Section <code>valuesFileDirectory</code> Absolute directory path on the host machine where configuration files from the GUI will be generated <code>valuesFileName</code> Full name of the file with configuration, stored inside the <code>valuesFileDirectory</code> <code>keepSectionFiles</code> Decides if additional configuration files should be generated <code>true</code>"},{"location":"microk8s/configuration/values-params-description/#splunk-section","title":"Splunk section","text":"Variable Description Default <code>enabled</code> Enables sending data to Splunk <code>true</code> <code>protocol</code> The protocol of the HEC endpoint: <code>https</code> or <code>http</code> <code>https</code> <code>port</code> The port of the HEC endpoint <code>8088</code> <code>host</code> IP address or a domain name of a Splunk instance <code>path</code> URN to Splunk collector <code>/services/collector</code> <code>token</code> Splunk HTTP Event Collector token <code>00000000-0000-0000-0000-000000000000</code> <code>insecureSSL</code> Checks for the certificate of the HEC endpoint when sending data over HTTPS <code>false</code> <code>sourcetypeTraps</code> Source type for trap events <code>sc4snmp:traps</code> <code>sourcetypePollingEvents</code> Source type for non-metric polling event <code>sc4snmp:event</code> <code>sourcetypePollingMetrics</code> Source type for metric polling event <code>sc4snmp:metric</code> <code>eventIndex</code> Name of the event index <code>netops</code> <code>metricsIndex</code> Name of the metrics index <code>netmetrics</code>"},{"location":"microk8s/configuration/values-params-description/#sim-section","title":"Sim section","text":"<p>Detailed documentation about configuring sim can be found in Splunk Infrastructure Monitoring.</p> Variable Description Default <code>enabled</code> Enables sending data to Splunk Observability Cloud/ SignalFx <code>false</code> <code>signalfxToken</code> Splunk Observability org access token <code>signalfxRealm</code> Splunk Observability realm to send telemetry data to <code>resources</code> CPU and memory limits and requests for pod <code>service.annotations</code> Annotations to append under sim service <code>secret.create</code> Option to configure <code>signalfxToken</code> and <code>signalfxRealm</code> as kubernetes secrets <code>true</code> <code>secret.name</code> Name of existing secret in kubernetes with <code>signalfxToken</code> and <code>signalfxRealm</code> <code>replicaCount</code> Number of created replicas when autoscaling is disabled <code>1</code> <code>autoscaling.enabled</code> Enables autoscaling for pods <code>false</code> <code>image</code> Refer to Image Section <code>autoscaling.minReplicas</code> Minimum number of running pods when autoscaling is enabled <code>autoscaling.maxReplicas</code> Maximum number of running pods when autoscaling is enabled <code>autoscaling.targetCPUUtilizationPercentage</code> CPU % threshold that must be exceeded on pods to spawn another replica <code>autoscaling.targetMemoryUtilizationPercentage</code> Memory % threshold that must be exceeded on pods to spawn another replica <code>podAntiAffinity</code> Kubernetes documentation <code>soft</code> <code>nodeSelector</code> Kubernetes documentation"},{"location":"microk8s/configuration/values-params-description/#scheduler","title":"Scheduler","text":"<p>Detailed documentation about configuring:</p> <ul> <li>scheduler can be found in Scheduler.</li> <li>groups can be found in Configuring Groups.</li> <li>profiles can be found in Configuring Profiles.</li> </ul> Variable Description Default <code>groups</code> Creates groups of host devices to collect data from <code>profiles</code> Definitions of data to poll from devices <code>customTranslations</code> Sets custom names for mapping MIB fields <code>resources</code> CPU and memory limits and requests for pod <code>logLevel</code> Log level for a scheduler <code>INFO</code> <code>tasksExpiryTime</code> Tasks expiration time in seconds <code>60</code> <code>communities</code> Defines a version of SNMP protocol and SNMP community string <code>podAntiAffinity</code> Kubernetes documentation <code>soft</code> <code>nodeSelector</code> Kubernetes documentation <code>tolerations</code> Kubernetes documentation"},{"location":"microk8s/configuration/values-params-description/#poller","title":"Poller","text":"<p>Detailed documentation about configuring poller can be found in Poller.</p> Variable Description Default <code>metricsIndexingEnabled</code> Appends OID indexes to metrics <code>false</code> <code>pollBaseProfiles</code> Enables polling base profiles <code>true</code> <code>maxOidToProcess</code> Maximum number of OIDs requested from SNMP Agent at once <code>70</code> <code>usernameSecrets</code> List of kubernetes secrets name that will be used for polling <code>inventory</code> List of configuration for polling <code>logLevel</code> Log level for a poller pod <code>INFO</code>"},{"location":"microk8s/configuration/values-params-description/#worker","title":"Worker","text":"<p>Detailed documentation about configuring worker can be found in Worker.</p> Variable Description Default <code>poller</code> Section with configuration for worker poller pods <code>trap</code> Section with configuration for worker trap pods <code>sender</code> Section with configuration for worker sender pods <code>x.replicaCount</code> Number of pod replicas when autoscaling is disabled poller/trap - <code>2</code>, sender - <code>1</code> <code>x.concurrency</code> Minimum number of threads in a pod <code>4</code> <code>x.prefetch</code> Number of tasks consumed from the queue at once poller - <code>1</code>, traps/sender - <code>30</code> <code>x.autoscaling.enabled</code> Enables autoscaling for pod poller - <code>false</code> <code>x.autoscaling.minReplicas</code> Minimum number of running pods when autoscaling is enabled poller - <code>2</code> <code>x.autoscaling.maxReplicas</code> Maximum number of running pods when autoscaling is enabled poller - <code>10</code> <code>x.autoscaling.targetCPUUtilizationPercentage</code> CPU % threshold that must be exceeded on pods to spawn another replica poller - <code>80</code> <code>x.resources</code> CPU and memory limits and requests for pod <code>trap.resolveAddress.cacheSize</code> Maximum number of records in cache <code>500</code> <code>trap.resolveAddress.cacheTTL</code> Time to live of the cached record in seconds <code>1800</code> <code>livenessProbe</code> Liveness probes are used in Kubernetes to know when a pod is alive or dead <code>readinessProbe</code> Readiness probes are used to know when a pod is ready to serve traffic <code>xProbe.enabled</code> If livenessProbe or readinessProbe are enabled <code>xProbe.exec.command</code> The exec command for the probe to run in the container Check <code>values.yaml</code> <code>xProbe.initialDelaySeconds</code> Number of seconds after the container has started before probes are initiated livenessProbe - <code>80</code>, readinessProbe - <code>30</code> <code>xProbe.periodSeconds</code> Frequency of performing the probe in seconds livenessProbe - <code>10</code>, readinessProbe - <code>5</code> <code>taskTimeout</code> Task timeout in seconds when process takes a long time <code>2400</code> <code>walkRetryMaxInterval</code> Maximum time interval between walk attempts <code>180</code> <code>walkMaxRetries</code> Maximum number of walk retries <code>5</code> <code>ignoreNotIncreasingOid</code> Ignoring <code>occurred: OID not increasing</code> issues for hosts specified in the array <code>profilesReloadDelay</code> Delay of polling profiles after inventory reload <code>60</code> <code>logLevel</code> Log level for workers <code>INFO</code> <code>udpConnectionTimeout</code> Timeout for SNMP operations in seconds <code>3</code> <code>ignoreEmptyVarbinds</code> Ignores \u201cEmpty SNMP response message\u201d in responses <code>false</code> <code>podAntiAffinity</code> Kubernetes documentation <code>soft</code> <code>nodeSelector</code> Kubernetes documentation <code>tolerations</code> Kubernetes documentation"},{"location":"microk8s/configuration/values-params-description/#inventory","title":"Inventory","text":"<p>Detailed documentation about configuring inventory can be found in Poller.</p> Variable Description Default <code>secret.create</code> Enables creation of the kubernetes secret <code>true</code> <code>secret.name</code> Name of existing secret in kubernetes <code>service.annotations</code> Annotations to append under inventory service <code>resources</code> CPU and memory limits and requests for pod <code>nodeSelector</code> Kubernetes documentation <code>tolerations</code> Kubernetes documentation"},{"location":"microk8s/configuration/values-params-description/#traps","title":"Traps","text":"<p>Detailed documentation about configuring traps can be found in Traps.</p> Variable Description Default <code>replicaCount</code> Number of created replicas when autoscaling disabled <code>2</code> <code>usernameSecrets</code> Defines SNMPv3 secrets for trap messages sent by SNMP device <code>securityEngineId</code> SNMP Engine ID of the TRAP sending application <code>80003a8c04</code> <code>aggregateTrapsEvents</code> Enables collecting traps events as one event inside Splunk <code>false</code> <code>communities</code> Defines a version of SNMP protocol and SNMP community string <code>service.annotations</code> Annotations to append under traps service <code>service.usemetallb</code> Enables using metallb <code>true</code> <code>service.metallbsharingkey</code> Sets metallb.universe.tf/allow-shared-ip annotation in trap service <code>splunk-connect</code> <code>service.type</code> Kubernetes documentation <code>LoadBalancer</code> <code>service.port</code> Port of the service to use for IPv4 <code>162</code> <code>service.nodePort</code> Port when the <code>service.type</code> is <code>nodePort</code> <code>30000</code> <code>service.externalTrafficPolicy</code> Controls how Kubernetes routes traffic <code>Local</code> <code>service.ipv6Port</code> Port of the service to use for IPv6 <code>162</code> <code>service.ipv6NodePort</code> Port when the <code>service.type</code> is <code>nodePort</code> and IPv6 is enabled <code>2163</code> <code>loadBalancerIP</code> Sets loadBalancer IP address in the metallb pool <code>30001</code> <code>ipFamilyPolicy</code> Specifies if the service is dual stack or single stack <code>SingleStack</code> <code>ipFamilies</code> Defines the address families used for chosen <code>ipFamilyPolicy</code> <code>IPv4</code> <code>resources</code> CPU and memory limits and requests for pod <code>autoscaling.enabled</code> Enables autoscaling for pods <code>false</code> <code>autoscaling.minReplicas</code> Minimum number of running pods when autoscaling is enabled <code>1</code> <code>autoscaling.maxReplicas</code> Maximum number of running pods when autoscaling is enabled <code>100</code> <code>autoscaling.targetCPUUtilizationPercentage</code> CPU % threshold that must be exceeded on pods to spawn another replica <code>80</code> <code>autoscaling.targetMemoryUtilizationPercentage</code> Memory % threshold that must be exceeded on pods to spawn another replica <code>logLevel</code> Log level for a traps pod <code>INFO</code> <code>podAntiAffinity</code> Kubernetes documentation <code>soft</code> <code>nodeSelector</code> Kubernetes documentation <code>tolerations</code> Kubernetes documentation"},{"location":"microk8s/configuration/values-params-description/#serviceaccount","title":"serviceAccount","text":"Variable Description Default <code>create</code> Specifies whether a service account should be created <code>true</code> <code>annotations</code> Annotations to add to the service account <code>name</code> The name of the service account to use."},{"location":"microk8s/configuration/values-params-description/#mongodb","title":"MongoDb","text":"<p>Detailed documentation about configuring mongodb can be found in MongoDB. It is advised to  not change those settings.</p>"},{"location":"microk8s/configuration/values-params-description/#redis","title":"Redis","text":"<p>Detailed documentation about configuring redis can be found in Redis. It is advised to not  change those settings.</p>"},{"location":"microk8s/configuration/values-params-description/#others","title":"Others","text":"Variable Description Default <code>imagePullSecrets</code> Kubernetes documentation  <code>useDeprecatedAPI</code> Enables older version of Kubernetes to use <code>false</code> <code>commonAnnotations</code> Annotations added to all services"},{"location":"microk8s/configuration/worker-configuration/","title":"Worker","text":""},{"location":"microk8s/configuration/worker-configuration/#worker-configuration","title":"Worker Configuration","text":"<p>The <code>worker</code> is a kubernetes pod which is responsible for the actual execution of polling, processing trap messages, and sending  data to Splunk.</p>"},{"location":"microk8s/configuration/worker-configuration/#worker-types","title":"Worker types","text":"<p>SC4SNMP has two base functionalities: monitoring traps and polling. These operations are handled by 3 types of workers:</p> <ol> <li> <p>The <code>trap</code> worker consumes all the trap related tasks produced by the trap pod. </p> </li> <li> <p>The <code>poller</code> worker consumes all the tasks related to polling.</p> </li> <li> <p>The <code>sender</code> worker handles sending data to Splunk. You need to always have at least one sender pod running.</p> </li> </ol>"},{"location":"microk8s/configuration/worker-configuration/#worker-configuration-file","title":"Worker configuration file","text":"<p>Worker configuration is kept in the <code>values.yaml</code> file in the <code>worker</code> section. <code>worker</code> has 3 subsections: <code>poller</code>, <code>sender</code>, and <code>trap</code>, that refer to the workers types. <code>values.yaml</code> is used during the installation process for configuring Kubernetes values. The <code>worker</code> default configuration is the following: </p> <pre><code>worker:\n  # workers are responsible for the actual execution of polling, processing trap messages, and sending data to Splunk.\n  # More: https://splunk.github.io/splunk-connect-for-snmp/main/microk8s/configuration/worker-configuration/\n\n  # The poller worker consumes all the tasks related to polling\n  poller:\n    # number of the poller replicas when autoscaling is set to false\n    replicaCount: 2\n    # minimum number of threads in a pod\n    concurrency: 4\n    # how many tasks are consumed from the queue at once\n    prefetch: 1\n    autoscaling:\n      # enabling autoscaling for poller worker pods\n      enabled: false\n      # minimum number of running poller worker pods when autoscaling is enabled\n      minReplicas: 2\n      # maximum number of running poller worker pods when autoscaling is enabled\n      maxReplicas: 10\n      # CPU % threshold that must be exceeded on poller worker pods to spawn another replica\n      targetCPUUtilizationPercentage: 80\n\n    resources:\n      # the resources limits for poller worker container\n      limits:\n        cpu: 500m\n      # the resources requests for poller worker container\n      requests:\n        cpu: 250m\n\n  # The trap worker consumes all the trap related tasks produced by the trap pod\n  trap:\n    # number of the trap replicas when autoscaling is set to false\n    replicaCount: 2\n    # Use reverse dns lookup of trap ip address and send the hostname to splunk\n    resolveAddress:\n      enabled: false\n      cacheSize: 500 # maximum number of records in cache\n      cacheTTL: 1800 # time to live of the cached record in seconds\n    # minimum number of threads in a pod\n    concurrency: 4\n    # how many tasks are consumed from the queue at once\n    prefetch: 30\n    autoscaling:\n      # enabling autoscaling for trap worker pods\n      enabled: false\n      # minimum number of running trap worker pods when autoscaling is enabled\n      minReplicas: 2\n      # maximum number of running trap worker pods when autoscaling is enabled\n      maxReplicas: 10\n      # CPU % threshold that must be exceeded on traps worker pods to spawn another replica\n      targetCPUUtilizationPercentage: 80\n    resources:\n      # the resources limits for trap worker container\n      limits:\n        cpu: 500m\n      requests:\n        # the resources requests for trap worker container\n        cpu: 250m\n  # The sender worker handles sending data to Splunk\n  sender:\n    # number of the sender replicas when autoscaling is set to false\n    replicaCount: 1\n    # minimum number of threads in a pod\n    concurrency: 4\n    # how many tasks are consumed from the queue at once\n    prefetch: 30\n    autoscaling:\n      # enabling autoscaling for sender worker pods\n      enabled: false\n      # minimum number of running sender worker pods when autoscaling is enabled\n      minReplicas: 2\n      # maximum number of running sender worker pods when autoscaling is enabled\n      maxReplicas: 10\n      # CPU % threshold that must be exceeded on sender worker pods to spawn another replica\n      targetCPUUtilizationPercentage: 80\n    resources:\n      # the resources limits for sender worker container\n      limits:\n        cpu: 500m\n        # the resources requests for sender worker container\n      requests:\n        cpu: 250m\n  # Liveness probes are used in Kubernetes to know when a pod is alive or dead.\n  # A pod can be in a dead state for a number of reasons;\n  # the application could be crashed, some error in the application etc.\n  livenessProbe:\n    # whether it should be turned on or not\n    enabled: false\n    # The exec command for the liveness probe to run in the container.\n    exec:\n      command:\n        - sh\n        - -c\n        - test $(($(date +%s) - $(stat -c %Y /tmp/worker_heartbeat))) -lt 10\n    # Number of seconds after the container has started before liveness probes are initiated.\n    initialDelaySeconds: 80\n    # How often (in seconds) to perform the probe.\n    periodSeconds: 10\n\n  # Readiness probes are used to know when a pod is ready to serve traffic.\n  # Until a pod is ready, it won't receive traffic from Kubernetes services.\n  readinessProbe:\n    # whether it should be turned on or not\n    enabled: false\n    # The exec command for the readiness probe to run in the container.\n    exec:\n      command:\n        - sh\n        - -c\n        - test -e /tmp/worker_ready\n    # Number of seconds after the container has started before readiness probes are initiated.\n    initialDelaySeconds: 30\n    # How often (in seconds) to perform the probe.\n    periodSeconds: 5\n\n\n  # task timeout in seconds (usually necessary when walk process takes a long time)\n  taskTimeout: 2400\n  # maximum time interval between walk attempts\n  walkRetryMaxInterval: 180\n  # maximum number of walk retries\n  walkMaxRetries: 5\n  # ignoring `occurred: OID not increasing` issues for hosts specified in the array, ex:\n  #   ignoreNotIncreasingOid:\n  #    - \"127.0.0.1:164\"\n  #    - \"127.0.0.6\"\n  ignoreNotIncreasingOid: []\n  # logging level, possible options: DEBUG, INFO, WARNING, ERROR, CRITICAL, or FATAL\n  logLevel: \"INFO\"\n  podAntiAffinity: soft\n  # udpConnectionTimeout timeout in seconds for SNMP operations\n  udpConnectionTimeout: 3\n\n  # in case of seeing \"Empty SNMP response message\" this variable can be set to true\n  ignoreEmptyVarbinds: false\n</code></pre> <p>All parameters are described in the Worker parameters section.</p>"},{"location":"microk8s/configuration/worker-configuration/#worker-scaling","title":"Worker scaling","text":"<p>You can adjust worker pods in two ways: set fixed value in <code>replicaCount</code>, or enable <code>autoscaling</code>, which scales pods automatically. </p>"},{"location":"microk8s/configuration/worker-configuration/#real-life-scenario-i-use-sc4snmp-for-only-trap-monitoring-and-i-want-to-use-my-resources-effectively","title":"Real life scenario: I use SC4SNMP for only trap monitoring, and I want to use my resources effectively.","text":"<p>If you do not use polling at all, set <code>worker.poller.replicaCount</code> to <code>0</code>. If you want to use polling in the future, you need to increase <code>replicaCount</code>.  To monitor traps, adjust <code>worker.trap.replicaCount</code> depending on your needs and <code>worker.sender.replicaCount</code> to send traps to Splunk. Usually, you need significantly fewer sender pods than trap pods.</p> <p>The following is an example of <code>values.yaml</code> without using autoscaling:</p> <pre><code>worker:\n  trap:\n    replicaCount: 4\n  sender:\n    replicaCount: 1\n  poller:\n    replicaCount: 0\n  logLevel: \"WARNING\"\n</code></pre> <p>The following is an example of <code>values.yaml</code> with autoscaling:</p> <pre><code>worker:\n  trap:\n    autoscaling:\n      enabled: true\n      minReplicas: 4\n      maxReplicas: 10\n      targetCPUUtilizationPercentage: 80\n  sender:\n    autoscaling:\n      enabled: true\n      minReplicas: 2\n      maxReplicas: 5\n      targetCPUUtilizationPercentage: 80\n  poller:\n    replicaCount: 0\n  logLevel: \"WARNING\"\n</code></pre> <p>In the previous example, both trap and sender pods are autoscaled. During an upgrade process, the number of pods is created through <code>minReplicas</code>, and then new ones are created only if the CPU threshold exceeds the <code>targetCPUUtilizationPercentage</code>, which by default is 80%. This solution helps you to keep  resources usage adjusted to what you actually need. </p> <p>After the helm upgrade process, you will see <code>horizontalpodautoscaler</code> in <code>microk8s kubectl get all -n sc4snmp</code>:</p> <pre><code>NAME                                                                             REFERENCE                                               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/snmp-mibserver                               Deployment/snmp-mibserver                               1%/80%    1         3         1          97m\nhorizontalpodautoscaler.autoscaling/snmp-splunk-connect-for-snmp-worker-sender   Deployment/snmp-splunk-connect-for-snmp-worker-sender   1%/80%    2         5         2          28m\nhorizontalpodautoscaler.autoscaling/snmp-splunk-connect-for-snmp-worker-trap     Deployment/snmp-splunk-connect-for-snmp-worker-trap     1%/80%    4         10        4          28m\n</code></pre> <p>If you see <code>&lt;unknown&gt;/80%</code> in the <code>TARGETS</code> section instead of the CPU percentage, you probably do not have the <code>metrics-server</code> add-on enabled. Enable it using <code>microk8s enable metrics-server</code>.</p>"},{"location":"microk8s/configuration/worker-configuration/#real-life-scenario-i-have-a-significant-delay-in-polling","title":"Real life scenario: I have a significant delay in polling","text":"<p>Sometimes when polling is configured to be run frequently and on many devices, workers get overloaded  and there is a delay in delivering data to Splunk. To avoid these situations, scale poller and sender pods. Because of the walk cycles, (walk is a costly operation that is only run once in a while), poller workers require more resources  for a short time. For this reason, enabling autoscaling is recommended. </p> <p>See the following example of <code>values.yaml</code> with autoscaling:</p> <pre><code>worker:\n  trap:\n    autoscaling:\n      enabled: true\n      minReplicas: 4\n      maxReplicas: 10\n      targetCPUUtilizationPercentage: 80\n  sender:\n    autoscaling:\n      enabled: true\n      minReplicas: 2\n      maxReplicas: 5\n      targetCPUUtilizationPercentage: 80\n  poller:\n    autoscaling:\n      enabled: true\n      minReplicas: 2\n      maxReplicas: 20\n      targetCPUUtilizationPercentage: 80\n  logLevel: \"WARNING\"\n</code></pre> <p>Remember that the system will not scale itself infinitely. There is a finite amount of resources that you can allocate.  By default, every worker has configured the following resources:</p> <pre><code>    resources:\n      limits:\n        cpu: 500m\n      requests:\n        cpu: 250m\n</code></pre>"},{"location":"microk8s/configuration/worker-configuration/#i-have-autoscaling-enabled-and-experience-problems-with-mongo-and-redis-pod","title":"I have autoscaling enabled and experience problems with Mongo and Redis pod","text":"<p>If MongoDB and Redis pods are crushing, and some of the pods are in an infinite <code>Pending</code> state, that means  you have exhausted your resources and SC4SNMP cannot scale more. You should decrease the number of <code>maxReplicas</code> in  workers, so that it is not going beyond the available CPU.</p>"},{"location":"microk8s/configuration/worker-configuration/#i-do-not-know-how-to-set-autoscaling-parameters-and-how-many-replicas-i-need","title":"I do not know how to set autoscaling parameters and how many replicas I need","text":"<p>The best way to see if pods are overloaded is to run the following command:</p> <pre><code>microk8s kubectl top pods -n sc4snmp\n</code></pre> <pre><code>NAME                                                          CPU(cores)   MEMORY(bytes)   \nsnmp-mibserver-7f879c5b7c-nnlfj                               1m           3Mi             \nsnmp-mongodb-869cc8586f-q8lkm                                 18m          225Mi           \nsnmp-redis-master-0                                           10m          2Mi             \nsnmp-splunk-connect-for-snmp-scheduler-558dccfb54-nb97j       2m           136Mi           \nsnmp-splunk-connect-for-snmp-trap-5878f89bbf-24wrz            2m           129Mi           \nsnmp-splunk-connect-for-snmp-trap-5878f89bbf-z9gd5            2m           129Mi           \nsnmp-splunk-connect-for-snmp-worker-poller-599c7fdbfb-cfqjm   260m         354Mi           \nsnmp-splunk-connect-for-snmp-worker-poller-599c7fdbfb-ztf7l   312m         553Mi           \nsnmp-splunk-connect-for-snmp-worker-sender-579f796bbd-vmw88   14m           257Mi           \nsnmp-splunk-connect-for-snmp-worker-trap-5474db6fc6-46zhf     3m           259Mi           \nsnmp-splunk-connect-for-snmp-worker-trap-5474db6fc6-mjtpv     4m           259Mi   \n</code></pre> <p>Here you can see how much CPU and Memory is being used by the pods. If the CPU is close to 500m, which is the limit for one pod by default, enable autoscaling/increase maxReplicas or increase replicaCount with autoscaling off.</p> <p>See Horizontal Autoscaling to adjust the maximum replica value to the resources you have.</p> <p>See Scaling with Microk8s for more information.</p>"},{"location":"microk8s/configuration/worker-configuration/#reverse-dns-lookup-in-trap-worker","title":"Reverse DNS lookup in trap worker","text":"<p>If you want to see the hostname instead of the IP address of the incoming traps in Splunk, you can enable reverse dns lookup for the incoming traps using the following configuration:</p> <pre><code>worker:\n  trap:\n    resolveAddress:\n      enabled: true\n      cacheSize: 500 # maximum number of records in cache\n      cacheTTL: 1800 # time to live of the cached record in seconds\n</code></pre> <p>Trap worker uses in memory cache to store the results of the reverse dns lookup. If you restart the worker, the cache will be cleared.</p>"},{"location":"microk8s/configuration/worker-configuration/#worker-parameters","title":"Worker parameters","text":"Variable Description Default worker.poller.replicaCount Number of poller worker replicas 2 worker.poller.concurrency Minimum number of threads in a poller worker pod 4 worker.poller.prefetch Number of tasks consumed from the queue at once 1 worker.poller.autoscaling.enabled Enabling autoscaling for poller worker pods false worker.poller.autoscaling.minReplicas Minimum number of running poller worker pods when autoscaling is enabled 2 worker.poller.autoscaling.maxReplicas Maximum number of running poller worker pods when autoscaling is enabled 10 worker.poller.autoscaling.targetCPUUtilizationPercentage CPU % threshold that must be exceeded on poller worker pods to spawn another replica 80 worker.poller.resources.limits The resources limits for poller worker container cpu: 500m worker.poller.resources.requests The requested resources for poller worker container cpu: 250m worker.trap.replicaCount Number of trap worker replicas 2 worker.trap.concurrency Minimum number of threads in a trap worker pod 4 worker.trap.prefetch Number of tasks consumed from the queue at once 30 worker.trap.resolveAddress.enabled Enable reverse dns lookup of the IP address of the processed trap false worker.trap.resolveAddress.cacheSize Maximum number of reverse dns lookup result records stored in cache 500 worker.trap.resolveAddress.cacheTTL Time to live of the cached reverse dns lookup record in seconds 1800 worker.trap.autoscaling.enabled Enabling autoscaling for trap worker pods false worker.trap.autoscaling.minReplicas Minimum number of running trap worker pods when autoscaling is enabled 2 worker.trap.autoscaling.maxReplicas Maximum number of running trap worker pods when autoscaling is enabled 10 worker.trap.autoscaling.targetCPUUtilizationPercentage CPU % threshold that must be exceeded on trap worker pods to spawn another replica 80 worker.trap.resources.limits The resource limit for the poller worker container cpu: 500m worker.trap.resources.requests The requested resources for the poller worker container cpu: 250m worker.sender.replicaCount The number of sender worker replicas 1 worker.sender.concurrency Minimum number of threads in a sender worker pod 4 worker.sender.prefetch Number of tasks consumed from the queue at once 30 worker.sender.autoscaling.enabled Enabling autoscaling for sender worker pods false worker.sender.autoscaling.minReplicas Minimum number of running sender worker pods when autoscaling is enabled 2 worker.sender.autoscaling.maxReplicas Maximum number of running sender worker pods when autoscaling is enabled 10 worker.sender.autoscaling.targetCPUUtilizationPercentage CPU % threshold that must be exceeded on sender worker pods to spawn another replica 80 worker.sender.resources.limits The resource limit for the poller worker container cpu: 500m worker.sender.resources.requests The requested resources for the poller worker container cpu: 250m worker.livenessProbe.enabled Whether the liveness probe is enabled false worker.livenessProbe.exec.command The exec command for the liveness probe to run in the container Check values.yaml worker.livenessProbe.initialDelaySeconds Number of seconds after the container has started before liveness probe is initiated 80 worker.livenessProbe.periodSeconds Frequency of performing the probe in seconds 10 worker.readinessProbe.enabled Whether the readiness probe should be turned on or not false worker.readinessProbe.exec.command The exec command for the readiness probe to run in the container Check values.yaml worker.readinessProbe.initialDelaySeconds Number of seconds after the container has started before readiness probe is initiated 30 worker.readinessProbe.periodSeconds Frequency of performing the probe in seconds 5 worker.taskTimeout Task timeout in seconds when process takes a long time 2400 worker.walkRetryMaxInterval Maximum time interval between walk attempts 180 worker.walkMaxRetries Maximum number of walk retries 5 worker.ignoreNotIncreasingOid Ignoring <code>occurred: OID not increasing</code> issues for hosts specified in the array [] worker.logLevel Logging level, possible options: DEBUG, INFO, WARNING, ERROR, CRITICAL, or FATAL INFO worker.podAntiAffinity Kubernetes documentation soft worker.udpConnectionTimeout Timeout for SNMP operations in seconds 3 worker.ignoreEmptyVarbinds Ignores \u201cEmpty SNMP response message\u201d in responses false"},{"location":"microk8s/gui/apply-changes/","title":"Apply changes","text":""},{"location":"microk8s/gui/apply-changes/#apply-changes","title":"Apply changes","text":"<p>In order to apply changes from the GUI to the core SC4SNMP, press the <code>Apply changes</code> button. Update can be made minimum 5 minutes after the previous one was applied. If the <code>Apply changes</code> button is clicked earlier, new update will be scheduled automatically  and the following message with ETA will be displayed:</p> <p></p> <p>Scheduled update triggers new kubernetes job <code>job/snmp-splunk-connect-for-snmp-inventory</code>. If the ETA elapsed and the  previous <code>job/snmp-splunk-connect-for-snmp-inventory</code> is still present in the <code>sc4snmp</code> kubernetes namespace, creation of the new job will be retried 10 times. If <code>Apply changes</code> is clicked during retries, the following message will be displayed:</p> <p></p>"},{"location":"microk8s/gui/enable-gui/","title":"Enable GUI","text":""},{"location":"microk8s/gui/enable-gui/#sc4snmp-gui","title":"SC4SNMP GUI","text":"<p>SC4SNMP GUI is deployed in kubernetes and can be accessed through the web browser.</p>"},{"location":"microk8s/gui/enable-gui/#enabling-gui","title":"Enabling GUI","text":"<p>To enable GUI, the following section must be added to <code>values.yaml</code> file and <code>UI.enable</code> variable must be set to <code>true</code>:</p> <pre><code>UI:\n  enable: true\n  frontEnd:\n    NodePort: 30001\n    pullPolicy: \"Always\"\n  backEnd:\n    NodePort: 30002\n    pullPolicy: \"Always\"\n  valuesFileDirectory: \"\"\n  valuesFileName: \"\"\n  keepSectionFiles: true\n</code></pre> <ul> <li><code>NodePort</code>: port number on which GUI will be accessible. It has to be from a range <code>30000-32767</code>.</li> <li><code>pullPolicy</code>: kubernetes pull policy.</li> <li><code>valuesFileDirectory</code>: this is an obligatory field if UI is used. It is an absolute directory path on the host machine  where configuration files from the GUI will be generated. It is used to keep all the changes from the GUI so that users can  easily switch back from using UI to the current sc4snmp version. It is advised to create new folder for those files,  because this directory is mounted to the Kubernetes pod and GUI application has full write access to this directory.</li> <li><code>valuesFileName</code>: [OPTIONAL] full name of the file with configuration (e.g. <code>values.yaml</code>) that is stored inside the  <code>valuesFileDirectory</code> directory. If this file name is provided, and it exists in this directory, then GUI will update  appropriate sections in provided <code>values.yaml</code> file. If this file name is not provided, or provided file name cannot be found inside <code>valuesFileDirectory</code> then inside that directory there will be created three files with the latest GUI configuration of groups, profiles and inventory. Those configuration can be copied and pasted to the appropriate sections in the original <code>values.yaml</code> file.</li> </ul> <p>Template of initial <code>values.yaml</code>:</p> <pre><code>```yaml\nscheduler:\n  profiles: |\n\n  groups: |\n\npoller:\n  inventory: |-\n```\n</code></pre> <p>This part of configuration can be also pasted to the <code>values.yaml</code> used for SC4SNMP installation.</p> <ul> <li><code>keepSectionFiles</code>:  if valid <code>valuesFileName</code> was provided then by setting this variable to <code>true</code> or <code>false</code> user can  decide whether to keep additional files with configuration of groups, profiles and inventory. If valid <code>valuesFileName</code>  was NOT provided, then those files are created regardless of this variable.</li> </ul> <p>To access the GUI, in the browser type the IP address of your Microk8s cluster followed by the NodePort number from the  frontEnd section, e.g. <code>192.168.123.13:30001</code>.</p>"},{"location":"microk8s/gui/groups-gui/","title":"Configuring Groups","text":""},{"location":"microk8s/gui/groups-gui/#configuring-groups-in-gui","title":"Configuring groups in GUI","text":"<p>SC4SNMP groups can be configured in <code>Groups</code> tab.</p> <p></p> <p>After pressing <code>Add group</code> button or plus sign next to the <code>Group</code>, new group can be added.</p> <p></p> <p>Configured groups are displayed on the left-hand side, under the <code>Group name</code> label. After clicking on the group name,  all devices belonging to the given group are displayed. To add a new device, click the plus sign next to the group name.  Configuration of the device is the same as in the <code>values.yaml</code> file. For details check Configuring groups.</p> <p></p> <p>To edit a group name, click the pencil icon next to the group name.</p> <p></p> <p>To edit device, click the pencil icon in the row of the given device.</p> <p></p>"},{"location":"microk8s/gui/inventory-gui/","title":"Configuring Inventory","text":""},{"location":"microk8s/gui/inventory-gui/#configuring-inventory-in-gui","title":"Configuring inventory in GUI","text":"<p>SC4SNMP inventory can be configured in <code>Inventory</code> tab.</p> <p></p> <p>After pressing <code>Add device/group</code> button, new single device or group can be added. Configuration of the device is the same as in the <code>inventory.yaml</code> file. For details check Poller configuration.</p> <p></p> <p>To edit a device or group, click the pencil icon next in the desired row.</p> <p> </p>"},{"location":"microk8s/gui/profiles-gui/","title":"Configuring Profiles","text":""},{"location":"microk8s/gui/profiles-gui/#configuring-profiles-in-gui","title":"Configuring profiles in GUI","text":"<p>SC4SNMP profiles can be configured in <code>Profiles</code> tab.</p> <p></p> <p>After pressing <code>Add profile</code> button, new profile will be added. Configuration of the profile is the same as in the <code>values.yaml</code> file. For details check Configuring profiles.</p> <p></p> <p>Type of the profile can be changed:</p> <p></p> <p>Examples of configuration of <code>Smart</code> and <code>Conditional</code> profiles:</p> <p> </p> <p>All configured profiles can be edited by clicking the pencil icon:</p> <p></p>"},{"location":"microk8s/mk8s/k8s-microk8s-scaling/","title":"Scaling with Microk8s","text":""},{"location":"microk8s/mk8s/k8s-microk8s-scaling/#scaling-snmp-with-microk8s","title":"Scaling SNMP with microk8s","text":"<p>The following guide is to present how to bootstrap master and replica nodes for microk8s cluster and to explore the possibilities of scaling SC4SNMP.</p>"},{"location":"microk8s/mk8s/k8s-microk8s-scaling/#scaling-criteria","title":"Scaling criteria","text":"<p>Below is the formula that can help with deciding when to scale the system.</p> <p><code>2 * periodic_task_exec_time * inventory_size &gt;= workers_count * task_period</code></p> <p>where:</p> <ul> <li><code>inventory_size</code> - Amount of item in inventory (<code>values.yaml</code>).</li> <li><code>workers_count</code> - Amount of running workers for <code>polling</code> / <code>walk</code> (pod workers).</li> <li><code>task_period</code> - <code>walk</code> / <code>polling</code> period time (<code>values.yaml</code>).</li> <li><code>periodic_task_exec_time</code> - Execution time of <code>polling</code> / <code>walk</code> task (metrics at screenshot).</li> </ul> <p></p> <p>If the left side of equation is higher you need to scale <code>workers_count</code> or increase <code>task_period</code>.</p>"},{"location":"microk8s/mk8s/k8s-microk8s-scaling/#make-microk8s-cluster","title":"Make microk8s cluster","text":""},{"location":"microk8s/mk8s/k8s-microk8s-scaling/#bootstrap-master-node","title":"Bootstrap master node","text":"<ol> <li> <p>Setup master node using following guide. </p> </li> <li> <p>Generate joining token for replicas:</p> </li> </ol> <pre><code>microk8s add-node\n</code></pre> <ol> <li>After running <code>add-node</code> on stdout you will get a command (like <code>microk8s join &lt;master_node&gt;:25000/&lt;token&gt;</code>) that you need to remember.</li> </ol>"},{"location":"microk8s/mk8s/k8s-microk8s-scaling/#bootstrap-replica-nodes","title":"Bootstrap replica nodes","text":"<ol> <li>Installing microk8s on replica node: </li> </ol> <pre><code>sudo snap install microk8s --classic --channel=1.30/stable\nsudo usermod -a -G microk8s $USER\nsudo chown -f -R $USER ~/.kube\nsu - $USER\n</code></pre> <ol> <li>After running <code>add-node</code> on master node you will get on stdout command that you need to run on your replica node:</li> </ol> <pre><code>microk8s join &lt;master_node&gt;:25000/&lt;token&gt;\n</code></pre> <ol> <li>Check that replica joined cluster:</li> </ol> <pre><code>microk8s kubectl get nodes\n</code></pre> <p>New nodes should appear:</p> <pre><code>NAME                  STATUS   ROLES    AGE   VERSION\ni-05ecfbf799e480188   Ready    &lt;none&gt;   25h   v1.30.5\ni-0733cb329576e6c78   Ready    &lt;none&gt;   25h   v1.30.5\ni-0b27bcc06fc5c660e   Ready    &lt;none&gt;   25h   v1.30.5\n</code></pre>"},{"location":"microk8s/mk8s/k8s-microk8s-scaling/#scaling-sc4snmp","title":"Scaling SC4SNMP","text":"<ol> <li> <p>Install SC4SNMP if it is not installed yet.</p> </li> <li> <p>Add <code>worker</code> section in <code>values.yaml</code>:</p> </li> </ol> <pre><code>worker:\n  poller:\n    replicaCount: 4\n  trap:\n    replicaCount: 4\n  sender:\n    replicaCount: 4\n</code></pre> <ol> <li>Add <code>traps</code> replica count in <code>values.yaml</code>:</li> </ol> <pre><code>traps:\n  replicaCount: 4\n</code></pre> <ol> <li>Redeploy SC4SNMP:</li> </ol> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <ol> <li>Check that SC4SNMP has been scaled:</li> </ol> <pre><code>microk8s kubectl get pods -n sc4snmp\n</code></pre> <p>You should get 4 replicas for each worker and traps service:</p> <pre><code>NAME                                                          READY   STATUS      RESTARTS   AGE\nsnmp-mibserver-5df74fb678-zkj9m                               1/1     Running     0          25h\nsnmp-mongodb-6dc5c4f74d-xg6p7                                 2/2     Running     0          25h\nsnmp-redis-master-0                                           1/1     Running     0          25h\nsnmp-splunk-connect-for-snmp-inventory-k9t87                  0/1     Completed   0          3m\nsnmp-splunk-connect-for-snmp-scheduler-76848cf748-57qbx       1/1     Running     0          25h\nsnmp-splunk-connect-for-snmp-trap-9f55664c4-9dv7d             1/1     Running     0          3m1s\nsnmp-splunk-connect-for-snmp-trap-9f55664c4-crgld             1/1     Running     0          3m1s\nsnmp-splunk-connect-for-snmp-trap-9f55664c4-sb768             1/1     Running     0          25h\nsnmp-splunk-connect-for-snmp-trap-9f55664c4-tkhcp             1/1     Running     0          3m1s\nsnmp-splunk-connect-for-snmp-worker-poller-7487956697-4hvpl   1/1     Running     0          21h\nsnmp-splunk-connect-for-snmp-worker-poller-7487956697-8bvnn   1/1     Running     0          3m1s\nsnmp-splunk-connect-for-snmp-worker-poller-7487956697-9dfgt   1/1     Running     0          3m1s\nsnmp-splunk-connect-for-snmp-worker-poller-7487956697-hlhvz   1/1     Running     0          24h\nsnmp-splunk-connect-for-snmp-worker-sender-657589666f-979d2   1/1     Running     0          3m1s\nsnmp-splunk-connect-for-snmp-worker-sender-657589666f-mrvg9   1/1     Running     0          3m1s\nsnmp-splunk-connect-for-snmp-worker-sender-657589666f-qtcr8   1/1     Running     0          21h\nsnmp-splunk-connect-for-snmp-worker-sender-657589666f-tc8sv   1/1     Running     0          24h\nsnmp-splunk-connect-for-snmp-worker-trap-859dc47d9b-6fbs2     1/1     Running     0          24h\nsnmp-splunk-connect-for-snmp-worker-trap-859dc47d9b-kdcdb     1/1     Running     0          3m1s\nsnmp-splunk-connect-for-snmp-worker-trap-859dc47d9b-sfxvb     1/1     Running     0          3m\nsnmp-splunk-connect-for-snmp-worker-trap-859dc47d9b-xmmwv     1/1     Running     0          21h\n</code></pre>"},{"location":"microk8s/mk8s/k8s-microk8s-scaling/#autoscaling-sc4snmp","title":"Autoscaling SC4SNMP","text":"<ol> <li> <p>Install SC4SNMP if it is not installed yet.</p> </li> <li> <p>Add autoscaling options to <code>values.yaml</code>:</p> </li> </ol> <pre><code>worker:\n  poller:\n    autoscaling:\n      enabled: true\n      minReplicas: 5\n      maxReplicas: 10\n  trap:\n    autoscaling:\n      enabled: true\n      minReplicas: 5\n      maxReplicas: 10\n  sender:\n    autoscaling:\n      enabled: true\n      minReplicas: 5\n      maxReplicas: 10\n\ntraps:\n  autoscaling:\n    enabled: true\n    minReplicas: 5\n    maxReplicas: 10\n</code></pre> <ol> <li>Redeploy SC4SNMP:</li> </ol> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <ol> <li>Check that SC4SNMP scaled:</li> </ol> <pre><code>microk8s kubectl get po -n sc4snmp\n</code></pre> <p>After applying the changes, each worker and trap service will have from 5 to 10 instances:</p> <pre><code>NAME                                                          READY   STATUS      RESTARTS   AGE\nsnmp-mibserver-6fdcdf9ddd-7bvmj                               1/1     Running     0          25h\nsnmp-mongodb-6dc5c4f74d-6b7mf                                 2/2     Running     0          25h\nsnmp-redis-master-0                                           1/1     Running     0          25h\nsnmp-splunk-connect-for-snmp-inventory-sssgs                  0/1     Completed   0          3m37s\nsnmp-splunk-connect-for-snmp-scheduler-5fcb6dcb44-r79ff       1/1     Running     0          25h\nsnmp-splunk-connect-for-snmp-trap-5788bc498c-62xsq            1/1     Running     0          2m10s\nsnmp-splunk-connect-for-snmp-trap-5788bc498c-bmlhg            1/1     Running     0          2m10s\nsnmp-splunk-connect-for-snmp-trap-5788bc498c-p7mkq            1/1     Running     0          2m10s\nsnmp-splunk-connect-for-snmp-trap-5788bc498c-t8q9c            1/1     Running     0          2m10s\nsnmp-splunk-connect-for-snmp-trap-5788bc498c-xjjp2            1/1     Running     0          24h\nsnmp-splunk-connect-for-snmp-worker-poller-5d76b9b675-25tbf   1/1     Running     0          16m\nsnmp-splunk-connect-for-snmp-worker-poller-5d76b9b675-dc6zr   1/1     Running     0          16m\nsnmp-splunk-connect-for-snmp-worker-poller-5d76b9b675-g7vpr   1/1     Running     0          16m\nsnmp-splunk-connect-for-snmp-worker-poller-5d76b9b675-gdkgq   1/1     Running     0          16m\nsnmp-splunk-connect-for-snmp-worker-poller-5d76b9b675-pg6cj   1/1     Running     0          24h\nsnmp-splunk-connect-for-snmp-worker-sender-7757fb7f89-56h9w   1/1     Running     0          24h\nsnmp-splunk-connect-for-snmp-worker-sender-7757fb7f89-hr54w   1/1     Running     0          16m\nsnmp-splunk-connect-for-snmp-worker-sender-7757fb7f89-j7wcn   1/1     Running     0          16m\nsnmp-splunk-connect-for-snmp-worker-sender-7757fb7f89-sgsdg   0/1     Pending     0          16m\nsnmp-splunk-connect-for-snmp-worker-sender-7757fb7f89-xrpfx   1/1     Running     0          16m\nsnmp-splunk-connect-for-snmp-worker-trap-6b8fd89868-79x2l     0/1     Pending     0          16m\nsnmp-splunk-connect-for-snmp-worker-trap-6b8fd89868-br7pf     1/1     Running     0          24h\nsnmp-splunk-connect-for-snmp-worker-trap-6b8fd89868-cnmh9     0/1     Pending     0          16m\nsnmp-splunk-connect-for-snmp-worker-trap-6b8fd89868-dhdgg     1/1     Running     0          16m\nsnmp-splunk-connect-for-snmp-worker-trap-6b8fd89868-wcwq5     0/1     Pending     0          16m\n</code></pre>"},{"location":"microk8s/mk8s/k8s-microk8s/","title":"Platform Microk8s","text":""},{"location":"microk8s/mk8s/k8s-microk8s/#splunk-connect-for-snmp-using-microk8s","title":"Splunk Connect for SNMP using MicroK8s","text":"<p>See the following requirements to use any Linux deployment of Microk8s to support SC4SNMP.  The minimum requirements below are suitable for proof of value and small installations, actual requirements will differ.</p> <p>Single node minimum: </p> <ul> <li>4 cores</li> <li>8 GB of memory per node</li> <li>50 GB mounted as /</li> </ul> <p>Three node minimum per node:</p> <ul> <li>4 cores</li> <li>8 GB of memory per node</li> <li>50 GB mounted /</li> </ul>"},{"location":"microk8s/mk8s/k8s-microk8s/#microk8s-installation-on-ubuntu","title":"MicroK8s installation on Ubuntu","text":"<p>The following quick start guidance is based on Ubuntu 20.04LTS with MicroK8s and internet access. See other deployment options in the MicroK8s documentation, including offline and with proxy. </p>"},{"location":"microk8s/mk8s/k8s-microk8s/#enabling-ipv6","title":"Enabling IPv6","text":"<p>If you plan to poll or receive trap notifications from IPv6 addresses, firstly check the instruction for enabling  IPv6.</p>"},{"location":"microk8s/mk8s/k8s-microk8s/#install-microk8s-using-snap","title":"Install MicroK8s using Snap","text":"<pre><code>sudo snap install microk8s --classic --channel=1.30/stable\n</code></pre> <p>Add a user to the microk8s group so the <code>sudo</code> command is no longer necessary: </p><pre><code>sudo usermod -a -G microk8s $USER\nsudo chown -f -R $USER ~/.kube\nsu - $USER\n</code></pre> <p>Wait for Installation of microk8s to complete: </p><pre><code>microk8s status --wait-ready\n</code></pre>"},{"location":"microk8s/mk8s/k8s-microk8s/#install-required-services-for-sc4snmp","title":"Install required services for SC4SNMP","text":"<p>The following commands can be issued from any node in a cluster:</p> <pre><code>sudo systemctl enable iscsid\nmicrok8s enable helm3\nmicrok8s enable hostpath-storage\nmicrok8s enable rbac\nmicrok8s enable metrics-server\nmicrok8s status --wait-ready\n</code></pre> <p>Install the DNS server for microk8s and configure the forwarding DNS servers. Replace the IP addressed below (opendns) with the allowed values for your network: </p> <pre><code>microk8s enable dns:208.67.222.222,208.67.220.220\nmicrok8s status --wait-ready\n</code></pre>"},{"location":"microk8s/mk8s/k8s-microk8s/#install-metallb","title":"Install Metallb","text":"<p>When installing Metallb, you will be prompted for one or more IPs to use as entry points into the cluster. If you plan to enable clustering, this IP should not be assigned to the host (floats). If you do not plan to cluster, then this IP should be the IP of your host.</p> <p>Note: a single IP in cidr format is <code>x.x.x.x/32</code>. Use CIDR or range syntax for single server installations. This can be the same as the primary IP.</p> <pre><code>microk8s enable metallb\nmicrok8s status --wait-ready\n</code></pre>"},{"location":"microk8s/mk8s/k8s-microk8s/#add-nodes-optional","title":"Add nodes (optional)","text":"<p>If you need cluster mode use following guide.</p>"},{"location":"microk8s/offlineinstallation/offline-microk8s/","title":"Install Microk8s","text":""},{"location":"microk8s/offlineinstallation/offline-microk8s/#offline-microk8s-installation-issues","title":"Offline Microk8s installation issues","text":"<p>See install alternatives for offline installation of Microk8s. There are additional steps to install microk8s offline. See the following steps to install offline:</p>"},{"location":"microk8s/offlineinstallation/offline-microk8s/#importing-images","title":"Importing images","text":"<p>After running the following:</p> <pre><code>snap ack microk8s_{microk8s_version}.assert\nsnap install microk8s_{microk8s_version}.snap --classic\n</code></pre> <p>You should check if the microk8s instance is healthy. Do it using the following command:</p> <pre><code>microk8s kubectl get pods -A\n</code></pre> <p>The output will probably look like: </p><pre><code>NAMESPACE      NAME                                       READY   STATUS     RESTARTS   AGE\nkube-system    calico-kube-controllers-7c9c8dd885-fg8f2   0/1     Pending    0          14m\nkube-system    calico-node-zg4c4                          0/1     Init:0/3   0          23s\n</code></pre> <p>The pods are in the <code>Pending</code>/<code>Init</code> state because they are trying to download images, which is impossible to do offline.  In order to make them download, you need to download all the images on a different server with an internet connection,  pack it up, and import it to a microk8s image registry on your offline server.</p>"},{"location":"microk8s/offlineinstallation/offline-microk8s/#packing-up-images-for-an-offline-environment","title":"Packing up images for an offline environment","text":"<p>You need to monitor</p> <pre><code>microk8s kubectl get events -A\n</code></pre> <p>to see if <code>microk8s</code> fails to pull images, and then import anything it needs. An example of such information is:</p> <pre><code>kube-system    0s          Warning   Failed              pod/calico-node-sc784                           Failed to pull image \"docker.io/calico/cni:v3.21.4\": rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/calico/cni:v3.21.4\": failed to resolve reference \"docker.io/calico/cni:v3.21.4\": failed to do request: Head \"https://registry-1.docker.io/v2/calico/cni/manifests/v3.21.4\": dial tcp 54.83.42.45:443: i/o timeout\nkube-system    0s          Warning   Failed              pod/calico-node-sc784                           Error: ErrImagePull\n</code></pre> <p>The previous information shows that you lack a <code>docker.io/calico/cni:v3.21.4</code> image, and need to import it in order to fix the issue.</p> <p>The process to do this action is always the following:</p> <p></p><pre><code>docker pull &lt;needed_image&gt;\ndocker save &lt;needed_image&gt; &gt; image.tar\n</code></pre> Transfer package to the offline lab and execute: <pre><code>microk8s ctr image import image.tar\n</code></pre>"},{"location":"microk8s/offlineinstallation/offline-microk8s/#example-of-the-offline-installation","title":"Example of the offline installation","text":"<p>For example, <code>microk8s</code> version <code>3597</code> requires the following images to work correctly:</p> <pre><code>docker pull docker.io/calico/kube-controllers:v3.21.4 \ndocker pull docker.io/calico/node:v3.21.4\ndocker pull docker.io/calico/pod2daemon-flexvol:v3.21.4\ndocker pull docker.io/calico/cni:v3.21.4  \ndocker pull k8s.gcr.io/pause:3.1 \ndocker pull k8s.gcr.io/metrics-server/metrics-server:v0.5.2 \n</code></pre> <p>You should issue the above commands on your instance connected to the internet, then save it to <code>tar</code> packages:</p> <pre><code>docker save docker.io/calico/kube-controllers:v3.21.4 &gt; kube-controllers.tar\ndocker save docker.io/calico/node:v3.21.4 &gt; node.tar\ndocker save docker.io/calico/pod2daemon-flexvol:v3.21.4 &gt; pod2daemon-flexvol.tar\ndocker save docker.io/calico/cni:v3.21.4 &gt; cni.tar\ndocker save k8s.gcr.io/pause:3.1  &gt; pause.tar\ndocker save cdkbot/hostpath-provisioner:1.2.0 &gt; cdkbot.tar \ndocker save k8s.gcr.io/metrics-server/metrics-server:v0.5.2 &gt; metrics.tar\n</code></pre> <p>After that, <code>scp</code> those packages to your offline server and import it to its <code>microk8s</code> image registry:</p> <pre><code>microk8s ctr image import kube-controllers.tar\nmicrok8s ctr image import node.tar\nmicrok8s ctr image import pod2daemon-flexvol.tar\nmicrok8s ctr image import cni.tar\nmicrok8s ctr image import pause.tar\nmicrok8s ctr image import metrics.tar\n</code></pre> <p>Info</p> <p>For other versions of <code>microk8s</code>, tags of images may differ. </p> <p>After running the following:</p> <pre><code>microk8s enable hostpath-storage\nmicrok8s enable rbac\nmicrok8s enable metrics-server\n</code></pre> <p>The microk8s instance should be the following:</p> <pre><code>NAMESPACE      NAME                                       READY   STATUS                  RESTARTS   AGE\nkube-system    calico-kube-controllers-7c9c8dd885-wxms9   1/1     Running                 0          3h21m\nkube-system    calico-node-8cxsq                          1/1     Running                 0          3h21m\nkube-system    hostpath-provisioner-f57964d5f-zs4sj       1/1     Running                 0          5m41s\nkube-system    metrics-server-5f8f64cb86-x7k29            1/1     Running                 0          2m15s\n</code></pre>"},{"location":"microk8s/offlineinstallation/offline-microk8s/#enabling-dns-and-metallb","title":"Enabling DNS and Metallb","text":"<p><code>dns</code> and <code>metallb</code> do not require importing any images, so you can enable them simply through the following commands:</p> <pre><code>microk8s enable dns\nmicrok8s enable metallb\n</code></pre> <p>For more information on <code>metallb</code>, see Install metallb.</p>"},{"location":"microk8s/offlineinstallation/offline-microk8s/#installing-helm3","title":"Installing helm3","text":"<p>Additionally, you need to install the helm3 add-on.  See the following steps:</p> <ol> <li>Check your server\u2019s platform with:</li> </ol> <pre><code>dpkg --print-architecture\n</code></pre> <p>The output would be, for example: <code>amd64</code>. You need the platform to download the correct version of helm.</p> <ol> <li> <p>Download the helm package from <code>https://get.helm.sh/helm-v3.8.0-linux-{{arch}}.tar.gz</code>, where <code>{{arch}}</code> should be  replaced with the result from the previous command, for example: <code>https://get.helm.sh/helm-v3.8.0-linux-amd64.tar.gz</code>.</p> </li> <li> <p>Rename the package to <code>helm.tar.gz</code> and send it to an offline lab.</p> </li> <li>Create <code>tmp</code> directory in <code>/var/snap/microk8s/current</code> and copy the package in the following locations: </li> </ol> <pre><code>sudo mkdir -p /var/snap/microk8s/current/tmp/helm3\nsudo cp helm.tar.gz /var/snap/microk8s/current/tmp/helm3\n</code></pre> <ol> <li>Go to the directory containing <code>enable</code> script for <code>helm3</code>:</li> </ol> <pre><code>cd /var/snap/microk8s/common/addons/core/addons/helm3\n</code></pre> <p>Open <code>enable</code> file with vi, nano, or some other editor. Comment this line:</p> <pre><code>#fetch_as $SOURCE_URI/helm-$HELM_VERSION-linux-${SNAP_ARCH}.tar.gz \"$SNAP_DATA/tmp/helm3/helm.tar.gz\"\n</code></pre> <p>Save file.</p> <ol> <li>Run <code>microk8s enable helm3</code></li> </ol>"},{"location":"microk8s/offlineinstallation/offline-microk8s/#verify-your-instance","title":"Verify your instance","text":"<p>Check if all the add-ons were installed successfully using the following command: <code>microk8s status --wait-ready</code>. An example of a correct output is:</p> <pre><code>microk8s is running\nhigh-availability: no\n  datastore master nodes: 127.0.0.1:19001\n  datastore standby nodes: none\naddons:\n  enabled:\n    dns                  # (core) CoreDNS\n    ha-cluster           # (core) Configure high availability on the current node\n    helm3                # (core) Helm 3 - Kubernetes package manager\n    hostpath-storage     # (core) Storage class; allocates storage from host directory\n    metallb              # (core) Loadbalancer for your Kubernetes cluster\n    metrics-server       # (core) K8s Metrics Server for API access to service metrics\n    rbac                 # (core) Role-Based Access Control for authorisation\n    storage              # (core) Alias to hostpath-storage add-on, deprecated\n  disabled:\n    community            # (core) The community addons repository\n    dashboard            # (core) The Kubernetes dashboard\n    gpu                  # (core) Automatic enablement of Nvidia CUDA\n    helm                 # (core) Helm 2 - the package manager for Kubernetes\n    host-access          # (core) Allow Pods connecting to Host services smoothly\n    ingress              # (core) Ingress controller for external access\n    mayastor             # (core) OpenEBS MayaStor\n    prometheus           # (core) Prometheus operator for monitoring and logging\n    registry             # (core) Private image registry exposed on localhost:32000\n</code></pre>"},{"location":"microk8s/offlineinstallation/offline-sc4snmp/","title":"Install SC4SNMP","text":""},{"location":"microk8s/offlineinstallation/offline-sc4snmp/#offline-sc4snmp-installation","title":"Offline SC4SNMP installation","text":"<p>See the following options for an offline SC4SNMP installation. </p>"},{"location":"microk8s/offlineinstallation/offline-sc4snmp/#local-machine-with-internet-access","title":"Local machine with internet access","text":"<p>To install the SC4SNMP offline, first, download some packages from the Github release and then move them to the SC4SNMP installation server. Those packages are:</p> <ul> <li><code>dependencies-images.tar</code></li> <li><code>splunk-connect-for-snmp-chart.tar</code></li> </ul> <p>Additionally, you will need:</p> <ul> <li><code>pull_mibserver.sh</code> script</li> <li><code>pull_gui_images.sh</code> script</li> </ul> <p>to easily pull and export mibserver image and GUI images.</p> <p>Moreover, the SC4SNMP docker image must be pulled, saved as a <code>.tar</code> package, and then moved to the server as well.  This process requires Docker to be installed locally.</p> <p>Images can be pulled from the following repository: <code>ghcr.io/splunk/splunk-connect-for-snmp/container:&lt;tag&gt;</code>.  The latest tag can be found in The Splunk Connect for SNMP Repository, under the Releases section with the label <code>latest</code>.</p> <p>See the following example of docker pull command:</p> <pre><code>docker pull ghcr.io/splunk/splunk-connect-for-snmp/container:&lt;tag&gt;\n</code></pre> <p>Afterwards, save the image. The directory where this image will be saved can be specified after the <code>&gt;</code> sign:</p> <pre><code>docker save ghcr.io/splunk/splunk-connect-for-snmp/container:&lt;tag&gt; &gt; snmp_image.tar\n</code></pre> <p>Other packages you have to pull are mibserver and GUI images. Do this by executing <code>pull_mibserver.sh</code> and  <code>pull_gui_images.sh</code> scripts from the Release section, or copy-pasting its content. See the following:</p> <pre><code>chmod a+x pull_mibserver.sh # you'll probably need to make file executable\n./pull_mibserver.sh\nchmod a+x pull_gui_images.sh\n./pull_gui_images.sh\n</code></pre> <p>Those scripts should produce <code>mibserver.tar</code> with the image of the mibserver and <code>sc4snmp-gui-images.tar</code> with GUI images inside.</p> <p>All five packages, <code>mibserver.tar</code>, <code>snmp_image.tar</code>, <code>dependencies-images.tar</code>, <code>sc4snmp-gui-images.tar</code> and <code>splunk-connect-for-snmp-chart.tar</code>, must be moved to the SC4SNMP installation server.</p>"},{"location":"microk8s/offlineinstallation/offline-sc4snmp/#installation-on-the-server","title":"Installation on the server","text":"<p>On the server, all the images must be imported to the microk8s cluster. This can be done with the following command:</p> <pre><code>microk8s ctr image import &lt;name_of_tar_image&gt;\n</code></pre> <p>Run the following commands:</p> <pre><code>microk8s ctr image import dependencies-images.tar\nmicrok8s ctr image import snmp_image.tar\nmicrok8s ctr image import mibserver.tar\n</code></pre> <p>Afterwards, create <code>values.yaml</code>. It is a little different from <code>values.yaml</code> used in an online installation.  The difference between the two files is the following, which is used for automatic image pulling:</p> <pre><code>image:\n  pullPolicy: \"Never\"\n</code></pre> <p>An example <code>values.yaml</code> file can be found in the Offline SC4SNMP values.yaml template.</p> <p>Next, unpack the chart package <code>splunk-connect-for-snmp-chart.tar</code>. It will result in creating the following <code>splunk-connect-for-snmp</code> directory:</p> <pre><code>tar -xvf splunk-connect-for-snmp-chart.tar --exclude='._*'\n</code></pre> <p>Finally, run the helm install command in the directory where both the <code>values.yaml</code> and <code>splunk-connect-for-snmp</code> directories are located:</p> <pre><code>microk8s helm3 install snmp -f values.yaml splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre>"},{"location":"microk8s/offlineinstallation/offline-sck/","title":"Install Splunk OpenTelemetry Collector for Kubernetes","text":""},{"location":"microk8s/offlineinstallation/offline-sck/#splunk-opentelemetry-collector-for-kubernetes-offline-installation","title":"Splunk OpenTelemetry Collector for Kubernetes offline installation","text":"<p>See the following options to install the Splunk OpenTelemetry Collector for Kubernetes. </p>"},{"location":"microk8s/offlineinstallation/offline-sck/#local-machine-with-internet-access","title":"Local machine with internet access","text":"<p>To install Splunk OpenTelemetry Collector offline, first, download the packed chart <code>splunk-otel-collector-&lt;tag&gt;.tgz</code> and the otel image <code>otel_image.tar</code> from the Github release, where <code>&lt;tag&gt;</code> is the current OpenTelemetry release tag. Both packages must be later moved to the installation server.</p>"},{"location":"microk8s/offlineinstallation/offline-sck/#installation-on-the-server","title":"Installation on the server","text":"<p>The Otel image has to be imported to the <code>microk8s</code> registry with:</p> <pre><code>microk8s ctr image import otel_image.tar \n</code></pre> <p>The imported package must be unpacked with the following command :</p> <pre><code>tar -xvf splunk-otel-collector-&lt;tag&gt;.tgz --exclude='._*'\n</code></pre> <p>In order to run Splunk OpenTelemetry Collector on your environment, replace <code>&lt;&gt;</code> variables according to the following description: </p><pre><code>microk8s helm3 install sck \\\n  --set=\"clusterName=&lt;cluster_name&gt;\" \\\n  --set=\"splunkPlatform.endpoint=&lt;splunk_endpoint&gt;\" \\\n  --set=\"splunkPlatform.insecureSkipVerify=&lt;insecure_skip_verify&gt;\" \\\n  --set=\"splunkPlatform.token=&lt;splunk_token&gt;\" \\\n  --set=\"logsEngine=otel\" \\\n  --set=\"splunkPlatform.metricsEnabled=true\" \\\n  --set=\"splunkPlatform.metricsIndex=em_metrics\" \\\n  --set=\"splunkPlatform.index=em_logs\" \\\n  splunk-otel-collector\n</code></pre>"},{"location":"microk8s/offlineinstallation/offline-sck/#variables-description","title":"Variables description","text":"Placeholder Description Example splunk_endpoint host address of splunk instance <code>https://endpoint.example.com:8088/services/collector</code> insecure_skip_verify is insecure ssl allowed <code>false</code> splunk_token Splunk HTTP Event Collector token <code>450a69af-16a9-4f87-9628-c26f04ad3785</code> cluster_name name of the cluster <code>my-cluster</code> <p>An example of a correctly filled command is: </p><pre><code>microk8s helm3 install sck \\\n  --set=\"clusterName=my-cluster\" \\\n  --set=\"splunkPlatform.endpoint=https://endpoint.example.com/services/collector\" \\\n  --set=\"splunkPlatform.insecureSkipVerify=false\" \\\n  --set=\"splunkPlatform.token=4d22911c-18d9-4706-ae7b-dd1b976ca6f7\" \\\n  --set=\"splunkPlatform.metricsEnabled=true\" \\\n  --set=\"splunkPlatform.metricsIndex=em_metrics\" \\\n  --set=\"splunkPlatform.index=em_logs\" \\\n  splunk-otel-collector\n</code></pre>"},{"location":"microk8s/offlineinstallation/offline-sck/#install-splunk-opentelemetry-collector-with-helm-for-splunk-observability-for-kubernetes","title":"Install Splunk OpenTelemetry Collector with HELM for Splunk Observability for Kubernetes","text":"<p>To run Splunk OpenTelemetry Collector on your environment, replace <code>&lt;&gt;</code> variables according to the following description:</p> <pre><code>microk8s helm3 install sck\n--set=\"clusterName=&lt;cluster_name&gt;\"\n--set=\"splunkObservability.realm=&lt;realm&gt;\"\n--set=\"splunkObservability.accessToken=&lt;token&gt;\"\n--set=\"splunkObservability.ingestUrl=&lt;ingest_url&gt;\"\n--set=\"splunkObservability.apiUrl=&lt;api_url&gt;\"\n--set=\"splunkObservability.metricsEnabled=true\"\n--set=\"splunkObservability.tracesEnabled=false\"\n--set=\"splunkObservability.logsEnabled=false\"\nsplunk-otel-collector\n</code></pre>"},{"location":"microk8s/offlineinstallation/offline-sck/#variables-description_1","title":"Variables description","text":"Placeholder Description Example cluster_name name of the cluster <code>my_cluster</code> realm Realm obtained from the Splunk Observability Cloud environment <code>us0</code> token Token obtained from the Splunk Observability Cloud environment <code>BCwaJ_Ands4Xh7Nrg</code> ingest_url Ingest URL from the Splunk Observability Cloud environment <code>https://ingest..signalfx.com</code> api_url API URL from the Splunk Observability Cloud environment <code>https://api..signalfx.com</code> <p>An example of a correctly filled command is: </p><pre><code>microk8s helm3 install sck \n--set=\"clusterName=my_cluster\"\n--set=\"splunkObservability.realm=us0\"\n--set=\"splunkObservability.accessToken=BCwaJ_Ands4Xh7Nrg\"\n--set=\"splunkObservability.ingestUrl=https://ingest..signalfx.com\"\n--set=\"splunkObservability.apiUrl=https://api..signalfx.com\"\n--set=\"splunkObservability.metricsEnabled=true\"\n--set=\"splunkObservability.tracesEnabled=false\"\n--set=\"splunkObservability.logsEnabled=false\"\nsplunk-otel-collector\n</code></pre>"},{"location":"troubleshooting/configuring-logs/","title":"Accessing and configuring logs","text":""},{"location":"troubleshooting/configuring-logs/#configuring-sc4snmp-loglevel","title":"Configuring SC4SNMP loglevel","text":"<p>SC4SNMP log level can be configured in <code>values.yaml</code> file. The default value for it is <code>INFO</code>, other  possible levels to set are <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code> or <code>FATAL</code>. To change  the log level for a specific component, add the following configuration to <code>values.yaml</code>:</p> <pre><code>worker:\n    logLevel: \"DEBUG\"\n</code></pre> <p>And redeploy SC4SNMP. </p> <p>Log level configuration can be set for <code>worker</code>, <code>poller</code>, <code>scheduler</code> and <code>traps</code>.</p>"},{"location":"troubleshooting/configuring-logs/#accessing-sc4snmp-logs","title":"Accessing SC4SNMP logs","text":"<p>SC4SNMP logs can be browsed in Splunk in <code>em_logs</code> index, provided that sck-otel is installed. Logs can be also accessed directly in kubernetes using terminal.</p>"},{"location":"troubleshooting/configuring-logs/#accessing-logs-via-splunk","title":"Accessing logs via Splunk","text":"<p>If sck-otel is installed, browse <code>em_logs</code> index. Logs can be further filtered  for example by the sourcetype field. Example search command to get logs from poller: </p><pre><code>index=em_logs sourcetype=\"kube:container:splunk-connect-for-snmp-worker-poller\"\n</code></pre>"},{"location":"troubleshooting/configuring-logs/#accessing-logs-in-kubernetes","title":"Accessing logs in kubernetes","text":"<p>To access logs directly in kubernetes, first run <code>microk8s kubectl -n sc4snmp get pods</code>. This will output all pods: </p><pre><code>NAME                                                          READY   STATUS    RESTARTS   AGE\nsnmp-splunk-connect-for-snmp-worker-trap-99f49c557-j9jwx      1/1     Running   0          29m\nsnmp-splunk-connect-for-snmp-trap-56f75f9754-kmlgb            1/1     Running   0          29m\nsnmp-splunk-connect-for-snmp-scheduler-7bb8c79855-rgjkj       1/1     Running   0          29m\nsnmp-mibserver-784bd599fd-6xzfj                               1/1     Running   0          29m\nsnmp-splunk-connect-for-snmp-worker-poller-78b46d668f-59mv4   1/1     Running   0          29m\nsnmp-splunk-connect-for-snmp-worker-sender-6f8496bfbf-cvt9l   1/1     Running   0          29m\nsnmp-mongodb-7579dc7867-mlnst                                 2/2     Running   0          29m\nsnmp-redis-master-0                                           1/1     Running   0          29m\n</code></pre> <p>Now select the desired pod and run <code>microk8s kubectl -n sc4snmp logs pod/&lt;pod-name&gt;</code> command. Example command to retrieve logs from <code>splunk-connect-for-snmp-worker-poller</code>: </p><pre><code>microk8s kubectl -n sc4snmp logs pod/snmp-splunk-connect-for-snmp-worker-poller-78b46d668f-59mv4\n</code></pre>"},{"location":"troubleshooting/configuring-logs/#accessing-logs-in-docker","title":"Accessing logs in docker","text":"<p>Refer to splunk logging for instructions on how to enable logging in docker and  sent them to Splunk.</p> <p>To access logs directly in docker, first run <code>docker ps</code>. This will output all containers:</p> <pre><code>CONTAINER ID   IMAGE                                                            COMMAND                  CREATED          STATUS          PORTS                                                                                  NAMES\nafcd8f4850cd   ghcr.io/splunk/splunk-connect-for-snmp/container:1.12.0-beta.1   \"./entrypoint.sh cel\u2026\"   19 seconds ago   Up 17 seconds                                                                                          docker_compose-worker-poller-1\n5cea46cee0cb   ghcr.io/splunk/splunk-connect-for-snmp/container:1.12.0-beta.1   \"./entrypoint.sh cel\u2026\"   19 seconds ago   Up 17 seconds                                                                                          docker_compose-worker-sender-1\n1c5154c91191   ghcr.io/splunk/splunk-connect-for-snmp/container:1.12.0-beta.1   \"./entrypoint.sh cel\u2026\"   19 seconds ago   Up 17 seconds                                                                                          sc4snmp-scheduler\n8f6e60903780   ghcr.io/splunk/splunk-connect-for-snmp/container:1.12.0-beta.1   \"./entrypoint.sh trap\"   19 seconds ago   Up 17 seconds   0.0.0.0:2163-&gt;2163/udp, :::2163-&gt;2163/udp, 0.0.0.0:162-&gt;2162/udp, [::]:162-&gt;2162/udp   sc4snmp-traps\nf146802a0a8d   ghcr.io/splunk/splunk-connect-for-snmp/container:1.12.0-beta.1   \"./entrypoint.sh cel\u2026\"   19 seconds ago   Up 16 seconds                                                                                          docker_compose-worker-poller-2\n70e0fe076cdf   ghcr.io/splunk/splunk-connect-for-snmp/container:1.12.0-beta.1   \"./entrypoint.sh cel\u2026\"   19 seconds ago   Up 17 seconds                                                                                          docker_compose-worker-trap-2\n090cc957b600   ghcr.io/splunk/splunk-connect-for-snmp/container:1.12.0-beta.1   \"./entrypoint.sh cel\u2026\"   19 seconds ago   Up 16 seconds                                                                                          docker_compose-worker-trap-1\n24aac5c89d80   ghcr.io/pysnmp/mibs/container:latest                             \"/bin/sh -c '/app/lo\u2026\"   19 seconds ago   Up 18 seconds   8080/tcp                                                                               snmp-mibserver\na5bef5a5a02c   bitnami/mongodb:6.0.9-debian-11-r5                               \"/opt/bitnami/script\u2026\"   19 seconds ago   Up 18 seconds   27017/tcp                                                                              mongo\n76f966236c1b   bitnami/redis:7.2.1-debian-11-r0                                 \"/opt/bitnami/script\u2026\"   19 seconds ago   Up 18 seconds   6379/tcp                                                                               redis\n163d880eaf8c   coredns/coredns:1.11.1                                           \"/coredns -conf /Cor\u2026\"   19 seconds ago   Up 18 seconds   53/tcp, 53/udp                                                                         coredns\n</code></pre> <p>Now select the desired container and run <code>docker logs &lt;container_name/id&gt;</code> command.  Example command to retrieve logs from <code>splunk-connect-for-snmp-worker-poller</code>:</p> <pre><code>docker logs docker_compose-worker-poller-1\n</code></pre>"},{"location":"troubleshooting/docker-commands/","title":"Docker commands","text":""},{"location":"troubleshooting/docker-commands/#docker-commands","title":"Docker commands","text":"<p>For full display of docker commands and their usage can be found at docker documentation.  Below are the most common commands used to troubleshoot issues with SC4SNMP. </p>"},{"location":"troubleshooting/docker-commands/#common-flags","title":"Common flags","text":"<p>The following are some common flags that can be used with the <code>docker</code> commands:</p> <ul> <li><code>-a</code> flag is used to list all resources</li> </ul> <p>For more flags and options, you can refer to the docker documentation.</p>"},{"location":"troubleshooting/docker-commands/#accessing-logs-in-docker","title":"Accessing logs in docker","text":"<p>The instruction on how to set up and access the logs can be found in SC4SNMP logs </p>"},{"location":"troubleshooting/docker-commands/#the-ls-and-ps-commands","title":"The ls and ps commands","text":"<p>The <code>ls</code> or <code>ps</code> command are used to list the resources in docker. The following are the example of resources that  can be listed using the commands:</p> <pre><code>docker compose ls\ndocker network ls\ndocker image ls\ndocker container ls\ndocker ps\ndocker ps -a\ndocker compose ps &lt;service_name/id&gt;\n</code></pre>"},{"location":"troubleshooting/docker-commands/#the-inspect-command","title":"The inspect command","text":"<p>The <code>inspect</code> command is used to get detailed information about the resources in docker. The following are the  example of resources that can be inspected:</p> <pre><code>docker inspect --type &lt;resource_type&gt; &lt;resource_name/id&gt;\ndocker network inspect &lt;resource_name/id&gt;\ndocker image inspect &lt;resource_name/id&gt;\n</code></pre>"},{"location":"troubleshooting/docker-commands/#the-logs-command","title":"The logs command","text":"<p>The <code>logs</code> command is used to get the logs of the resources in docker.  The following are some examples of how to use the <code>logs</code> command:</p> <pre><code>docker logs &lt;container_name/id&gt;\ndocker compose logs &lt;service_name/id&gt;\n</code></pre>"},{"location":"troubleshooting/docker-commands/#the-exec-command","title":"The exec command","text":"<p>The <code>exec</code> command is used to execute a command in a running container. The following is an example of how to  use the <code>exec</code> command:</p> <pre><code>docker exec -it &lt;container_name/id&gt; sh -c &lt;command&gt;\n</code></pre>"},{"location":"troubleshooting/docker-commands/#the-stats-command","title":"The stats command","text":"<p>The <code>stats</code> command is used to display the live resource usage statistics of a container. The following are some examples of how to use the <code>stats</code> command:</p> <pre><code>docker stats\ndocker stats &lt;container_name/id&gt;\n</code></pre>"},{"location":"troubleshooting/docker-commands/#examples-of-command-usage","title":"Examples of command usage","text":""},{"location":"troubleshooting/docker-commands/#check-secret-for-snmp-v3","title":"Check secret for snmp v3","text":"<p>One of the issues related to snmp v3 can be incorrectly configured secrets in docker.  Below you can find the instruction to check the existing secrets.</p> <p>To check the existing secrets: </p><pre><code>~$ docker exec -it docker_compose-worker-poller-1 sh -c \"ls secrets/snmpv3\"\nmy_secret\n</code></pre> To get more details about one secret you can use command: <pre><code>~$ docker exec -it docker_compose-worker-poller-1 sh -c \"ls secrets/snmpv3/my_secret\"\nauthKey  authProtocol  contextEngineId  privKey  privProtocol  userName\n</code></pre> Replace my_secret with the name of the secret you want to check and docker_compose-worker-poller-1 with the name of the container. <p>To see the configured details of the secret:  </p><pre><code>~$ docker exec -it docker_compose-worker-poller-1 sh -c 'cd secrets/snmpv3/my_secret &amp;&amp; for file in *; do echo \"$file= $(cat $file)\"; done'\nauthKey= admin1234\nauthProtocol= SHA\ncontextEngineId= 80003a8c04\nprivKey= admin1234\nprivProtocol= AES\nuserName= r-wuser\n</code></pre> Replace my_secret with the name of the secret you want to check and docker_compose-worker-poller-1 with the name of the container."},{"location":"troubleshooting/docker-commands/#check-containers-health","title":"Check containers health","text":"<p>To check the health of the containers, you can use the <code>ps</code> command to look at the <code>STATUS</code>.  If the <code>STATUS</code> is not <code>Up</code> or the containers restarts continuously, then there might be an issue with it.  You can also use the <code>inspect</code> command to get more detailed information about the container and see if there are any  errors or warnings in the <code>state</code> or use <code>logs</code> command to see the logs of the container.</p>"},{"location":"troubleshooting/docker-commands/#check-resource-usage","title":"Check resource usage","text":"<p>To check the resource usage of the containers, you can use the <code>stats</code> command.  With this command, you can see the CPU and memory usage of the containers in real-time and compare it with the ones  assigned in <code>resources</code> section in the configuration yaml. If they are close to each other you might consider increasing the resources assigned.</p>"},{"location":"troubleshooting/docker-commands/#check-network","title":"Check network","text":"<p>Checking the network configuration can be useful when enabling the dual-stack for SC4SNMP.</p> <p>One of useful commands to check the network configuration is: </p><pre><code>~$ docker network ls\nNETWORK ID     NAME              DRIVER    SCOPE\n7e46b3818089   bridge            bridge    local\n1401c370b8f4   docker_gwbridge   bridge    local\n12ca971fa954   host              host      local\nrssypcqbwarx   ingress           overlay   swarm\nb6c176852f41   none              null      local\n978e06ffcd4a   sc4snmp_network   bridge    local\n</code></pre> This command is showing all the network configured in the docker. The network created for sc4snmp by default is named <code>sc4snmp_network</code>. <p>To see details of configured network use: </p><pre><code>~$ docker network inspect sc4snmp_network\n[\n    {\n        \"Name\": \"sc4snmp_network\",\n        \"Id\": \"978e06ffcd4a49de5cd78a038050530342a029b1b1a1f1967254f701ae5ff1a0\",\n        \"Created\": \"2024-10-10T11:38:01.627727666Z\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.28.0.0/16\",\n                    \"Gateway\": \"172.28.0.1\"\n                },\n                {\n                    \"Subnet\": \"fd02::/64\",\n                    \"Gateway\": \"fd02::1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {\n            \"231b21c24bd722d684349174cc5aebf40cf294617aa98741a4af1269ed930fcc\": {\n                \"Name\": \"docker_compose-worker-poller-1\",\n                \"EndpointID\": \"0195750f0539535615ebdb24d8ee7eb967d31ca3c86a0d5d4b5c21f907cb61a0\",\n                \"MacAddress\": \"02:42:ac:1c:00:0b\",\n                \"IPv4Address\": \"172.28.0.11/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"25479e15afee663a7d0ad7b97f734f65d35672c49e9610f9e0406975d616e584\": {\n                \"Name\": \"snmp-mibserver\",\n                \"EndpointID\": \"68a27a27fc5acc7b1350cb5f073abf9218f1c0fa4ede5f037a67fdcce46ec91b\",\n                \"MacAddress\": \"02:42:ac:1c:00:03\",\n                \"IPv4Address\": \"172.28.0.3/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"35f2bdd191898f7186a0c00dbffa5cc700e9d72e07efb6f3b341c6b8ce14d5f5\": {\n                \"Name\": \"coredns\",\n                \"EndpointID\": \"0c76c32e9b9b1dd033141332dee9a8f954c4a83ea5344ee4c93af057d2523d9a\",\n                \"MacAddress\": \"02:42:ac:1c:00:ff\",\n                \"IPv4Address\": \"172.28.0.255/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"3dc9f0d293578a7aca1b6b33cc3557f82262849e2be488a9cda729152854b9a9\": {\n                \"Name\": \"docker_compose-worker-trap-2\",\n                \"EndpointID\": \"88fc3701b04803d6317ad5d23031f880ec96c2206185c1994184580932ed5865\",\n                \"MacAddress\": \"02:42:ac:1c:00:0c\",\n                \"IPv4Address\": \"172.28.0.12/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"43c5893f2688da599dd0331a328937b19a62496f4eb06eaa40a9cad8e879c567\": {\n                \"Name\": \"redis\",\n                \"EndpointID\": \"c1c91866f67ed76d83e78a6b11e5001b0cf65107df3b7d4733373653be7f5e6a\",\n                \"MacAddress\": \"02:42:ac:1c:00:04\",\n                \"IPv4Address\": \"172.28.0.4/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"52fa13245149422e559d4ff7a2f6c929b46ebfffdbafb52efcaade26e861128e\": {\n                \"Name\": \"sc4snmp-traps\",\n                \"EndpointID\": \"926187b2e4c3e9753dd260e8fa9db2745c20ed6c87f73f2df4870f0cb3be1511\",\n                \"MacAddress\": \"02:42:ac:1c:00:05\",\n                \"IPv4Address\": \"172.28.0.5/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"68813263e9d6a74e70061f85f9044ec334cce9aee364804566b4823e6960ae04\": {\n                \"Name\": \"docker_compose-worker-poller-2\",\n                \"EndpointID\": \"06d883d0ee21926be450b8c0adf4c31da7f13ceaa70dba3d0830608d5c192b2d\",\n                \"MacAddress\": \"02:42:ac:1c:00:08\",\n                \"IPv4Address\": \"172.28.0.8/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"78b04a7cd5c9ec1d3aaf014fd10c0ad89d401ad63093052a26111066198639af\": {\n                \"Name\": \"docker_compose-worker-sender-1\",\n                \"EndpointID\": \"0e9c84d4e7d1ce6362bba33c41161086a2de4623161a0ef34ce746d9983a4be7\",\n                \"MacAddress\": \"02:42:ac:1c:00:09\",\n                \"IPv4Address\": \"172.28.0.9/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"a34c808997eb56ab5c4043be3d9cd5ceb86f5b0f481b7bd51009eace9ff12965\": {\n                \"Name\": \"mongo\",\n                \"EndpointID\": \"992f5fd3eed5e646c250d61cc1d3c94bf43dc2ad0621f0044dbfd718d24325d5\",\n                \"MacAddress\": \"02:42:ac:1c:00:02\",\n                \"IPv4Address\": \"172.28.0.2/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"b197d6b5ac9a0a69d8afb9a613006e916eacffd4c3a2c71e3ee8db927c307457\": {\n                \"Name\": \"sc4snmp-scheduler\",\n                \"EndpointID\": \"3753aec5d05a24683fb04f29284297444957e466fd5d5ffc6f40f8b58d04c443\",\n                \"MacAddress\": \"02:42:ac:1c:00:07\",\n                \"IPv4Address\": \"172.28.0.7/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"b52716b229679ec14fcc3236eee4e64f6f2b2c257889979ebb7d4b091c8cd0db\": {\n                \"Name\": \"docker_compose-worker-trap-1\",\n                \"EndpointID\": \"f1066da76315c595b6bd606e2f0437b16ec33b2c16e3f659682910e6a79ecb24\",\n                \"MacAddress\": \"02:42:ac:1c:00:0a\",\n                \"IPv4Address\": \"172.28.0.10/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {},\n        \"Labels\": {\n            \"com.docker.compose.network\": \"sc4snmp_network\",\n            \"com.docker.compose.project\": \"docker_compose\",\n            \"com.docker.compose.version\": \"2.29.7\"\n        }\n    }\n]\n</code></pre> <p>One section of the command is showing the <code>containers</code> assigned to that network with their ipv4 and ipv6 addresses. The commands also shows if the ipv6 is enabled and what subnets are assigned to the network.</p>"},{"location":"troubleshooting/k8s-commands/","title":"Kubernetes commands","text":""},{"location":"troubleshooting/k8s-commands/#kubectl-commands","title":"Kubectl commands","text":"<p>For full display of kubernetes commands and their usage can be found at kubectl documentation.  Below are the most common commands used to troubleshoot issues with SC4SNMP. </p>"},{"location":"troubleshooting/k8s-commands/#common-flags","title":"Common flags","text":"<p>The following are some common flags that can be used with the <code>kubectl</code> commands:</p> <ul> <li><code>-A</code> flag is used to list all resources in all namespaces</li> <li><code>-n</code> flag is used to specify the namespace of the resource</li> <li><code>-f</code> flag is used to specify the file that contains the resource definition</li> <li><code>-o</code> flag is used to specify the output format of the command</li> </ul> <p>For more flags and options, you can refer to the kubectl documentation.</p>"},{"location":"troubleshooting/k8s-commands/#accessing-logs-in-kubernetes","title":"Accessing logs in kubernetes","text":"<p>The instruction on how to set up and access the logs can be found in SC4SNMP logs </p>"},{"location":"troubleshooting/k8s-commands/#the-get-command","title":"The get command","text":"<p>The <code>get</code> command is used to list one or more resources of selected type. The following are some examples of how to use the <code>get</code> command: </p><pre><code>microk8s kubectl get all\nmicrok8s kubectl get pods \nmicrok8s kubectl get svc\nmicrok8s kubectl get deployments\nmicrok8s kubectl get events\nmicrok8s kubectl get nodes\nmicrok8s kubectl get configmaps\nmicrok8s kubectl get secrets\nmicrok8s kubectl get ippools\n</code></pre> <p>For example to list all pods running in sc4snmp namespace you can use command: </p><pre><code>~$ microk8s kubectl get pods -n sc4snmp\nNAME                                                          READY   STATUS    RESTARTS        AGE\nsnmp-mibserver-95df967b9-cjhvz                                1/1     Running   1 (5h13m ago)   27h\nsnmp-mongodb-6dc5c4f74d-pxpxb                                 2/2     Running   2 (5h13m ago)   27h\nsnmp-redis-master-0                                           1/1     Running   1 (5h13m ago)   25h\nsnmp-splunk-connect-for-snmp-scheduler-7c675d7dd7-6ql2g       1/1     Running   2 (5h13m ago)   27h\nsnmp-splunk-connect-for-snmp-trap-755b58b8c5-kg5f4            1/1     Running   1 (5h13m ago)   27h\nsnmp-splunk-connect-for-snmp-trap-755b58b8c5-r8szq            1/1     Running   1 (5h13m ago)   27h\nsnmp-splunk-connect-for-snmp-worker-poller-5956f6dfb4-rs7mv   1/1     Running   1 (5h13m ago)   27h\nsnmp-splunk-connect-for-snmp-worker-poller-5956f6dfb4-wjxb6   1/1     Running   1 (5h13m ago)   27h\nsnmp-splunk-connect-for-snmp-worker-sender-76f5d49478-spvp2   1/1     Running   1 (5h13m ago)   27h\nsnmp-splunk-connect-for-snmp-worker-trap-5c4dbf8889-4njg2     1/1     Running   1 (5h13m ago)   27h\nsnmp-splunk-connect-for-snmp-worker-trap-5c4dbf8889-5hc6j     1/1     Running   1 (5h13m ago)   27h\n</code></pre>"},{"location":"troubleshooting/k8s-commands/#the-describe-command","title":"The describe command","text":"<p>The <code>describe</code> command is used to get detailed information about a resource. The following are some examples of how to use the <code>describe</code> command: </p><pre><code>microk8s kubectl describe all \nmicrok8s kubectl describe pod &lt;pod-name&gt;\nmicrok8s kubectl describe svc &lt;service-name&gt;\nmicrok8s kubectl describe deployment &lt;deployment-name&gt;\nmicrok8s kubectl describe events\nmicrok8s kubectl describe node &lt;node-name&gt;\nmicrok8s kubectl describe configmap &lt;configmap-name&gt;\nmicrok8s kubectl describe secret &lt;secret&gt;\nmicrok8s kubectl describe ippool &lt;ippool-name&gt;\n</code></pre> <p>For example to get detailed information about a service you can use command: </p><pre><code>~$ microk8s kubectl describe svc/snmp-splunk-connect-for-snmp-trap -n sc4snmp\nName:                     snmp-splunk-connect-for-snmp-trap\nNamespace:                sc4snmp\nLabels:                   app.kubernetes.io/instance=snmp\n                          app.kubernetes.io/managed-by=Helm\n                          app.kubernetes.io/name=splunk-connect-for-snmp-trap\n                          app.kubernetes.io/version=1.11.0\n                          helm.sh/chart=splunk-connect-for-snmp-1.11.0\nAnnotations:              meta.helm.sh/release-name: snmp\n                          meta.helm.sh/release-namespace: sc4snmp\n                          metallb.universe.tf/allow-shared-ip: splunk-connect\nSelector:                 app.kubernetes.io/instance=snmp,app.kubernetes.io/name=splunk-connect-for-snmp-trap\nType:                     LoadBalancer\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.153.183.151\nIPs:                      10.153.183.151\nIP:                       34.207.186.189\nLoadBalancer Ingress:     34.207.186.189\nPort:                     snmp-udp  162/UDP\nTargetPort:               2162/UDP\nNodePort:                 snmp-udp  31810/UDP\nEndpoints:                10.3.209.194:2162,10.3.209.210:2162\nSession Affinity:         None\nExternal Traffic Policy:  Local\nHealthCheck NodePort:     31789\nEvents:\n  Type    Reason        Age                   From             Message\n  ----    ------        ----                  ----             -------\n  Normal  nodeAssigned  95s (x45 over 3h30m)  metallb-speaker  announcing from node \"ip-172-31-18-142\" with protocol \"layer2\"\n</code></pre>"},{"location":"troubleshooting/k8s-commands/#the-exec-command","title":"The exec command","text":"<p>The <code>exec</code> command is used to execute a command in a container. The following are some examples of how to use the <code>exec</code> command: </p><pre><code>microk8s kubectl exec -it &lt;pod-name&gt; -- &lt;command&gt;\n</code></pre> <p>For example to connect to the container you can use: </p><pre><code>~$ microk8s kubectl exec -it snmp-mibserver-95df967b9-cjhvz -n sc4snmp  -- /bin/bash \nI have no name!@snmp-mibserver-95df967b9-cjhvz:/app$ \n</code></pre>"},{"location":"troubleshooting/k8s-commands/#the-top-command","title":"The top command","text":"<p>The <code>top</code> command is used to display resource (CPU/memory) usage. The following are options of how to  use the <code>top</code> command: </p><pre><code>microk8s kubectl top nodes\nmicrok8s kubectl top pods\n</code></pre> <p>For example to display resource usage of nodes you can use: </p><pre><code>~$ microk8s kubectl top pods\nNAME                                                              CPU(cores)   MEMORY(bytes)   \nsck-splunk-otel-collector-agent-jrl62                             34m          209Mi           \nsck-splunk-otel-collector-k8s-cluster-receiver-5c56564cf5-ks2zb   3m           99Mi    \n</code></pre>"},{"location":"troubleshooting/k8s-commands/#examples-of-command-usage","title":"Examples of command usage","text":""},{"location":"troubleshooting/k8s-commands/#check-secret-for-snmp-v3","title":"Check secret for snmp v3","text":"<p>One of the issues related to snmp v3 can be incorrectly configured secrets in kubernetes.  Below you can find the instruction to check the existing secrets and decode their value.</p> <p>To check the existing secrets: </p><pre><code>~$ microk8s kubectl get secret -n sc4snmp\nNAME                             TYPE                 DATA   AGE\nsh.helm.release.v1.snmp.v1       helm.sh/release.v1   1      23h\nsh.helm.release.v1.snmp.v2       helm.sh/release.v1   1      21h\nsplunk-connect-for-snmp-splunk   Opaque               1      23h\ntesting1                         Opaque               6      68m\n</code></pre> To get more details about one secret you can use command: <pre><code>~$ microk8s kubectl describe secret/testing1 -n sc4snmp\nName:         testing1\nNamespace:    sc4snmp\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\n\nType:  Opaque\n\nData\n====\nprivProtocol:  3 bytes\nsecurityName:  7 bytes\nuserName:      8 bytes\nauthKey:       10 bytes\nauthProtocol:  3 bytes\nprivKey:       10 bytes\n</code></pre> The secrets in kubernetes are not visible in describe command. To fully see them you have to decode them. Below are some methods to do that: <ul> <li> <p>With json query: </p><pre><code>~$ microk8s kubectl get secrets/testing1 -n sc4snmp -o json | jq '.data | map_values(@base64d)'\n{\n  \"authKey\": \"testing123\",\n  \"authProtocol\": \"MD5\",\n  \"privKey\": \"testing123\",\n  \"privProtocol\": \"AES\",\n  \"securityName\": \"testing\",\n  \"userName\": \"testing1\"\n}\n</code></pre> </li> <li> <p>With template:  </p><pre><code>~$ microk8s kubectl get secrets/testing1 -n sc4snmp --template='{{ range $key, $value := .data }}{{ printf \"%s: %s\\n\" $key ($value | base64decode) }}{{ end }}'\nauthKey: testing123\nauthProtocol: MD5\nprivKey: testing123\nprivProtocol: AES\nsecurityName: testing\nuserName: testing1\n</code></pre> </li> </ul> <p>You can also check this thread for different decoding methods.</p>"},{"location":"troubleshooting/k8s-commands/#check-pods-health","title":"Check pods health","text":"<p>To check the health of the pods, you can use the <code>get</code> command to look at the <code>STATUS</code> and <code>RESTARTS</code> columns.  If the <code>STATUS</code> is not <code>Running</code> or the <code>RESTARTS</code> is not <code>0</code>, then there might be an issue with the pod.  You can also use the <code>describe</code> command to get more detailed information about the pod and see if there are any errors or warnings in the <code>Events</code>.</p>"},{"location":"troubleshooting/k8s-commands/#check-resource-usage","title":"Check resource usage","text":"<p>To check the resource usage of the nodes and pods, you can use the <code>top</code> command.  With this command, you can see the CPU and memory usage of the nodes and pods and compare it with the ones  assigned in <code>resources</code> section in the configuration yaml. If they are close to each other you might consider increasing the resources assigned.</p>"},{"location":"troubleshooting/k8s-commands/#check-network","title":"Check network","text":"<p>Checking the network configuration can be useful when enabling the dual-stack for SC4SNMP. The default network controller used by the microk8s is <code>calico</code>. </p> <p>One of useful commands to check the network configuration is: </p><pre><code>~$ microk8s kubectl describe daemonset/calico-node -n kube-system\n(...)\n    Environment:\n      DATASTORE_TYPE:                     kubernetes\n      WAIT_FOR_DATASTORE:                 true\n      NODENAME:                            (v1:spec.nodeName)\n      CALICO_NETWORKING_BACKEND:          &lt;set to the key 'calico_backend' of config map 'calico-config'&gt;  Optional: false\n      CLUSTER_TYPE:                       k8s,bgp\n      IP:                                 autodetect\n      IP_AUTODETECTION_METHOD:            first-found\n      CALICO_IPV4POOL_VXLAN:              Always\n      IP6_AUTODETECTION_METHOD:           first-found\n      CALICO_IPV6POOL_CIDR:               fd02::/64\n      IP6:                                autodetect\n      CALICO_IPV6POOL_VXLAN:              Always\n      FELIX_IPINIPMTU:                    &lt;set to the key 'veth_mtu' of config map 'calico-config'&gt;  Optional: false\n      FELIX_VXLANMTU:                     &lt;set to the key 'veth_mtu' of config map 'calico-config'&gt;  Optional: false\n      FELIX_WIREGUARDMTU:                 &lt;set to the key 'veth_mtu' of config map 'calico-config'&gt;  Optional: false\n      CALICO_IPV4POOL_CIDR:               10.3.0.0/16\n      CALICO_DISABLE_FILE_LOGGING:        true\n      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT\n      FELIX_IPV6SUPPORT:                  true\n      FELIX_HEALTHENABLED:                true\n      FELIX_FEATUREDETECTOVERRIDE:        ChecksumOffloadBroken=true\n(...)\n</code></pre> One section of the command is showing the <code>environment</code> variables used by the <code>calico</code> network controller.  With seeing them we can check if the different versions of IP are enabled and if the pools for them are  configured with subnet. <p>Next useful command to check when having issues with connectivity is: </p><pre><code>~$ microk8s kubectl describe service/webhook-service -n metallb-system\nName:              webhook-service\nNamespace:         metallb-system\nLabels:            &lt;none&gt;\nAnnotations:       &lt;none&gt;\nSelector:          component=controller\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.153.183.249\nIPs:               10.153.183.249\nPort:              &lt;unset&gt;  443/TCP\nTargetPort:        9443/TCP\nEndpoints:         10.3.209.208:9443\nSession Affinity:  None\n</code></pre> <code>Metallb</code> is the network load-balancer used by the SC4SNMP.  With checking the service configuration we can see the IP assigned to the service and the port it is listening on. When having the issues with dual-stack configuration the <code>IP Family Policy</code> and the <code>IP Families</code> fields should be checked."},{"location":"troubleshooting/k8s-commands/#check-service-configuration","title":"Check service configuration","text":"<p>Checking the service configuration can be useful when having issues with the traps connectivity.  For better explanation refer to: Wrong IP or port.</p>"},{"location":"troubleshooting/polling-issues/","title":"Polling issues","text":""},{"location":"troubleshooting/polling-issues/#identifying-polling-and-walk-issues","title":"Identifying Polling and Walk Issues","text":""},{"location":"troubleshooting/polling-issues/#check-when-snmp-walk-was-executed-last-time-for-the-device","title":"Check when SNMP WALK was executed last time for the device","text":"<ol> <li>Configure Splunk OpenTelemetry Collector for Kubernetes or Configure Docker Logs for Splunk.</li> <li>Go to your Splunk and execute search: <code>index=\"em_logs\"   \"Sending due task\" \"sc4snmp;&lt;IP_ADDRESS&gt;;walk\"</code>  and replace  with the pertinent IP Address. </li> </ol>"},{"location":"troubleshooting/polling-issues/#empty-snmp-response-message-problem","title":"\u201cEmpty SNMP response message\u201d problem","text":"<p>If you see the following line in the worker\u2019s logs:</p> <p></p><pre><code>[2022-01-04 11:44:22,553: INFO/ForkPoolWorker-1] Task splunk_connect_for_snmp.snmp.tasks.walk[8e62fc62-569c-473f-a765-ff92577774e5] retry: Retry in 3489s: SnmpActionError('An error of SNMP isWalk=True for a host 192.168.10.20 occurred: Empty SNMP response message')\n</code></pre> that causes an infinite retry of the walk operation. Add <code>worker.ignoreEmptyVarbinds</code> parameter to <code>values.yaml</code> and set it to true. <p>An example configuration for a worker in <code>values.yaml</code> is:</p> <pre><code>worker:\n  ignoreEmptyVarbinds: true\n</code></pre>"},{"location":"troubleshooting/polling-issues/#oid-not-increasing-problem","title":"\u201cOID not increasing\u201d problem","text":"<p>In case you see the following line in worker\u2019s logs:</p> <p></p><pre><code>[2022-01-04 11:44:22,553: INFO/ForkPoolWorker-1] Task splunk_connect_for_snmp.snmp.tasks.walk[8e62fc62-569c-473f-a765-ff92577774e5] retry: Retry in 3489s: SnmpActionError('An error of SNMP isWalk=True for a host 192.168.10.20 occurred: OID not increasing')\n</code></pre> that causes infinite retry of walk operation, add <code>worker.ignoreNotIncreasingOid</code> array to <code>values.yaml</code> and fill with the addresses of hosts where the problem appears. <p>An example configuration for a worker in <code>values.yaml</code> is:</p> <pre><code>worker:\n  ignoreNotIncreasingOid:\n    - \"127.0.0.1:164\"\n    - \"127.0.0.6\"\n</code></pre> <p>If you put in only the IP address (for example, <code>127.0.0.1</code>), then errors will be ignored for all of its devices (like <code>127.0.0.1:161</code>,  <code>127.0.0.1:163</code>\u2026). If you put the IP address and host as <code>{host}:{port}</code>, that means the error will be ignored only for this device.</p>"},{"location":"troubleshooting/polling-issues/#walking-a-device-takes-too-much-time","title":"Walking a device takes too much time","text":"<p>See Configure small walk profile to enable the small walk  functionality.</p>"},{"location":"troubleshooting/polling-issues/#an-error-of-snmp-iswalktrue-blocks-traffic-on-the-sc4snmp-instance","title":"An error of SNMP isWalk=True blocks traffic on the SC4SNMP instance","text":"<p>If you see many <code>An error of SNMP isWalk=True</code> errors in your logs, that means that there is a connection problem  with the hosts you are polling from. Walk will retry multiple times, which will eventually cause a worker to be blocked while it retries. In that case, you might want to limit the maximum retry time. You can do this by setting the variable <code>worker.walkRetryMaxInterval</code>, for example:</p> <pre><code>worker:\n  walkRetryMaxInterval: 60\n</code></pre> <p>With the previous configuration, \u2018walk\u2019 will retry exponentially from 30 seconds until it reaches 60 seconds. The default value for <code>worker.walkRetryMaxInterval</code> is 180.</p>"},{"location":"troubleshooting/polling-issues/#snmp-rollover","title":"SNMP Rollover","text":"<p>The Rollover problem is due to a finite stored integer value (especially when the value is 32-bit).  When it reaches its maximum, it gets rolled down to 0 again. This causes a strange drop in Analytics data. The most common case of this issue is interface speed on high speed ports. As a solution to this problem, SNMPv2 SMI defined a new object type, counter64, for 64-bit counters, see https://www.cisco.com/c/en/us/support/docs/ip/simple-network-management-protocol-snmp/26007-faq-snmpcounter.html. Not all the devices support it, but if they do, poll the counter64 type OID instead of the counter32 one.  For example, use <code>ifHCInOctets</code> instead of <code>ifInOctets</code>.</p> <p>If 64-bit counter is not supported on your device, you can write your own Splunk queries that calculate the shift based on the maximum integer value and the current state. The same works for values large enough that they don\u2019t fit into a 64-bit value. An example for an appropriate Splunk query would be the following:</p> <pre><code>| streamstats current=f last(ifInOctets) as p_ifInOctets last(ifOutOctets) as p_ifOutOctets by ifAlias             \n| eval in_delta=(ifInOctets - p_ifInOctets)\n| eval out_delta=(ifOutOctets - p_ifOutOctets)\n| eval max=pow(2,64)\n| eval out = if(out_delta&lt;0,((max+out_delta)*8/(5*60*1000*1000*1000)),(out_delta)*8/(5*60*1000*1000*1000))\n| timechart span=5m avg(in) AS in, avg(out) AS out by ifAlias\n</code></pre>"},{"location":"troubleshooting/polling-issues/#polling-authentication-errors","title":"Polling authentication errors","text":""},{"location":"troubleshooting/polling-issues/#unknown-usm-user","title":"Unknown USM user","text":"<p>In case of polling SNMPv3 devices, <code>Unknown USM user</code> error suggests wrong username. Verify  that the kubernetes secret with the correct username has been created (SNMPv3 configuration).</p>"},{"location":"troubleshooting/polling-issues/#wrong-snmp-pdu-digest","title":"Wrong SNMP PDU digest","text":"<p>In case of polling SNMPv3 devices, <code>Wrong SNMP PDU digest</code> error suggests wrong authentication key. Verify  that the kubernetes secret with the correct authentication key has been created (SNMPv3 configuration).</p>"},{"location":"troubleshooting/polling-issues/#no-snmp-response-received-before-timeout","title":"No SNMP response received before timeout","text":"<p><code>No SNMP response received before timeout</code> error might have several root causes. Some of them are:</p> <ul> <li>wrong device IP or port</li> <li>SNMPv2c wrong community string</li> <li>SNMPv3 wrong privacy key</li> </ul>"},{"location":"troubleshooting/polling-issues/#field-is-immutable-error-during-helm-upgrade","title":"\u201cField is immutable\u201d error during helm upgrade","text":"<pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/charts/splunk-connect-for-snmp/ --namespace=sc4snmp --create-namespace\nError: UPGRADE FAILED: cannot patch \"snmp-splunk-connect-for-snmp-inventory\" with kind Job: Job.batch \"snmp-splunk-connect-for-snmp-inventory\" is invalid: (...) : field is immutable\n</code></pre> <p>The immutable error is due to the limitation placed on an inventory job. As the SC4SNMP requires several checks before applying updates, it is designed to allow changes in the inventory task after 5 minutes. </p> <p>The status of the inventory can be checked with the following command: </p><pre><code>microk8s kubectl -n sc4snmp get pods | grep inventory\n</code></pre> If the command is not empty, wait and execute it again after the inventory job finishes. This is when it is no longer visible in the output. <p>If the changes are required to be applied immediately, the previous inventory job can be deleted with the following command: </p><pre><code>microk8s kubectl delete job/snmp-splunk-connect-for-snmp-inventory -n sc4snmp\n</code></pre> The upgrade command can be executed again."},{"location":"troubleshooting/polling-issues/#the-following-profiles-have-invalid-configuration-or-the-following-groups-have-invalid-configuration-errors","title":"\u201cThe following profiles have invalid configuration\u201d or \u201cThe following groups have invalid configuration\u201d errors","text":"<p>Following errors are examples of wrong configuration: </p><pre><code>The following groups have invalid configuration and won't be used: ['group1']. Please check indentation and keywords spelling inside mentioned groups configuration.\n</code></pre> <pre><code>The following profiles have invalid configuration and won't be used: ['standard_profile', 'walk_profile']. Please check indentation and keywords spelling inside mentioned profiles configuration.\n</code></pre> Errors above indicate, that the mentioned groups or profiles might have wrong indentation or some keywords were omitted or misspelled. Refer to: <ul> <li>kubernetes: Configuring profiles or Configuring Groups</li> <li>docker: Scheduler configuration</li> </ul> <p>sections to check how the correct configuration should look like.</p>"},{"location":"troubleshooting/traps-issues/","title":"Traps issues","text":""},{"location":"troubleshooting/traps-issues/#identifying-traps-issues","title":"Identifying Traps issues","text":""},{"location":"troubleshooting/traps-issues/#wrong-ip-or-port","title":"Wrong IP or port","text":"<p>The first possible answer to why traps are not sent to Splunk is that SNMP agents send trap messages to the wrong IP  address or port. To check what is the correct address of traps server, run the following command:</p> <pre><code>microk8s kubectl -n sc4snmp get services\n</code></pre> <p>This command should output similar data: </p><pre><code>NAME                                TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)         AGE\nsnmp-redis-headless                 ClusterIP      None             &lt;none&gt;           6379/TCP        113s\nsnmp-mibserver                      ClusterIP      10.152.183.163   &lt;none&gt;           80/TCP          113s\nsnmp-mongodb                        ClusterIP      10.152.183.118   &lt;none&gt;           27017/TCP       113s\nsnmp-redis-master                   ClusterIP      10.152.183.61    &lt;none&gt;           6379/TCP        113s\nsnmp-mongodb-metrics                ClusterIP      10.152.183.50    &lt;none&gt;           9216/TCP        113s\nsnmp-splunk-connect-for-snmp-trap   LoadBalancer   10.152.183.190   114.241.233.134   162:32180/UDP   113s\n</code></pre> <p>Check the <code>EXTERNAL-IP</code> of <code>snmp-splunk-connect-for-snmp-trap</code> and the second port number for this service. In this case  the full <code>snmp-splunk-connect-for-snmp-trap</code> address will be <code>114.241.233.134:32180</code>.</p> <p>In case agents send traps to the correct address, but there is still no data in the <code>netops</code> index, there might be some issues with credentials. These errors can be seen in logs of the <code>snmp-splunk-connect-for-snmp-trap</code> pod. </p>"},{"location":"troubleshooting/traps-issues/#unknown-snmp-community-name-encountered","title":"Unknown SNMP community name encountered","text":"<p>In case of using community string for authentication purposes, the following error should be expected if the arriving trap  has a community string not configured in SC4SNMP: </p><pre><code>2024-02-06 15:42:14,885 ERROR Security Model failure for device ('18.226.181.199', 42514): Unknown SNMP community name encountered\n</code></pre> <p>If this error occurs, check if the appropriate community is defined under <code>traps.communities</code> in <code>values.yaml</code>. See the  following example of a <code>public</code> community configuration: </p><pre><code>traps:\n  communities:\n    public:\n      communityIndex:\n      contextEngineId:\n      contextName:\n      tag:\n      securityName:\n</code></pre>"},{"location":"troubleshooting/traps-issues/#unknown-snmp-security-name-encountered","title":"Unknown SNMP security name encountered","text":"<p>While sending SNMP v3 traps in case of wrong username or engine id configuration, the following error should be expected:  </p><pre><code>2024-02-06 15:42:14,091 ERROR Security Model failure for device ('18.226.181.199', 46066): Unknown SNMP security name encountered\n</code></pre> <p>If this error occurs, verify that the kubernetes secret with the correct username has been created (SNMPv3 configuration). After creating the secret, add it under <code>traps.usernameSecrets</code> in <code>values.yaml</code>. Check that the correct snmp engine id is configured under <code>traps.securityEngineId</code>. See the following example of a <code>values.yaml</code> with configured secret and engine id: </p><pre><code>traps:\n  usernameSecrets:\n    - my-secret-name\n  securityEngineId:\n    - \"090807060504030201\"\n</code></pre>"},{"location":"troubleshooting/traps-issues/#authenticator-mismatched","title":"Authenticator mismatched","text":"<p>While sending SNMP v3 traps in case of wrong authentication protocol or password configuration, the following error should be expected:  </p><pre><code>2024-02-06 15:42:14,642 ERROR Security Model failure for device ('18.226.181.199', 54806): Authenticator mismatched\n</code></pre> If this error occurs, verify that the kubernetes secret with the correct authentication protocol and password has been created (SNMPv3 configuration). After creating the secret, add it under <code>traps.usernameSecrets</code> in <code>values.yaml</code>. See the following example of a <code>values.yaml</code> with configured secret: <pre><code>traps:\n  usernameSecrets:\n    - my-secret-name\n</code></pre>"},{"location":"troubleshooting/traps-issues/#ciphering-services-not-available-or-ciphertext-is-broken","title":"Ciphering services not available or ciphertext is broken","text":"<p>While sending SNMP v3 traps in case of wrong privacy protocol or password configuration, the following error should be expected:  </p><pre><code>2024-02-06 15:42:14,780 ERROR Security Model failure for device ('18.226.181.199', 48249): Ciphering services not available or ciphertext is broken\n</code></pre> If this error occurs, verify that the kubernetes secret with the correct privacy protocol and password has been created (SNMPv3 configuration). After creating the secret, add it under <code>traps.usernameSecrets</code> in <code>values.yaml</code>. See the following example of a <code>values.yaml</code> with configured secret: <pre><code>traps:\n  usernameSecrets:\n    - my-secret-name\n</code></pre>"}]}